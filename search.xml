<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[K8S调度器（1）]]></title>
    <url>%2Fblog%2F2019%2F08%2F03%2F20190803202810.html</url>
    <content type="text"><![CDATA[调度器概述K8S调度器主要负责将Pod调度到某Node上运行。可以把调度器看做一个黑盒，输入为：pod,nodeList，输出为node，即给定pod和node列表，返回一个将pod调度到某node的结果。 整个调度流程大致如下： graph LR; id1((Start))-->预选 预选-->优选 优选-->id2{是否调度到} id2-->|是|id3 id2-->|否|抢占 抢占-->id3((结束)) 源码阅读本文基于14.3源码 入口文件是在 cmd/kube-scheduler/scheduler.go，加载配置逻辑是 cmd/kube-scheduler/app 文件夹下。整个调度过程逻辑是在 pkg/scheduler下。 源码阅读主要从以下几方面：启动前初始化、启动过程、调度过程(预选、优选、抢占) 启动前初始化过程启动前初始化函数不少，重点关注pkg/scheduler/algorithmprovider/defaults包下的3个文件里的init()。 default.go/init()方法主要注册了算法provider，目前有两个算法provider，分别是DefaultProvider和ClusterAutoscalerProvider。provider是一组算法的组合，包括预选、优选过程中的众多算法。此处注册过程只用到了算法的名字，到真正用到算法时会根据算法名取算法实现。 register_predicates.go/init()方法主要注册多种预选算法，此处注册的方法通过名字与1中关联 register_priorities.go/init()方法主要注册多种优选算法，此处注册的方法通过名字与1中关联 启动过程123456789101112131415161718func main() &#123; rand.Seed(time.Now().UnixNano()) command := app.NewSchedulerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil &#123; fmt.Fprintf(os.Stderr, "%v\n", err) os.Exit(1) &#125;&#125; 首先从main函数(cmd/kube-scheduler/scheduler.go)看起：如上，可以看到main函数核心在 command.Execute，再找到command的类型是SchedulerCommand，然后追踪到该command的定义： 123456789101112131415func NewSchedulerCommand() *cobra.Command &#123; //... cmd := &amp;cobra.Command&#123; Use: "kube-scheduler", Long: `...`, Run: func(cmd *cobra.Command, args []string) &#123; if err := runCommand(cmd, args, opts); err != nil &#123; fmt.Fprintf(os.Stderr, "%v\n", err) os.Exit(1) &#125; &#125;, &#125; // ... return cmd&#125; 重点关注 Run:func(cmd *cobra.Command,args []string)，在这段代码里主要调用了 runCommand方法，再看这个方法： 1234567891011121314151617181920212223func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error &#123; verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) //... // 加载启动配置参数、选主（同一时间只有一个调度器真正在调度） c, err := opts.Config() stopCh := make(chan struct&#123;&#125;) // 根据配置的配置文件，complete配置 cc := c.Complete() // 根据featureGate决定使用的算法 algorithmprovider.ApplyFeatureGates() // Configz registration. if cz, err := configz.New("componentconfig"); err == nil &#123; cz.Set(cc.ComponentConfig) &#125; else &#123; return fmt.Errorf("unable to register configz: %s", err) &#125; return Run(cc, stopCh)&#125; 省略部分不重要代码，留下了核心代码并加了注释，应该很好懂了。最后执行Run方法，接下来看Run方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445func Run(cc schedulerserverconfig.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123; // 创建调度器，初始化调度器参数 sched, err := scheduler.New(cc.Client, cc.InformerFactory.Core().V1().Nodes(), cc.PodInformer, cc.InformerFactory.Core().V1().PersistentVolumes(), cc.InformerFactory.Core().V1().PersistentVolumeClaims(), cc.InformerFactory.Core().V1().ReplicationControllers(), cc.InformerFactory.Apps().V1().ReplicaSets(), cc.InformerFactory.Apps().V1().StatefulSets(), cc.InformerFactory.Core().V1().Services(), cc.InformerFactory.Policy().V1beta1().PodDisruptionBudgets(), cc.InformerFactory.Storage().V1().StorageClasses(), cc.Recorder, cc.ComponentConfig.AlgorithmSource, stopCh, scheduler.WithName(cc.ComponentConfig.SchedulerName), scheduler.WithHardPodAffinitySymmetricWeight(cc.ComponentConfig.HardPodAffinitySymmetricWeight), scheduler.WithPreemptionDisabled(cc.ComponentConfig.DisablePreemption), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithBindTimeoutSeconds(*cc.ComponentConfig.BindTimeoutSeconds)) // 启动所有 informers. go cc.PodInformer.Informer().Run(stopCh) cc.InformerFactory.Start(stopCh) // Wait for all caches to sync before scheduling. cc.InformerFactory.WaitForCacheSync(stopCh) controller.WaitForCacheSync("scheduler", stopCh, cc.PodInformer.Informer().HasSynced) // Prepare a reusable runCommand function. run := func(ctx context.Context) &#123; sched.Run() &lt;-ctx.Done() &#125; ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here defer cancel() // Leader election is disabled, so runCommand inline until done. run(ctx) return fmt.Errorf("finished without leader elect")&#125; 如上，主要初始化了调度器的参数，定义了一个可stop的方法run，最后执行run(ctx)，查看run方法，里边的核心代码是 sched.Run()，此时代码进入到了 pkg/scheduler包里。继续追踪： 1234567func (sched *Scheduler) Run() &#123; if !sched.config.WaitForCacheSync() &#123; return &#125; go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)&#125; 最后执行的代码 go wait.Until…，实际上调用了sched.scheduleOne方法，并无限循环。 至此，启动过程结束，代码真正进到了调度处。 调度过程123456789101112131415161718192021222324252627282930313233343536373839404142434445// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm&apos;s host fitting.func (sched *Scheduler) scheduleOne() &#123; // 从待调度队列中取出一个待调度的pod，此处无数据时会阻塞 pod := sched.config.NextPod() // Synchronously attempt to find a fit for the pod. start := time.Now() // 执行调度方法，返回调度结果 scheduleResult, err := sched.schedule(pod) // 如果调度出错，则可能进入抢占逻辑。 // 无论什么类型错误，进入if后都会return，结束本轮调度 if err != nil &#123; // 如果错误类型是 FitError if fitError, ok := err.(*core.FitError); ok &#123; // 如果不允许抢占，则报错 if !util.PodPriorityEnabled() || sched.config.DisablePreemption &#123; klog.V(3).Infof(&quot;Pod priority feature is not enabled or preemption is disabled by scheduler configuration.&quot; + &quot; No preemption is performed.&quot;) &#125; else &#123; // 否则，进入抢占 preemptionStartTime := time.Now() sched.preempt(pod, fitError) &#125; &#125; else &#123; // 否则，报错 klog.Errorf(&quot;error selecting node for pod: %v&quot;, err) metrics.PodScheduleErrors.Inc() &#125; return &#125; // ... 省略调度成功后绑定volume等操作&#125;func (sched *Scheduler) schedule(pod *v1.Pod) (core.ScheduleResult, error) &#123; result, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil &#123; pod = pod.DeepCopy() sched.recordSchedulingFailure(pod, err, v1.PodReasonUnschedulable, err.Error()) return core.ScheduleResult&#123;&#125;, err &#125; return result, err&#125; 主流程如上，先调度，再判断是否需要抢占，必要时进入抢占，调度成功则执行后续的收尾工作。再看一下调度方法(sched.config.Algorithm.Schedule)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (result ScheduleResult, err error) &#123; trace := utiltrace.New(fmt.Sprintf("Scheduling %s/%s", pod.Namespace, pod.Name)) defer trace.LogIfLong(100 * time.Millisecond) if err := podPassesBasicChecks(pod, g.pvcLister); err != nil &#123; return result, err &#125; nodes, err := nodeLister.List() if err != nil &#123; return result, err &#125; if len(nodes) == 0 &#123; return result, ErrNoNodesAvailable &#125; if err := g.snapshot(); err != nil &#123; return result, err &#125; trace.Step("Computing predicates") startPredicateEvalTime := time.Now() // 核心代码，预选过程，找到可运行pod的nodes filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil &#123; return result, err &#125; // 没找到，直接return if len(filteredNodes) == 0 &#123; return result, &amp;FitError&#123; Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, &#125; &#125; trace.Step("Prioritizing") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 &#123; return ScheduleResult&#123; SuggestedHost: filteredNodes[0].Name, EvaluatedNodes: 1 + len(failedPredicateMap), FeasibleNodes: 1, &#125;, nil &#125; metaPrioritiesInterface := g.priorityMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap) // 核心代码，优选过程，计算每个node的优先级得分 priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil &#123; return result, err &#125; trace.Step("Selecting host") // 找到优先级得分最大的node host, err := g.selectHost(priorityList) return ScheduleResult&#123; SuggestedHost: host, EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap), FeasibleNodes: len(filteredNodes), &#125;, err&#125; 最后本文仅讲解了调度器启动前的初始化过程以及调度整体流程，细节实现还需读者自行了解。]]></content>
      <categories>
        <category>k8s</category>
        <category>功能组件</category>
        <category>调度器</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s调度器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sync.Mutex源码]]></title>
    <url>%2Fblog%2F2019%2F07%2F06%2F20190706025011.html</url>
    <content type="text"><![CDATA[TOC 首先看下源码文件 sync/mutex.go里边开头的注释 sync包提供基本同步原语，如互斥锁，除了Once和WaitGroup外，大多数类型都是供低层级库的协程使用，更高级别的同步最好通过通道和通信来完成，不应复制包含此包中定义的类型的值。 由注释可以看出，官方建议使用channel来做同步，而且此包中的值不允许复制，主要原因是，复制后加锁和解锁操作就不作用在同一个对象上，这样就会出问题。如下例子运行时会报unlock of unlocked mutex错误： 12345678910111213141516package mainimport "sync"func main() &#123; lock := sync.Mutex&#123;&#125; Lock(lock) lock.Unlock()// 此行panic&#125;// 调用此函数时会发生复制，这里加锁针对的是副本加锁// 源lock未加锁，因此上边调用加锁会发生panic// sync包下其他类型复制会发生同样问题func Lock(lock sync.Mutex) &#123; lock.Lock()&#125; 再看下定义常量下边的注释： 锁公平： 互斥可以有两种操作模式：正常和饥饿。 在正常模式下，等待加锁的G按FIFO顺序排队，但被唤醒的waiter并不持有锁，它们与新来的G争夺所有权，新来的G有一个优势，它们已经在CPU上运行，而且可能有很多，所以一个被唤醒的G很有可能抢不到锁，在这种情况下，它将在等待队列前面排队，如果有waiter超过1ms无法获取互斥量，则互斥量mutex进入饥饿模式； 在饥饿模式下，互斥体的所有权直接从解锁的G移交给队列前边的waiter，新来的G不会去争夺锁，即使它看起来是解锁的，也不会去spinning，它们会在队尾排队。如果waiter收到互斥的所有权，并看到①它是队列中最后一个waiter或②它等待不到1ms，它将从互斥模式切换回正常模式。 正常模式有相当好的性能，因为Goroutine可以连续多次获取互斥，即使有阻塞的等待程序。饥饿模式对预防尾部潜伏期病变具有重要意义。 基本结构1234type Mutex struct &#123; state int32 sema uint32&#125; 其中 Mutex.sema表示信号量，Mutex.status字段每一位含义如下： 12345678910state: |32|31|...| |3|2|1| \__________/ | | | | | | | | | | +--- mutex的占用状态（1被占用，0可用） | | | | | +---mutex的当前goroutine是否被唤醒 | | | +---饥饿位，0正常，1饥饿 | +---等待唤醒以尝试锁定的goroutine的计数，0表示没有等待者 加锁解锁方法源码Mutex一共有两个方法：Lock() 和 Unlock() Lock()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// 锁定Mutex.// If the lock is already in use, the calling goroutine// blocks until the mutex is available.func (m *Mutex) Lock() &#123; // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) &#123; if race.Enabled &#123; race.Acquire(unsafe.Pointer(m)) &#125; return &#125; var waitStartTime int64 starving := false awoke := false iter := 0 old := m.state for &#123; // Don't spin in starvation mode, ownership is handed off to waiters // so we won't be able to acquire the mutex anyway.// 如果原状态为非饥饿模式的锁定状态，且canSpin，看样子是根据iter大小判断是否canSpin if old&amp;(mutexLocked|mutexStarving) == mutexLocked &amp;&amp; runtime_canSpin(iter) &#123; // Active spinning makes sense. // Try to set mutexWoken flag to inform Unlock // to not wake other blocked goroutines. // 尝试设置mutexWoken标记来通知Unlock，不去唤醒其他阻塞的G // if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; // 再次确认未被唤醒 old&gt;&gt;mutexWaiterShift != 0 &amp;&amp; // 确认有G在排队 atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) &#123; // 将对象锁置为唤醒状态 awoke = true // 标记当前G awoke &#125; runtime_doSpin() // 空转，根据iter，重试一定次数将不再空转 iter++// 自旋次数 old = m.state // 更新old continue &#125; new := old // 不可空转、 或 ！非饥饿模式的锁定状态 // 不要试图获取饥饿的mutex，新来的G必须排队. if old&amp;mutexStarving == 0 &#123; // 若old是非饥饿模式，标记加锁 new |= mutexLocked &#125; if old&amp;(mutexLocked|mutexStarving) != 0 &#123; // 如果old 已加锁/饥饿模式/加锁数!=0 则加锁数+1 new += 1 &lt;&lt; mutexWaiterShift &#125; // The current goroutine switches mutex to starvation mode.当前G切换到饥饿模式，但若mutex当前未加锁，则不切换 // But if the mutex is currently unlocked, don't do the switch. // Unlock expects that starving mutex has waiters, which will not // be true in this case. Unlock期望饥饿模式下有等待者，这种情况下不会为true if starving &amp;&amp; old&amp;mutexLocked != 0 &#123; // 若处于饥饿模式，且(已加锁或有waiter)，则new饥饿位做标记 new |= mutexStarving &#125; if awoke &#123; // 如果唤醒 // The goroutine has been woken from sleep, // so we need to reset the flag in either case. if new&amp;mutexWoken == 0 &#123; // 如果new的mutexWoken位为0，则抛异常 throw("sync: inconsistent mutex state") &#125;// 将唤醒位置0 new &amp;^= mutexWoken &#125;// 获锁成功 if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; if old&amp;(mutexLocked|mutexStarving) == 0 &#123; // 如果old值锁位与饥饿位标志都为0，则说明获锁成功直接break break // locked the mutex with CAS &#125; // If we were already waiting before, queue at the front of the queue. queueLifo := waitStartTime != 0 // 如果队列不为0，则入队 if waitStartTime == 0 &#123; waitStartTime = runtime_nanotime() &#125; runtime_SemacquireMutex(&amp;m.sema, queueLifo) starving = starving || runtime_nanotime()-waitStartTime &gt; starvationThresholdNs old = m.state if old&amp;mutexStarving != 0 &#123; // If this goroutine was woken and mutex is in starvation mode, // ownership was handed off to us but mutex is in somewhat // inconsistent state: mutexLocked is not set and we are still // accounted as waiter. Fix that. if old&amp;(mutexLocked|mutexWoken) != 0 || old&gt;&gt;mutexWaiterShift == 0 &#123; throw("sync: inconsistent mutex state") &#125; delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift) if !starving || old&gt;&gt;mutexWaiterShift == 1 &#123; // Exit starvation mode. // Critical to do it here and consider wait time. // Starvation mode is so inefficient, that two goroutines // can go lock-step infinitely once they switch mutex // to starvation mode. delta -= mutexStarving &#125; atomic.AddInt32(&amp;m.state, delta) break &#125; awoke = true iter = 0 &#125; else &#123; old = m.state &#125; &#125; if race.Enabled &#123; race.Acquire(unsafe.Pointer(m)) &#125;&#125; Unlock()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Unlock unlocks m.// It is a run-time error if m is not locked on entry to Unlock.//// 一个加锁状态的mutex不会与特定的G关联，允许在一个G加锁在另一个G解锁// A locked Mutex is not associated with a particular goroutine.// It is allowed for one goroutine to lock a Mutex and then// arrange for another goroutine to unlock it.func (m *Mutex) Unlock() &#123; if race.Enabled &#123; _ = m.state race.Release(unsafe.Pointer(m)) &#125; // Fast path: drop lock bit. 快速路径，删除锁标记 new := atomic.AddInt32(&amp;m.state, -mutexLocked) if (new+mutexLocked)&amp;mutexLocked == 0 &#123; throw("sync: unlock of unlocked mutex") &#125; if new&amp;mutexStarving == 0 &#123; // 如果new的饥饿标记位为0 old := new for &#123; // If there are no waiters or a goroutine has already // been woken or grabbed the lock, no need to wake anyone. // In starvation mode ownership is directly handed off from unlocking // goroutine to the next waiter. We are not part of this chain, // since we did not observe mutexStarving when we unlocked the mutex above. // So get off the way. // 如果没有waiter或有G已经被唤醒或获得锁，不必再去唤醒 // 在饥饿模式，解锁的G直接将锁所有权移交给下一个waiter // 我们不是 if old&gt;&gt;mutexWaiterShift == 0 || // 没有waiter在排队 old&amp;(mutexLocked|mutexWoken|mutexStarving) != 0 &#123;// 处于锁定、唤醒、饥饿状态 return &#125; // Grab the right to wake someone. // 抓住唤醒一个waiter的机会 new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; runtime_Semrelease(&amp;m.sema, false) return &#125; old = m.state &#125; &#125; else &#123; // Starving mode: handoff mutex ownership to the next waiter. // Note: mutexLocked is not set, the waiter will set it after wakeup. // But mutex is still considered locked if mutexStarving is set, // so new coming goroutines won't acquire it. // 饥饿模式：移交mutex所有权到下一个waiter // 注意：MutexLocked没有被设置，waiter将会在唤醒后设置它，但如果mutexStarving被设置的情况下 // 需要考虑锁定mutex，因此新到达的G不会获得锁 runtime_Semrelease(&amp;m.sema, true) &#125;&#125;]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-channel基础]]></title>
    <url>%2Fblog%2F2019%2F07%2F01%2F20190701004200.html</url>
    <content type="text"><![CDATA[结构 首先看一下channel的结构定义 1234567891011121314151617181920type hchan struct &#123; qcount uint // 队列长度，缓冲区中入队的数据长度 dataqsiz uint // 环形队列总长度，即缓冲区总长度 buf unsafe.Pointer // 队列中的数据 elemsize uint16 // chan中元素类型长度(unsafe.Sizeof(type)) closed uint32 // 是否关闭，0，1 elemtype *_type // chan中元素类型 sendx uint // 发送元素的索引 recvx uint // 接受元素的索引(配合sendx和buf，可以得到缓冲区真正的数据) recvq waitq // 等待接收数据的g队列 sendq waitq // 等待发送数据的g队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex // 锁&#125; 有缓冲channel对于一个有缓冲的通道，有如下规则： 发送操作会使通道复制被发送的元素。若因通道缓冲区已满而无法复制，则会阻塞发送操作的goroutine。复制的目的地有两种：①当通道缓冲区无数据且有接收方在等待接收数据时，数据将会被直接复制到接收方持有的内存地址；②否则，复制到通道的buf中 接收操作会使通道给出一个已发送给它的元素值的副本，如果通道无数据，则会阻塞接收操作的goroutine 对于同一个元素值来说，把它发送给某个通道的操作，一定会在从该通道接受它的操作完成之前完成 注意： channel变量是引用类型，因此它的初始值为nil，向一个nil的channel发送或接收数据将会导致永久阻塞 发送方向通道发送的数据会被复制，至少1次，最多两次。①1次的情形：A 已有接收方阻塞且通道无缓冲数据;B 无缓冲的channel ②2次情形：无缓冲channel不会复制2次，有缓冲channel只要缓冲区有数据，则都会发生两次 happen before发送操作与接收操作的happen before 无缓冲channel：接收操作完成都在发送操作完成后 有缓冲channel：缓冲长度为k，第c个数据接收完成发送在第k+c个数据发送之前 发送操作前后代码的happen beforegraph TD; subgraph 协程 g1 1-->idch1(channel操作) idch1-->A A-->X end subgraph 协程 g2 2-->idch2(channel操作) idch2-->B B-->Y end 解释：如上图两个协程 g1 和 g2，channel操作指的是读或者写操作，这里两个协程一读一写。happen-before会使代码执行有以下顺序： 1 -&gt; B -&gt; Y 2 -&gt; A -&gt; X 1 -&gt; A -&gt; X 2 -&gt; B -&gt; Y其他顺序无法保证，比如： A与B、Y，B与A、X，1与2，整体顺序如下图：graph TD; subgraph 协程 g1 1-->idch1(channel操作) idch1-->A A-->X end subgraph 协程 g2 2-->idch2(channel操作) idch2-->B B-->Y end 1-->idch2 idch2-->A 2-->idch1 idch1-->B 数据读写向通道写数据 先判断是否有读channel的G阻塞到这里，如果有，则直接将数据复制到等待读数据的G的elem域，并将其标记为_Grunnable，结束 否则，判断channel中已有数据是否小于缓冲大小，如果小于，则将数据复制到channel的循环队列中，结束 否则，判断如果是非阻塞，则直接返回false，表示写数据失败 否则，获取当前g，封装sudog，并把sudog放到channel的sendq域，然后将其标记为_Gwating，等有数据被读取，这里会被唤醒，并执行后续代码，最后返回true123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; // ... lock(&amp;c.lock) // ... if sg := c.recvq.dequeue(); sg != nil &#123; // 1 send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; // 2 if c.qcount &lt; c.dataqsiz &#123; // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled &#123; raceacquire(qp) racerelease(qp) &#125; typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ unlock(&amp;c.lock) return true &#125; // 3 if !block &#123; unlock(&amp;c.lock) return false &#125; // 4 // Block on the channel. Some receiver will complete our operation for us. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) goparkunlock(&amp;c.lock, waitReasonChanSend, traceEvGoBlockSend, 3) // ... releaseSudog(mysg) return true&#125; 从通道读数据 如果channel已关闭，且缓冲区无数据，则直接返回 否则，判断通道是否有写数据的G阻塞，如果有，则调用recv()函数并直接返回2.1 recv() 如果缓冲区无数据，则直接将数据从sudog的G.elem中copy到ep2.2 recv() 否则，将缓冲区第一个数据copy到ep中，并将阻塞的G.elem数据copy到缓冲区队列2.3 recv() 将sudog.g的状态从_Gwaiting转为_Grunnable 否则，若缓冲区队列已有数据，则直接从缓冲区复制，直接返回 否则，若非阻塞，直接返回失败 否则，获取当前g，初始化sudog，将sudog放至channel.recvq队列中，并将状态转至_Gwaiting，等待唤醒 唤醒之后，执行后续逻辑，返回接收数据成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; // ... lock(&amp;c.lock) // 1 if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; if raceenabled &#123; raceacquire(c.raceaddr()) &#125; unlock(&amp;c.lock) if ep != nil &#123; typedmemclr(c.elemtype, ep) &#125; return true, false &#125; // 2 if sg := c.sendq.dequeue(); sg != nil &#123; recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; // 3 if c.qcount &gt; 0 &#123; // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled &#123; raceacquire(qp) racerelease(qp) &#125; if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.qcount-- unlock(&amp;c.lock) return true, true &#125; // 4 if !block &#123; unlock(&amp;c.lock) return false, false &#125; // 5 // no sender available: block on this channel. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) goparkunlock(&amp;c.lock, waitReasonChanReceive, traceEvGoBlockRecv, 3) // 6 // someone woke us up if mysg != gp.waiting &#123; throw("G waiting list is corrupted") &#125; gp.waiting = nil if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed&#125; select12345678910111213141516171819202122232425262728293031323334353637383940414243444546// runtime/select.go// A runtimeSelect is a single case passed to rselect.// This must match ../reflect/value.go:/runtimeSelecttype runtimeSelect struct &#123; dir selectDir // selectSend、selectRecv、selectDefault 分别代表case里的事件 发送、接收、default typ unsafe.Pointer // channel type (not used here) ch *hchan // channel val unsafe.Pointer // ptr to data (SendDir) or ptr to receive buffer (RecvDir)&#125;func reflect_rselect(cases []runtimeSelect) (int, bool) &#123; if len(cases) == 0 &#123; block() &#125; sel := make([]scase, len(cases)) order := make([]uint16, 2*len(cases)) for i := range cases &#123; rc := &amp;cases[i] switch rc.dir &#123; case selectDefault: sel[i] = scase&#123;kind: caseDefault&#125; case selectSend: sel[i] = scase&#123;kind: caseSend, c: rc.ch, elem: rc.val&#125; case selectRecv: sel[i] = scase&#123;kind: caseRecv, c: rc.ch, elem: rc.val&#125; &#125; if raceenabled || msanenabled &#123; selectsetpc(&amp;sel[i]) &#125; &#125; return selectgo(&amp;sel[0], &amp;order[0], len(cases))&#125;// Select case descriptor.// Known to compiler.// Changes here must also be made in src/cmd/internal/gc/select.go's scasetype.type scase struct &#123; c *hchan // chan elem unsafe.Pointer // data element kind uint16 pc uintptr // race pc (for race detector / msan) releasetime int64&#125; 如上代码，初始时，会遍历select里的所有case，并将这些case放入scase数组，scase数组描述每个case的事件，最后将其作为参数调用selectgo方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387// selectgo 实现了select的功能.//// cas0 points to an array of type [ncases]scase, and order0 points to// an array of type [2*ncases]uint16. Both reside on the goroutine's// stack (regardless of any escaping in selectgo).//// selectgo 返回选中的nscase在数组cas0中的下标，它也是select调用语句中对应位置的序号// 如果选中的scase事件是channel接收事件，第二个参数表示是否有值发送到接收方func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) &#123; if debugSelect &#123; print("select: cas0=", cas0, "\n") &#125; cas1 := (*[1 &lt;&lt; 16]scase)(unsafe.Pointer(cas0)) order1 := (*[1 &lt;&lt; 17]uint16)(unsafe.Pointer(order0)) scases := cas1[:ncases:ncases] pollorder := order1[:ncases:ncases] lockorder := order1[ncases:][:ncases:ncases] // Replace send/receive cases involving nil channels with // caseNil so logic below can assume non-nil channel. for i := range scases &#123; cas := &amp;scases[i] if cas.c == nil &amp;&amp; cas.kind != caseDefault &#123; *cas = scase&#123;&#125; &#125; &#125; var t0 int64 if blockprofilerate &gt; 0 &#123; t0 = cputicks() for i := 0; i &lt; ncases; i++ &#123; scases[i].releasetime = -1 &#125; &#125; // The compiler rewrites selects that statically have // only 0 or 1 cases plus default into simpler constructs. // The only way we can end up with such small sel.ncase // values here is for a larger select in which most channels // have been nilled out. The general code handles those // cases correctly, and they are rare enough not to bother // optimizing (and needing to test). // generate permuted order for i := 1; i &lt; ncases; i++ &#123; j := fastrandn(uint32(i + 1)) pollorder[i] = pollorder[j] pollorder[j] = uint16(i) &#125; // sort the cases by Hchan address to get the locking order. // simple heap sort, to guarantee n log n time and constant stack footprint. for i := 0; i &lt; ncases; i++ &#123; j := i // Start with the pollorder to permute cases on the same channel. c := scases[pollorder[i]].c for j &gt; 0 &amp;&amp; scases[lockorder[(j-1)/2]].c.sortkey() &lt; c.sortkey() &#123; k := (j - 1) / 2 lockorder[j] = lockorder[k] j = k &#125; lockorder[j] = pollorder[i] &#125; for i := ncases - 1; i &gt;= 0; i-- &#123; o := lockorder[i] c := scases[o].c lockorder[i] = lockorder[0] j := 0 for &#123; k := j*2 + 1 if k &gt;= i &#123; break &#125; if k+1 &lt; i &amp;&amp; scases[lockorder[k]].c.sortkey() &lt; scases[lockorder[k+1]].c.sortkey() &#123; k++ &#125; if c.sortkey() &lt; scases[lockorder[k]].c.sortkey() &#123; lockorder[j] = lockorder[k] j = k continue &#125; break &#125; lockorder[j] = o &#125; if debugSelect &#123; for i := 0; i+1 &lt; ncases; i++ &#123; if scases[lockorder[i]].c.sortkey() &gt; scases[lockorder[i+1]].c.sortkey() &#123; print("i=", i, " x=", lockorder[i], " y=", lockorder[i+1], "\n") throw("select: broken sort") &#125; &#125; &#125; // lock all the channels involved in the select sellock(scases, lockorder) var ( gp *g sg *sudog c *hchan k *scase sglist *sudog sgnext *sudog qp unsafe.Pointer nextp **sudog )loop: // pass 1 - look for something already waiting var dfli int var dfl *scase var casi int var cas *scase var recvOK bool for i := 0; i &lt; ncases; i++ &#123; casi = int(pollorder[i]) cas = &amp;scases[casi] c = cas.c switch cas.kind &#123; case caseNil: continue case caseRecv: sg = c.sendq.dequeue() if sg != nil &#123; goto recv &#125; if c.qcount &gt; 0 &#123; goto bufrecv &#125; if c.closed != 0 &#123; goto rclose &#125; case caseSend: if raceenabled &#123; racereadpc(c.raceaddr(), cas.pc, chansendpc) &#125; if c.closed != 0 &#123; goto sclose &#125; sg = c.recvq.dequeue() if sg != nil &#123; goto send &#125; if c.qcount &lt; c.dataqsiz &#123; goto bufsend &#125; case caseDefault: dfli = casi dfl = cas &#125; &#125; if dfl != nil &#123; selunlock(scases, lockorder) casi = dfli cas = dfl goto retc &#125; // pass 2 - enqueue on all chans gp = getg() if gp.waiting != nil &#123; throw("gp.waiting != nil") &#125; nextp = &amp;gp.waiting for _, casei := range lockorder &#123; casi = int(casei) cas = &amp;scases[casi] if cas.kind == caseNil &#123; continue &#125; c = cas.c sg := acquireSudog() sg.g = gp sg.isSelect = true // No stack splits between assigning elem and enqueuing // sg on gp.waiting where copystack can find it. sg.elem = cas.elem sg.releasetime = 0 if t0 != 0 &#123; sg.releasetime = -1 &#125; sg.c = c // Construct waiting list in lock order. *nextp = sg nextp = &amp;sg.waitlink switch cas.kind &#123; case caseRecv: c.recvq.enqueue(sg) case caseSend: c.sendq.enqueue(sg) &#125; &#125; // wait for someone to wake us up gp.param = nil gopark(selparkcommit, nil, waitReasonSelect, traceEvGoBlockSelect, 1) sellock(scases, lockorder) gp.selectDone = 0 sg = (*sudog)(gp.param) gp.param = nil // pass 3 - dequeue from unsuccessful chans // otherwise they stack up on quiet channels // record the successful case, if any. // We singly-linked up the SudoGs in lock order. casi = -1 cas = nil sglist = gp.waiting // Clear all elem before unlinking from gp.waiting. for sg1 := gp.waiting; sg1 != nil; sg1 = sg1.waitlink &#123; sg1.isSelect = false sg1.elem = nil sg1.c = nil &#125; gp.waiting = nil for _, casei := range lockorder &#123; k = &amp;scases[casei] if k.kind == caseNil &#123; continue &#125; if sglist.releasetime &gt; 0 &#123; k.releasetime = sglist.releasetime &#125; if sg == sglist &#123; // sg has already been dequeued by the G that woke us up. casi = int(casei) cas = k &#125; else &#123; c = k.c if k.kind == caseSend &#123; c.sendq.dequeueSudoG(sglist) &#125; else &#123; c.recvq.dequeueSudoG(sglist) &#125; &#125; sgnext = sglist.waitlink sglist.waitlink = nil releaseSudog(sglist) sglist = sgnext &#125; if cas == nil &#123; // We can wake up with gp.param == nil (so cas == nil) // when a channel involved in the select has been closed. // It is easiest to loop and re-run the operation; // we'll see that it's now closed. // Maybe some day we can signal the close explicitly, // but we'd have to distinguish close-on-reader from close-on-writer. // It's easiest not to duplicate the code and just recheck above. // We know that something closed, and things never un-close, // so we won't block again. goto loop &#125; c = cas.c if debugSelect &#123; print("wait-return: cas0=", cas0, " c=", c, " cas=", cas, " kind=", cas.kind, "\n") &#125; if cas.kind == caseRecv &#123; recvOK = true &#125; if raceenabled &#123; if cas.kind == caseRecv &amp;&amp; cas.elem != nil &#123; raceWriteObjectPC(c.elemtype, cas.elem, cas.pc, chanrecvpc) &#125; else if cas.kind == caseSend &#123; raceReadObjectPC(c.elemtype, cas.elem, cas.pc, chansendpc) &#125; &#125; if msanenabled &#123; if cas.kind == caseRecv &amp;&amp; cas.elem != nil &#123; msanwrite(cas.elem, c.elemtype.size) &#125; else if cas.kind == caseSend &#123; msanread(cas.elem, c.elemtype.size) &#125; &#125; selunlock(scases, lockorder) goto retcbufrecv: // can receive from buffer if raceenabled &#123; if cas.elem != nil &#123; raceWriteObjectPC(c.elemtype, cas.elem, cas.pc, chanrecvpc) &#125; raceacquire(chanbuf(c, c.recvx)) racerelease(chanbuf(c, c.recvx)) &#125; if msanenabled &amp;&amp; cas.elem != nil &#123; msanwrite(cas.elem, c.elemtype.size) &#125; recvOK = true qp = chanbuf(c, c.recvx) if cas.elem != nil &#123; typedmemmove(c.elemtype, cas.elem, qp) &#125; typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.qcount-- selunlock(scases, lockorder) goto retcbufsend: // can send to buffer if raceenabled &#123; raceacquire(chanbuf(c, c.sendx)) racerelease(chanbuf(c, c.sendx)) raceReadObjectPC(c.elemtype, cas.elem, cas.pc, chansendpc) &#125; if msanenabled &#123; msanread(cas.elem, c.elemtype.size) &#125; typedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ selunlock(scases, lockorder) goto retcrecv: // can receive from sleeping sender (sg) recv(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) if debugSelect &#123; print("syncrecv: cas0=", cas0, " c=", c, "\n") &#125; recvOK = true goto retcrclose: // read at end of closed channel selunlock(scases, lockorder) recvOK = false if cas.elem != nil &#123; typedmemclr(c.elemtype, cas.elem) &#125; if raceenabled &#123; raceacquire(c.raceaddr()) &#125; goto retcsend: // can send to a sleeping receiver (sg) if raceenabled &#123; raceReadObjectPC(c.elemtype, cas.elem, cas.pc, chansendpc) &#125; if msanenabled &#123; msanread(cas.elem, c.elemtype.size) &#125; send(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) if debugSelect &#123; print("syncsend: cas0=", cas0, " c=", c, "\n") &#125; goto retcretc: if cas.releasetime &gt; 0 &#123; blockevent(cas.releasetime-t0, 1) &#125; return casi, recvOKsclose: // send on closed channel selunlock(scases, lockorder) panic(plainError("send on closed channel"))&#125;]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>channel</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine状态变换]]></title>
    <url>%2Fblog%2F2019%2F06%2F27%2F20190627235500.html</url>
    <content type="text"><![CDATA[G状态变换graph TD; id1((Start))-->|新建|Gidle Gidle-->|初始化|Grunnable Grunnable-->|开始运行|Grunning Grunning-->|运行完成|Gdead Gdead-->|结束|id100((End)) Grunning-->|等待事件|Gwaiting Gwaiting-->|"事件到来，G去向「①p.runq ②sched.runq ③running」"|Grunnable Grunning-->|"进入系统调用，此时如果可用M不够用会创建线程"|Gsyscall Gsyscall-->|"退出系统调用，被放入调度器可运行队列"|id2{是否可直接运行} id2-->|是|Grunning id2-->|否|Grunnable Grunning-->|需要扩展/收缩栈|Gcopystack Gcopystack-->|扩展收缩栈完成|Grunning Gdead-->|重新初始化|Grunnable P状态变换graph TD; id0((Start))-->|新建|Pgcstop Pgcstop-->|完成初始化|Pidle Pgcstop-->|启动调度|Pidle Pidle-->|与某个M建立关联|Prunning Prunning-->|与某个M断开关联|Pidle Prunning-->|进入系统调用|Psyscall Psyscall-->|退出系统调用|Prunning Pdead-->|结束|id100((End)) Pidle-->|丢弃|Pdead Prunning-->|丢弃|Pdead Psyscall-->|丢弃|Pdead Psyscall-->|停止调度|Pgcstop Pidle-->|停止调度|Pgcstop Prunning-->|停止调度|Pgcstop]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go程序启动过程分析]]></title>
    <url>%2Fblog%2F2019%2F06%2F27%2F20190627220000.html</url>
    <content type="text"><![CDATA[os：osx 10.14.5 go version: 1.12.5 darwin/amd64 不加特说说明的文件都是在 runtime 包下 程序入口 整个程序入口是在 rt0_darwin_amd64.s的第8行。此处只有一行汇编JMP _rt0_amd64(SB),跳转到asm_amd64.s中的第14行，配置好 argc argv后又跳转到 rt0_go(87行)。正式开始初始化go程序的运行时环境。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384TEXT runtime·rt0_go(SB),NOSPLIT,$0 // 将参数向前复制到一个偶数栈上 MOVQ DI, AX // argc MOVQ SI, BX // argv SUBQ $(4*8+7), SP // 2args 2auto ANDQ $~15, SP MOVQ AX, 16(SP) MOVQ BX, 24(SP) // 从给定（操作系统）栈中创建 istack。 // _cgo_init 可能更新 stackguard MOVQ $runtime·g0(SB), DI LEAQ (-64*1024+104)(SP), BX MOVQ BX, g_stackguard0(DI) MOVQ BX, g_stackguard1(DI) MOVQ BX, (g_stack+stack_lo)(DI) MOVQ SP, (g_stack+stack_hi)(DI) // 寻找正在运行的处理器信息 MOVL $0, AX CPUID MOVL AX, SI // CPU 相关的一些检测 (...)#ifdef GOOS_darwin // 跳过 TLS 设置 on Darwin JMP ok#endif LEAQ runtime·m0+m_tls(SB), DI CALL runtime·settls(SB) // 使用它进行存储，确保能正常运行 get_tls(BX) MOVQ $0x123, g(BX) MOVQ runtime·m0+m_tls(SB), AX CMPQ AX, $0x123 JEQ 2(PC) CALL runtime·abort(SB)ok: // 程序刚刚启动，此时位于主 OS 线程 // 设置 per-goroutine 和 per-mach 寄存器 get_tls(BX) LEAQ runtime·g0(SB), CX MOVQ CX, g(BX) LEAQ runtime·m0(SB), AX // 保存 m-&gt;g0 = g0 MOVQ CX, m_g0(AX) // 保存 m0 to g0-&gt;m MOVQ AX, g_m(CX) CLD // 约定 D 总是被清除 CALL runtime·check(SB) MOVL 16(SP), AX // 复制 argc MOVL AX, 0(SP) MOVQ 24(SP), AX // 复制 argv MOVQ AX, 8(SP) CALL runtime·args(SB) CALL runtime·osinit(SB) CALL runtime·schedinit(SB) // 创建一个新的 goroutine 来启动程序 MOVQ $runtime·mainPC(SB), AX // 入口 PUSHQ AX PUSHQ $0 // 参数大小 CALL runtime·newproc(SB) POPQ AX POPQ AX // 启动这个 M CALL runtime·mstart(SB) CALL runtime·abort(SB) // mstart 应该永不返回 RET (...) RETDATA runtime·mainPC+0(SB)/8,$runtime·main(SB)GLOBL runtime·mainPC(SB),RODATA,$8 引导 从上边汇编代码可以看出，整个过程按如下顺序执行： 初始化系统运行时信息 初始化调度时全局变量 m0 g0及关联关系 runtime.check (实现在runtime1.go)对编译器工作进行校验，确保运行时类型正确 runtime.args (runtime1.go).处理程序参数 runtime.osinit (不同os在不同文件，os_darwin.go).获得CPU核心数 runtime.schedinit (proc.go). 初始化调度器 runtime.newproc (proc.go). 根据主goroutine入口地址创建G，并放至G队列中。 runtime.mstart (proc.go). 开始调度循环 部分过程详细runtime.schedinit 和 runtime.newproc12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// runtime/proc.gofunc schedinit() &#123; \_g_ := getg() (...) // 最大系统线程数量（即 M），参考标准库 runtime/debug.SetMaxThreads sched.maxmcount = 10000 (...) // 模块数据验证 moduledataverify() // 栈、内存分配器、调度器相关初始化。 // 栈初始化，复用管理链表 stackinit() // 内存分配器初始化 mallocinit() // 初始化当前 M mcommoninit(_g_.m) // cpu 相关的初始化 cpuinit() // 必须在 alginit 之前运行 alginit() // maps 不能在此调用之前使用，从 CPU 指令集初始化哈希算法 // 模块加载相关的初始化 modulesinit() // 模块链接，提供 activeModules typelinksinit() // 使用 maps, activeModules itabsinit() // 初始化 interface table，使用 activeModules // 信号屏蔽字初始化 msigsave(_g_.m) initSigmask = _g_.m.sigmask // 处理y命令行用户参数和环境变量 goargs() goenvs() // 处理 GODEBUG、GOTRACEBACK 调试相关的环境变量设置 parsedebugvars() // 垃圾回收器初始化 gcinit() // 网络的上次轮询时间 sched.lastpoll = uint64(nanotime()) // 通过 CPU 核心数和 GOMAXPROCS 环境变量确定 P 的数量 procs := ncpu if n, ok := atoi32(gogetenv("GOMAXPROCS")); ok &amp;&amp; n &gt; 0 &#123; procs = n &#125; // 调整 P 的数量 // 这时所有 P 均为新建的 P，因此不能返回有本地任务的 P if procresize(procs) != nil &#123; throw("unknown runnable goroutine during bootstrap") &#125; // 不重要，调试相关 // For cgocheck &gt; 1, we turn on the write barrier at all times // and check all pointer writes. We can't do this until after // procresize because the write barrier needs a P. if debug.cgocheck &gt; 1 &#123; writeBarrier.cgo = true writeBarrier.enabled = true for _, p := range allp &#123; p.wbBuf.reset() &#125; &#125; if buildVersion == "" &#123; // 该条件永远不会被触发，此处只是为了防止 buildVersion 被编译器优化移除掉。 buildVersion = "unknown" &#125;&#125; 核心组件初始化msigsavestackinitgoroutine栈结构 12345678910111213141516type g struct &#123; // stack 描述了实际的栈内存：[stack.lo, stack.hi) stack stack // stackguard0 是对比 Go 栈增长的 prologue 的栈指针 // 如果 sp 寄存器比 stackguard0 小（由于栈往低地址方向增长），会触发栈拷贝和调度 // 通常情况下：stackguard0 = stack.lo + StackGuard，但被抢占时会变为 StackPreempt stackguard0 uintptr // stackguard1 是对比 C 栈增长的 prologue 的栈指针 // 当位于 g0 和 gsignal 栈上时，值为 stack.lo + StackGuard // 在其他栈上值为 ~0 用于触发 morestackc (并 crash) 调用 stackguard1 uintptr (...) // sched 描述了执行现场 sched gobuf (...)&#125; G的创建：(proc.go newproc()) 首先检查go函数及其参数的合法性 尝试从本地P的自由G列表和调度器的自由G列表获取可用G(line3271:gfget(_p_))，如果未获取到，则新建G(line3273:malg(_StackMin)) 初始化G，包括关联go函数及设置该G的状态和ID等 尝试将G放入本地P的runnext字段(line3348:runqput())：如果runnext为空，则直接放到runnext并返回；如果不为空，则替换，并将原runnext值放到本地P.runq末尾，如果满了将后一半的G移动至全局G队列 mallocinit内存分配器的初始化除去一些例行检查外，就是对堆的初始化了 123456789101112131415161718192021222324252627282930func mallocinit() &#123; // 一些涉及内存分配器的常量的检查，包括 // heapArenaBitmapBytes, physPageSize 等等 ... // 初始化堆 mheap_.init() _g_ := getg() _g_.m.mcache = allocmcache() // 创建初始的 arena 增长 hint if sys.PtrSize == 8 &amp;&amp; GOARCH != "wasm" &#123; for i := 0x7f; i &gt;= 0; i-- &#123; var p uintptr switch &#123; case GOARCH == "arm64" &amp;&amp; GOOS == "darwin": p = uintptr(i)&lt;&lt;40 | uintptrMask&amp;(0x0013&lt;&lt;28) (...) default: p = uintptr(i)&lt;&lt;40 | uintptrMask&amp;(0x00c0&lt;&lt;32) &#125; hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc()) hint.addr = p hint.next, mheap_.arenaHints = mheap_.arenaHints, hint &#125; &#125; else &#123; // 32 位机器，不关心 (...) &#125;&#125; 堆的初始化： 12345678910111213141516171819202122// 堆初始化func (h *mheap) init() &#123; // 初始化堆中各个组件的分配器 h.treapalloc.init(unsafe.Sizeof(treapNode&#123;&#125;), nil, nil, &amp;memstats.other_sys) h.spanalloc.init(unsafe.Sizeof(mspan&#123;&#125;), recordspan, unsafe.Pointer(h), &amp;memstats.mspan_sys) h.cachealloc.init(unsafe.Sizeof(mcache&#123;&#125;), nil, nil, &amp;memstats.mcache_sys) h.specialfinalizeralloc.init(unsafe.Sizeof(specialfinalizer&#123;&#125;), nil, nil, &amp;memstats.other_sys) h.specialprofilealloc.init(unsafe.Sizeof(specialprofile&#123;&#125;), nil, nil, &amp;memstats.other_sys) h.arenaHintAlloc.init(unsafe.Sizeof(arenaHint&#123;&#125;), nil, nil, &amp;memstats.other_sys) // 不对 mspan 的分配清零，后台扫描可以通过分配它来并发的检查一个 span // 因此 span 的 sweepgen 在释放和重新分配时候能存活，从而可以防止后台扫描 // 不正确的将其从 0 进行 CAS。 // // 因为 mspan 不包含堆指针，因此它是安全的 h.spanalloc.zero = false // h-&gt;mapcache 不需要初始化 for i := range h.central &#123; h.central[i].mcentral.init(spanClass(i)) &#125;&#125; mcommoninitM初始化 M 创建时机： 程序运行之初的 M0，无需创建已经存在的系统线程，只需对其进行初始化即可。1.1 schedinit –&gt; mcommoninit –&gt; mpreinit –&gt; msigsave –&gt; initSigmask –&gt; mstart 需要时创建的 M，某些特殊情况下一定会创建一个新的 M并进行初始化，而后创建系统线程：2.1 startm 时没有空闲 m2.2 startTemplateThread 时2.3 startTheWorldWithSema 时 p 如果没有 m2.4 main 时创建系统监控2.5 oneNewExtraM 时2.6 初始化过程： newm –&gt; allocm –&gt; mcommoninit –&gt; mpreinit –&gt; newm1 –&gt; newosproc –&gt; mstart P初始化 G初始化 创建 G 的过程也是相对比较复杂的，我们来总结一下这个过程：首先尝试从 P 本地 gfree 链表或全局 gfree 队列获取已经执行过的、已经执行过的 g 初始化过程中程序无论是本地队列还是全局队列都不可能获取到 g，因此创建一个新的 g，并为其分配运行线程（执行栈）。这时 g 处于 _Gidle 状态创建完成后，g 被更改为 _Gdead 状态，并根据要执行函数的入口地址和参数，初始化执行栈的 SP 和参数的入栈位置，并将需要的参数拷贝一份存入执行栈中根据 SP、参数，在 g.sched 中保存 SP 和 PC 指针来初始化 g 的运行现场将调用方、要执行的函数的入口 PC 进行保存，并将 g 的状态更改为 _Grunnable给 goroutine 分配 id，并将其放入 P 本地队列的队头或全局队列（初始化阶段队列肯定不是满的，因此不可能放入全局队列）检查空闲的 P，将其唤醒，准备执行 G，但我们目前处于初始化阶段，主 goroutine 尚未开始执行，因此这里不会唤醒 P。 gcinitprocresize 调用时已经 STW； 记录调整 P 的时间； 按需调整 allp 的大小； 按需初始化 allp 中的 P； 从 allp 移除不需要的 P，将释放的 P 队列中的任务扔进全局队列； 如果当前的 P 还可以继续使用（没有被移除），则将 P 设置为 _Prunning； 否则将第一个 P 抢过来给当前 G 的 M 进行绑定 最后挨个检查 P，将没有任务的 P 放入 idle 队列 出去当前 P 之外，将有任务的 P 彼此串联成链表，将没有任务的 P 放回到 idle 链表中 runtime.mstart]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine实例与分析]]></title>
    <url>%2Fblog%2F2019%2F06%2F25%2F20190625000000.html</url>
    <content type="text"><![CDATA[实例1：多个goroutine在单个P上运行时调度队列分析示例代码： 12345678910111213141516171819202122232425package mainimport ( "fmt" "runtime" "sync")func main() &#123; runtime.GOMAXPROCS(1) num := 10 wg := sync.WaitGroup&#123;&#125; wg.Add(num) for i := 0; i &lt; num; i++ &#123; go func(i int) &#123; fmt.Printf("%d ",i) wg.Done() &#125;(i) &#125; wg.Wait()&#125;// 打印结果9 0 1 2 3 4 5 6 7 8 此处打印结果应该是不定的，大多数是 9012345678。 原因：在创建协程时，当前协程会先把它放到本地P的runnext中去，该字段用于存放新创建的G，以求更早地运行它，如果此时runnext字段已有一个G，那么这个已有的G就会被踢到P的可运行G队列末尾。此处协程里逻辑比较简单，运行速度很快，所以大多数情况都会按 9 0-8这样顺序调度，如果协程里逻辑比较复杂、CPU执行效率低下或调度器执行了调度了其他任务，结果就可能与这里有出入。 更改num的值，在num值改为小于等于257，都按上述流程工作，因为P本地G队列长度为256，再加上runnext，P本地就可以持有257个G，当大于257个G创建时，它们会被放到全局G队列中去（如果p.runq满，其中一半的G会被转移到调度器的可运行G队列中）]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine总结]]></title>
    <url>%2Fblog%2F2019%2F06%2F24%2F20190624000000.html</url>
    <content type="text"><![CDATA[#]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]go调度器源码分析]]></title>
    <url>%2Fblog%2F2019%2F06%2F23%2F20190623220012.html</url>
    <content type="text"><![CDATA[调度基本数据结构goroutine 在 runtime 中的数据结构: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// stack 描述的是 Go 的执行栈，下界和上界分别为 [lo, hi]// 如果从传统内存布局的角度来讲，Go 的栈实际上是分配在 C 语言中的堆区的// 所以才能比 ulimit -s 的 stack size 还要大(1GB)type stack struct &#123; lo uintptr hi uintptr&#125;// g 的运行现场type gobuf struct &#123; sp uintptr // sp 寄存器 pc uintptr // pc 寄存器 g guintptr // g 指针 ctxt unsafe.Pointer // 这个似乎是用来辅助 gc 的 ret sys.Uintreg lr uintptr // 这是在 arm 上用的寄存器，不用关心 bp uintptr // 开启 GOEXPERIMENT=framepointer，才会有这个&#125;type g struct &#123; // 简单数据结构，lo 和 hi 成员描述了栈的下界和上界内存地址 stack stack // 在函数的栈增长 prologue 中用 sp 寄存器和 stackguard0 来做比较 // 如果 sp 比 stackguard0 小(因为栈向低地址方向增长)，那么就触发栈拷贝和调度 // 正常情况下 stackguard0 = stack.lo + StackGuard // 不过 stackguard0 在需要进行调度时，会被修改为 StackPreempt // 以触发抢占s stackguard0 uintptr // stackguard1 是在 C 栈增长 prologue 作对比的对象 // 在 g0 和 gsignal 栈上，其值为 stack.lo+StackGuard // 在其它的栈上这个值是 ~0(按 0 取反)以触发 morestack 调用(并 crash) stackguard1 uintptr _panic *_panic _defer *_defer m *m // 当前与 g 绑定的 m sched gobuf // goroutine 的现场 syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc stktopsp uintptr // expected sp at top of stack, to check in traceback param unsafe.Pointer // wakeup 时的传入参数 atomicstatus uint32 stackLock uint32 // sigprof/scang lock; TODO: fold in to atomicstatus goid int64 // goroutine id waitsince int64 // g 被阻塞之后的近似时间 waitreason string // if status==Gwaiting schedlink guintptr preempt bool // 抢占标记，这个为 true 时，stackguard0 是等于 stackpreempt 的 throwsplit bool // must not split stack raceignore int8 // ignore race detection events sysblocktraced bool // StartTrace has emitted EvGoInSyscall about this goroutine sysexitticks int64 // syscall 返回之后的 cputicks，用来做 tracing traceseq uint64 // trace event sequencer tracelastp puintptr // last P emitted an event for this goroutine lockedm muintptr // 如果调用了 LockOsThread，那么这个 g 会绑定到某个 m 上 sig uint32 writebuf []byte sigcode0 uintptr sigcode1 uintptr sigpc uintptr gopc uintptr // 创建该 goroutine 的语句的指令地址 startpc uintptr // goroutine 函数的指令地址 racectx uintptr waiting *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order cgoCtxt []uintptr // cgo traceback context labels unsafe.Pointer // profiler labels timer *timer // time.Sleep 缓存的定时器 selectDone uint32 // 该 g 是否正在参与 select，是否已经有人从 select 中胜出&#125; 当 g 遇到阻塞，或需要等待的场景时，会被打包成 sudog 这样一个结构。一个 g 可能被打包为多个 sudog 分别挂在不同的等待队列上: 12345678910111213141516171819202122232425262728293031// sudog 代表在等待列表里的 g，比如向 channel 发送/接收内容时// 之所以需要 sudog 是因为 g 和同步对象之间的关系是多对多的// 一个 g 可能会在多个等待队列中，所以一个 g 可能被打包为多个 sudog// 多个 g 也可以等待在同一个同步对象上// 因此对于一个同步对象就会有很多 sudog 了// sudog 是从一个特殊的池中进行分配的。用 acquireSudog 和 releaseSudog 来分配和释放 sudogtype sudog struct &#123; // 之后的这些字段都是被该 g 所挂在的 channel 中的 hchan.lock 来保护的 // shrinkstack depends on // this for sudogs involved in channel ops. g *g // isSelect 表示一个 g 是否正在参与 select 操作 // 所以 g.selectDone 必须用 CAS 来操作，以胜出唤醒的竞争 isSelect bool next *sudog prev *sudog elem unsafe.Pointer // data element (may point to stack) // 下面这些字段则永远都不会被并发访问 // 对于 channel 来说，waitlink 只会被 g 访问 // 对于信号量来说，所有的字段，包括上面的那些字段都只在持有 semaRoot 锁时才可以访问 acquiretime int64 releasetime int64 ticket uint32 parent *sudog // semaRoot binary tree waitlink *sudog // g.waiting list or semaRoot waittail *sudog // semaRoot c *hchan // channel&#125; 线程在 runtime 中的结构，对应一个 pthread，pthread 也会对应唯一的内核线程(task_struct): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970type m struct &#123; g0 *g // 用来执行调度指令的 goroutine morebuf gobuf // gobuf arg to morestack divmod uint32 // div/mod denominator for arm - known to liblink // Fields not known to debuggers. procid uint64 // for debuggers, but offset not hard-coded gsignal *g // signal-handling g goSigStack gsignalStack // Go-allocated signal handling stack sigmask sigset // storage for saved signal mask tls [6]uintptr // thread-local storage (for x86 extern register) mstartfn func() curg *g // 当前运行的用户 goroutine caughtsig guintptr // goroutine running during fatal signal p puintptr // attached p for executing go code (nil if not executing go code) nextp puintptr id int64 mallocing int32 throwing int32 preemptoff string // 该字段不等于空字符串的话，要保持 curg 始终在这个 m 上运行 locks int32 softfloat int32 dying int32 profilehz int32 helpgc int32 spinning bool // m 失业了，正在积极寻找工作~ blocked bool // m 正阻塞在 note 上 inwb bool // m 正在执行 write barrier newSigstack bool // minit on C thread called sigaltstack printlock int8 incgo bool // m 正在执行 cgo call freeWait uint32 // if == 0, safe to free g0 and delete m (atomic) fastrand [2]uint32 needextram bool traceback uint8 ncgocall uint64 // cgo 调用总计数 ncgo int32 // 当前正在执行的 cgo 订单计数 cgoCallersUse uint32 // if non-zero, cgoCallers in use temporarily cgoCallers *cgoCallers // cgo traceback if crashing in cgo call park note alllink *m // on allm schedlink muintptr mcache *mcache lockedg guintptr createstack [32]uintptr // stack that created this thread. freglo [16]uint32 // d[i] lsb and f[i] freghi [16]uint32 // d[i] msb and f[i+16] fflag uint32 // floating point compare flags lockedExt uint32 // tracking for external LockOSThread lockedInt uint32 // tracking for internal lockOSThread nextwaitm muintptr // 正在等待锁的下一个 m waitunlockf unsafe.Pointer // todo go func(*g, unsafe.pointer) bool waitlock unsafe.Pointer waittraceev byte waittraceskip int startingtrace bool syscalltick uint32 thread uintptr // thread handle freelink *m // on sched.freem // these are here because they are too large to be on the stack // of low-level NOSPLIT functions. libcall libcall libcallpc uintptr // for cpu profiler libcallsp uintptr libcallg guintptr syscall libcall // 存储 windows 平台的 syscall 参数 mOS&#125; 抽象数据结构，可以认为是 processor 的抽象，代表了任务执行时的上下文，m 必须获得 p 才能执行: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778type p struct &#123; lock mutex id int32 status uint32 // one of pidle/prunning/... link puintptr schedtick uint32 // 每次调用 schedule 时会加一 syscalltick uint32 // 每次系统调用时加一 sysmontick sysmontick // 上次 sysmon 观察到的 tick 时间 m muintptr // 和相关联的 m 的反向指针，如果 p 是 idle 的话，那这个指针是 nil mcache *mcache racectx uintptr deferpool [5][]*_defer // pool of available defer structs of different sizes (see panic.go) deferpoolbuf [5][32]*_defer // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen. goidcache uint64 goidcacheend uint64 // runnable 状态的 goroutine。访问时是不加锁的 runqhead uint32 runqtail uint32 runq [256]guintptr // runnext 非空时，代表的是一个 runnable 状态的 G， // 这个 G 是被 当前 G 修改为 ready 状态的， // 并且相比在 runq 中的 G 有更高的优先级 // 如果当前 G 的还有剩余的可用时间，那么就应该运行这个 G // 运行之后，该 G 会继承当前 G 的剩余时间 // If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready'd // goroutines to the end of the run queue. runnext guintptr // Available G's (status == Gdead) gfree *g gfreecnt int32 sudogcache []*sudog sudogbuf [128]*sudog tracebuf traceBufPtr // traceSweep indicates the sweep events should be traced. // This is used to defer the sweep start event until a span // has actually been swept. traceSweep bool // traceSwept and traceReclaimed track the number of bytes // swept and reclaimed by sweeping in the current sweep loop. traceSwept, traceReclaimed uintptr palloc persistentAlloc // per-P to avoid mutex // Per-P GC state gcAssistTime int64 // Nanoseconds in assistAlloc gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker gcBgMarkWorker guintptr gcMarkWorkerMode gcMarkWorkerMode // 当前标记 worker 的开始时间，单位纳秒 gcMarkWorkerStartTime int64 // gcw is this P's GC work buffer cache. The work buffer is // filled by write barriers, drained by mutator assists, and // disposed on certain GC state transitions. gcw gcWork // wbBuf is this P's GC write barrier buffer. // // TODO: Consider caching this in the running G. wbBuf wbBuf runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point pad [sys.CacheLineSize]byte&#125; 全局调度器，全局只有一个 schedt 类型的实例: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364type schedt struct &#123; // 下面两个变量需以原子访问访问。保持在 struct 顶部，以使其在 32 位系统上可以对齐 goidgen uint64 lastpoll uint64 lock mutex // 当修改 nmidle，nmidlelocked，nmsys，nmfreed 这些数值时 // 需要记得调用 checkdead midle muintptr // idle m's waiting for work nmidle int32 // 当前等待工作的空闲 m 计数 nmidlelocked int32 // 当前等待工作的被 lock 的 m 计数 mnext int64 // 当前预缴创建的 m 数，并且该值会作为下一个创建的 m 的 ID maxmcount int32 // 允许创建的最大的 m 数量 nmsys int32 // number of system m's not counted for deadlock nmfreed int64 // cumulative number of freed m's ngsys uint32 // number of system goroutines; updated atomically pidle puintptr // 空闲 p's npidle uint32 nmspinning uint32 // See "Worker thread parking/unparking" comment in proc.go. // 全局的可运行 g 队列 runqhead guintptr runqtail guintptr runqsize int32 // dead G 的全局缓存 gflock mutex gfreeStack *g gfreeNoStack *g ngfree int32 // sudog 结构的集中缓存 sudoglock mutex sudogcache *sudog // 不同大小的可用的 defer struct 的集中缓存池 deferlock mutex deferpool [5]*_defer // 被设置了 m.exited 标记之后的 m，这些 m 正在 freem 这个链表上等待被 free // 链表用 m.freelink 字段进行链接 freem *m gcwaiting uint32 // gc is waiting to run stopwait int32 stopnote note sysmonwait uint32 sysmonnote note // safepointFn should be called on each P at the next GC // safepoint if p.runSafePointFn is set. safePointFn func(*p) safePointWait int32 safePointNote note profilehz int32 // cpu profiling rate procresizetime int64 // 上次修改 gomaxprocs 的纳秒时间 totaltime int64 // ∫gomaxprocs dt up to procresizetime&#125; g/p/m 的关系Go 实现了所谓的 M:N 模型，执行用户代码的 goroutine 可以认为都是对等的 goroutine。不考虑 g0 和 gsignal 的话，我们可以简单地认为调度就是将 m 绑定到 p，然后在 m 中不断循环执行调度函数(runtime.schedule)，寻找可用的 g 来执行，下图为 m 绑定到 p 时，可能得到的 g 的来源: 123456789101112131415161718192021222324252627282930313233343536373839404142 +---------+ | binded +-------------+ +----+----+ |+------------------------------+ | v +-----------------------------+| | | +-------------------------+ | || +------------------+ | | | | | +------------------+ || | Local Run Queue | | | | +------------------+ | | | Global Run Queue | || other P +-+-+-+-+-+-+-+-+--+ | | | P | Local Run Queue | | | schedt +--+-+-+-+-+-+-+---+ || |G|G|G|G|G|G|G| | | | +-+-+-+-+-+-+-+-+--+ | | |G|G|G|G|G|G| || +-+-+-+-+-+-+-+ | | | |G|G|G|G|G|G|G| | | +-+-+-+-+-+-+ || ^ | | | +-+-+-+-+-+-+-+ | | ^ |+------------+-----------------+ | | ^ | +------------+----------------+ | | +-------+-----------------+ | | | | | | | | | | | | | | | | | | | | | | | | | | v | | +------+-------+ .-. +----------------+ | | | steal +---------------( M )-----+ runqget +-----+ | +--------------+ `-&apos; +----------------+ | | | | +-------+---------+ +-----------------------------------------+ globrunqget | | +-----------------+ | | +----------+--------+ | get netpoll g | +----------+--------+ | | | +--------------+--------------+ | | | | netpoll v | | +-+-+-+-+ | | |G|G|G|G| | | +-+-+-+-+ | +-----------------------------+ 这张图展示了 g、p、m 三者之间的大致关系。m 是执行实体，对应的是操作系统线程。可以看到 m 会从绑定的 p 的本地队列、sched 中的全局队列、netpoll 中获取可运行的 g，实在找不着还会去其它的 p 那里去偷。 p 如何初始化程序启动时，会依次调用： graph TD runtime.schedinit --> runtime.procresize 在 procresize 中会将全局 p 数组初始化，并将这些 p 串成链表放进 sched 全局调度器的 pidle 队列中: 1234567891011121314for i := nprocs - 1; i &gt;= 0; i-- &#123; p := allp[i] // ... // 设置 p 的状态 p.status = _Pidle // 初始化时，所有 p 的 runq 都是空的，所以一定会走这个 if if runqempty(p) &#123; // 将 p 放到全局调度器的 pidle 队列中 pidleput(p) &#125; else &#123; // ... &#125;&#125; pidleput 也比较简单，没啥可说的: 1234567891011func pidleput(_p_ *p) &#123; if !runqempty(_p_) &#123; throw("pidleput: P has non-empty run queue") &#125; // 简单的链表操作 _p_.link = sched.pidle sched.pidle.set(_p_) // pidle count + 1 atomic.Xadd(&amp;sched.npidle, 1)&#125; 所有 p 在程序启动的时候就已经被初始化完毕了，除非手动调用 runtime.GOMAXPROCS。 12345678910111213141516func GOMAXPROCS(n int) int &#123; lock(&amp;sched.lock) ret := int(gomaxprocs) unlock(&amp;sched.lock) if n &lt;= 0 || n == ret &#123; return ret &#125; stopTheWorld("GOMAXPROCS") // newprocs will be processed by startTheWorld newprocs = int32(n) startTheWorld() return ret&#125; 在 startTheWorld 中会调用 procresize。 g 如何创建在用户代码里一般这么写: 123go func() &#123; // do the stuff&#125;() 实际上会被翻译成 runtime.newproc，特权语法只是个语法糖。如果你要在其它语言里实现类似的东西，只要实现编译器翻译之后的内容就好了。具体流程: graph TD runtime.newproc --> runtime.newproc1 newproc 干的事情也比较简单 12345678910111213141516func newproc(siz int32, fn *funcval) &#123; // add 是一个指针运算，跳过函数指针 // 把栈上的参数起始地址找到 argp := add(unsafe.Pointer(&amp;fn), sys.PtrSize) pc := getcallerpc() systemstack(func() &#123; newproc1(fn, (*uint8)(argp), siz, pc) &#125;)&#125;// funcval 是一个变长结构，第一个成员是函数指针// 所以上面的 add 是跳过这个 fntype funcval struct &#123; fn uintptr // variable-size, fn-specific data here&#125; runtime 里比较常见的 getcallerpc 和 getcallersp，代码里的注释写的比较明白了: 12345678910// For example://// func f(arg1, arg2, arg3 int) &#123;// pc := getcallerpc()// sp := getcallersp(unsafe.Pointer(&amp;arg1))//&#125;//// These two lines find the PC and SP immediately following// the call to f (where f will return).// getcallerpc 返回的是调用函数之后的那条程序指令的地址，即 callee 函数返回时要执行的下一条指令的地址。 systemstack 在 runtime 中用的也比较多，其功能为让 m 切换到 g0 上执行各种调度函数。至于啥是 g0，在讲 m 的时候再说。 newproc1 的工作流程也比较简单: graph TD newproc1 --> newg newg[gfget] --> nil{is nil?} nil -->|yes|E[init stack] nil -->|no|C[malg] C --> D[set g status=> idle->dead] D --> allgadd E --> G[set g status=> dead-> runnable] allgadd --> G G --> runqput 删掉了不关心的细节后的代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253func newproc1(fn *funcval, argp *uint8, narg int32, callerpc uintptr) &#123; _g_ := getg() if fn == nil &#123; _g_.m.throwing = -1 // do not dump full stacks throw("go of nil func value") &#125; _g_.m.locks++ // disable preemption because it can be holding p in a local var siz := narg siz = (siz + 7) &amp;^ 7 _p_ := _g_.m.p.ptr() newg := gfget(_p_) if newg == nil &#123; newg = malg(_StackMin) casgstatus(newg, _Gidle, _Gdead) allgadd(newg) // publishes with a g-&gt;status of Gdead so GC scanner doesn't look at uninitialized stack. &#125; totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame totalSize += -totalSize &amp; (sys.SpAlign - 1) // align to spAlign sp := newg.stack.hi - totalSize spArg := sp // 初始化 g，g 的 gobuf 现场，g 的 m 的 curg // 以及各种寄存器 memclrNoHeapPointers(unsafe.Pointer(&amp;newg.sched), unsafe.Sizeof(newg.sched)) newg.sched.sp = sp newg.stktopsp = sp newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function newg.sched.g = guintptr(unsafe.Pointer(newg)) gostartcallfn(&amp;newg.sched, fn) newg.gopc = callerpc newg.startpc = fn.fn if _g_.m.curg != nil &#123; newg.labels = _g_.m.curg.labels &#125; casgstatus(newg, _Gdead, _Grunnable) newg.goid = int64(_p_.goidcache) _p_.goidcache++ runqput(_p_, newg, true) if atomic.Load(&amp;sched.npidle) != 0 &amp;&amp; atomic.Load(&amp;sched.nmspinning) == 0 &amp;&amp; mainStarted &#123; wakep() &#125; _g_.m.locks-- if _g_.m.locks == 0 &amp;&amp; _g_.preempt &#123; // restore the preemption request in case we've cleared it in newstack _g_.stackguard0 = stackPreempt &#125;&#125; 所以 go func 执行的结果是调用 runqput 将 g 放进了执行队列。但在放队列之前还做了点小动作: 1newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function gostartcallfn1234567891011121314151617181920212223242526// adjust Gobuf as if it executed a call to fn// and then did an immediate gosave.func gostartcallfn(gobuf *gobuf, fv *funcval) &#123; var fn unsafe.Pointer if fv != nil &#123; fn = unsafe.Pointer(fv.fn) &#125; else &#123; fn = unsafe.Pointer(funcPC(nilfunc)) &#125; gostartcall(gobuf, fn, unsafe.Pointer(fv))&#125;// adjust Gobuf as if it executed a call to fn with context ctxt// and then did an immediate gosave.func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer) &#123; sp := buf.sp if sys.RegSize &gt; sys.PtrSize &#123; sp -= sys.PtrSize *(*uintptr)(unsafe.Pointer(sp)) = 0 &#125; sp -= sys.PtrSize *(*uintptr)(unsafe.Pointer(sp)) = buf.pc // 注意这里，这个，这里的 buf.pc 实际上是 goexit 的 pc buf.sp = sp buf.pc = uintptr(fn) buf.ctxt = ctxt&#125; 在 gostartcall 中把 newproc1 时设置到 buf.pc 中的 goexit 的函数地址放到了 goroutine 的栈顶，然后重新设置 buf.pc 为 goroutine 函数的位置。这样做的目的是为了在执行完任何 goroutine 的函数时，通过 RET 指令，都能从栈顶把 sp 保存的 goexit 的指令 pop 到 pc 寄存器，效果相当于任何 goroutine 执行函数执行完之后，都会去执行 runtime.goexit，完成一些清理工作后再进入 schedule。 在之后的 m 的 schedule 讲解中会看到更详细的调度循环过程。 runqput因为是放 runq 而不是直接执行，因而什么时候开始执行并不是用户代码能决定得了的。再看看 runqput 这个函数: 1234567891011121314151617181920212223242526272829303132333435363738// runqput 尝试把 g 放到本地执行队列中// next 参数如果是 false 的话，runqput 会将 g 放到运行队列的尾部// If next if false, runqput adds g to the tail of the runnable queue.// If next is true, runqput puts g in the _p_.runnext slot.// If the run queue is full, runnext puts g on the global queue.// Executed only by the owner P.func runqput(_p_ *p, gp *g, next bool) &#123; if randomizeScheduler &amp;&amp; next &amp;&amp; fastrand()%2 == 0 &#123; next = false &#125; if next &#123; retryNext: oldnext := _p_.runnext if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) &#123; goto retryNext &#125; if oldnext == 0 &#123; return &#125; // 把之前的 runnext 踢到正常的 runq 中 gp = oldnext.ptr() &#125;retry: h := atomic.Load(&amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h &lt; uint32(len(_p_.runq)) &#123; _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.Store(&amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return &#125; if runqputslow(_p_, gp, h, t) &#123; return &#125; // 队列没有满的话，上面的 put 操作会成功 goto retry&#125; runqputslow12345678910111213141516171819202122232425262728293031323334353637// 因为 slow，所以会一次性把本地队列里的多个 g (包含当前的这个) 放到全局队列// 只会被 g 的 owner P 执行func runqputslow(_p_ *p, gp *g, h, t uint32) bool &#123; var batch [len(_p_.runq)/2 + 1]*g // 先从本地队列抓一批 g n := t - h n = n / 2 if n != uint32(len(_p_.runq)/2) &#123; throw("runqputslow: queue is not full") &#125; for i := uint32(0); i &lt; n; i++ &#123; batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr() &#125; if !atomic.Cas(&amp;_p_.runqhead, h, h+n) &#123; // cas-release, commits consume return false &#125; batch[n] = gp if randomizeScheduler &#123; for i := uint32(1); i &lt;= n; i++ &#123; j := fastrandn(i + 1) batch[i], batch[j] = batch[j], batch[i] &#125; &#125; // 把这些 goroutine 构造成链表 for i := uint32(0); i &lt; n; i++ &#123; batch[i].schedlink.set(batch[i+1]) &#125; // 将链表放到全局队列中 lock(&amp;sched.lock) globrunqputbatch(batch[0], batch[n], int32(n+1)) unlock(&amp;sched.lock) return true&#125; 操作全局 sched 时，需要获取全局 sched.lock 锁，全局锁争抢的开销较大，所以才称之为 slow。p 和 g 在 m 中交互时，因为现场永远是单线程，所以很多时候不用加锁。 m 工作机制在 runtime 中有三种线程，一种是主线程，一种是用来跑 sysmon 的线程，一种是普通的用户线程。主线程在 runtime 由对应的全局变量: runtime.m0 来表示。用户线程就是普通的线程了，和 p 绑定，执行 g 中的任务。虽然说是有三种，实际上前两种线程整个 runtime 就只有一个实例。用户线程才会有很多实例。 主线程 m0主线程中用来跑 runtime.main，流程线性执行，没有跳转: graph TD runtime.main --> A[init max stack size] A --> B[systemstack execute -> newm -> sysmon] B --> runtime.lockOsThread runtime.lockOsThread --> runtime.init runtime.init --> runtime.gcenable runtime.gcenable --> main.init main.init --> main.main sysmon 线程sysmon 是在 runtime.main 中启动的，不过需要注意的是 sysmon 并不是在 m0 上执行的。因为: 123systemstack(func() &#123; newm(sysmon, nil)&#125;) 创建了新的 m，但这个 m 又与普通的线程不一样，因为不需要绑定 p 就可以执行。是与整个调度系统脱离的。 sysmon 内部是个死循环，主要负责以下几件事情: checkdead，检查是否所有 goroutine 都已经锁死，如果是的话，直接调用 runtime.throw，强制退出。这个操作只在启动的时候做一次 将 netpoll 返回的结果注入到全局 sched 的任务队列 收回因为 syscall 而长时间阻塞的 p，同时抢占那些执行时间过长的 g 如果 span 内存闲置超过 5min，那么释放掉 流程图: graph TD sysmon --> usleep usleep --> checkdead checkdead --> |every 10ms|C[netpollinited && lastpoll != 0] C --> |yes|netpoll netpoll --> injectglist injectglist --> retake C --> |no|retake retake --> A[check forcegc needed] A --> B[scavenge heap once in a while] B --> usleep 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119// sysmon 不需要绑定 P 就可以运行，所以不允许 write barriers////go:nowritebarrierrecfunc sysmon() &#123; lock(&amp;sched.lock) sched.nmsys++ checkdead() unlock(&amp;sched.lock) // 如果一个 heap span 在一次GC 之后 5min 都没有被使用过 // 那么把它交还给操作系统 scavengelimit := int64(5 * 60 * 1e9) if debug.scavenge &gt; 0 &#123; // Scavenge-a-lot for testing. forcegcperiod = 10 * 1e6 scavengelimit = 20 * 1e6 &#125; lastscavenge := nanotime() nscavenge := 0 lasttrace := int64(0) idle := 0 // how many cycles in succession we had not wokeup somebody delay := uint32(0) for &#123; if idle == 0 &#123; // 初始化时 20us sleep delay = 20 &#125; else if idle &gt; 50 &#123; // start doubling the sleep after 1ms... delay *= 2 &#125; if delay &gt; 10*1000 &#123; // 最多到 10ms delay = 10 * 1000 &#125; usleep(delay) if debug.schedtrace &lt;= 0 &amp;&amp; (sched.gcwaiting != 0 || atomic.Load(&amp;sched.npidle) == uint32(gomaxprocs)) &#123; lock(&amp;sched.lock) if atomic.Load(&amp;sched.gcwaiting) != 0 || atomic.Load(&amp;sched.npidle) == uint32(gomaxprocs) &#123; atomic.Store(&amp;sched.sysmonwait, 1) unlock(&amp;sched.lock) // Make wake-up period small enough // for the sampling to be correct. maxsleep := forcegcperiod / 2 if scavengelimit &lt; forcegcperiod &#123; maxsleep = scavengelimit / 2 &#125; shouldRelax := true if osRelaxMinNS &gt; 0 &#123; next := timeSleepUntil() now := nanotime() if next-now &lt; osRelaxMinNS &#123; shouldRelax = false &#125; &#125; if shouldRelax &#123; osRelax(true) &#125; notetsleep(&amp;sched.sysmonnote, maxsleep) if shouldRelax &#123; osRelax(false) &#125; lock(&amp;sched.lock) atomic.Store(&amp;sched.sysmonwait, 0) noteclear(&amp;sched.sysmonnote) idle = 0 delay = 20 &#125; unlock(&amp;sched.lock) &#125; // trigger libc interceptors if needed if *cgo_yield != nil &#123; asmcgocall(*cgo_yield, nil) &#125; // 如果 10ms 没有 poll 过 network，那么就 netpoll 一次 lastpoll := int64(atomic.Load64(&amp;sched.lastpoll)) now := nanotime() if netpollinited() &amp;&amp; lastpoll != 0 &amp;&amp; lastpoll+10*1000*1000 &lt; now &#123; atomic.Cas64(&amp;sched.lastpoll, uint64(lastpoll), uint64(now)) gp := netpoll(false) // 非阻塞 -- 返回一个 goroutine 的列表 if gp != nil &#123; // Need to decrement number of idle locked M's // (pretending that one more is running) before injectglist. // Otherwise it can lead to the following situation: // injectglist grabs all P's but before it starts M's to run the P's, // another M returns from syscall, finishes running its G, // observes that there is no work to do and no other running M's // and reports deadlock. incidlelocked(-1) injectglist(gp) incidlelocked(1) &#125; &#125; // 接收在 syscall 状态阻塞的 P // 抢占长时间运行的 G if retake(now) != 0 &#123; idle = 0 &#125; else &#123; idle++ &#125; // 检查是否需要 force GC(两分钟一次的) if t := (gcTrigger&#123;kind: gcTriggerTime, now: now&#125;); t.test() &amp;&amp; atomic.Load(&amp;forcegc.idle) != 0 &#123; lock(&amp;forcegc.lock) forcegc.idle = 0 forcegc.g.schedlink = 0 injectglist(forcegc.g) unlock(&amp;forcegc.lock) &#125; // 每过一段时间扫描一次堆 if lastscavenge+scavengelimit/2 &lt; now &#123; mheap_.scavenge(int32(nscavenge), uint64(now), uint64(scavengelimit)) lastscavenge = now nscavenge++ &#125; if debug.schedtrace &gt; 0 &amp;&amp; lasttrace+int64(debug.schedtrace)*1000000 &lt;= now &#123; lasttrace = now schedtrace(debug.scheddetail &gt; 0) &#125; &#125;&#125; checkdead1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// 检查死锁的场景// 该检查基于当前正在运行的 M 的数量，如果 0，那么就是 deadlock 了// 检查的时候必须持有 sched.lock 锁func checkdead() &#123; // 对于 -buildmode=c-shared 或者 -buildmode=c-archive 来说 // 没有 goroutine 正在运行也是 OK 的。因为调用这个库的程序应该是在运行的 if islibrary || isarchive &#123; return &#125; // If we are dying because of a signal caught on an already idle thread, // freezetheworld will cause all running threads to block. // And runtime will essentially enter into deadlock state, // except that there is a thread that will call exit soon. if panicking &gt; 0 &#123; return &#125; run := mcount() - sched.nmidle - sched.nmidlelocked - sched.nmsys if run &gt; 0 &#123; return &#125; if run &lt; 0 &#123; print("runtime: checkdead: nmidle=", sched.nmidle, " nmidlelocked=", sched.nmidlelocked, " mcount=", mcount(), " nmsys=", sched.nmsys, "\n") throw("checkdead: inconsistent counts") &#125; grunning := 0 lock(&amp;allglock) for i := 0; i &lt; len(allgs); i++ &#123; gp := allgs[i] if isSystemGoroutine(gp) &#123; continue &#125; s := readgstatus(gp) switch s &amp;^ _Gscan &#123; case _Gwaiting: grunning++ case _Grunnable, _Grunning, _Gsyscall: unlock(&amp;allglock) print("runtime: checkdead: find g ", gp.goid, " in status ", s, "\n") throw("checkdead: runnable g") &#125; &#125; unlock(&amp;allglock) if grunning == 0 &#123; // possible if main goroutine calls runtime·Goexit() throw("no goroutines (main called runtime.Goexit) - deadlock!") &#125; // Maybe jump time forward for playground. gp := timejump() if gp != nil &#123; casgstatus(gp, _Gwaiting, _Grunnable) globrunqput(gp) _p_ := pidleget() if _p_ == nil &#123; throw("checkdead: no p for timer") &#125; mp := mget() if mp == nil &#123; // There should always be a free M since // nothing is running. throw("checkdead: no m for timer") &#125; mp.nextp.set(_p_) notewakeup(&amp;mp.park) return &#125; getg().m.throwing = -1 // do not dump full stacks throw("all goroutines are asleep - deadlock!")&#125; retake123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// forcePreemptNS is the time slice given to a G before it is// preempted.const forcePreemptNS = 10 * 1000 * 1000 // 10msfunc retake(now int64) uint32 &#123; n := 0 // Prevent allp slice changes. This lock will be completely // uncontended unless we're already stopping the world. lock(&amp;allpLock) // We can't use a range loop over allp because we may // temporarily drop the allpLock. Hence, we need to re-fetch // allp each time around the loop. for i := 0; i &lt; len(allp); i++ &#123; _p_ := allp[i] if _p_ == nil &#123; // 在 procresize 修改了 allp 但还没有创建新的 p 的时候 // 会有这种情况 continue &#125; pd := &amp;_p_.sysmontick s := _p_.status if s == _Psyscall &#123; // 从 syscall 接管 P，如果它进行 syscall 已经经过了一个 sysmon 的 tick(至少 20us) t := int64(_p_.syscalltick) if int64(pd.syscalltick) != t &#123; pd.syscalltick = uint32(t) pd.syscallwhen = now continue &#125; // 一方面如果没有其它工作可做的话，我们不想接管 p // 但另一方面为了避免 sysmon 线程陷入沉睡，我们最终还是会接管这些 p if runqempty(_p_) &amp;&amp; atomic.Load(&amp;sched.nmspinning)+atomic.Load(&amp;sched.npidle) &gt; 0 &amp;&amp; pd.syscallwhen+10*1000*1000 &gt; now &#123; continue &#125; // 解开 allplock 的锁，然后就可以持有 sched.lock 锁了 unlock(&amp;allpLock) // Need to decrement number of idle locked M's // (pretending that one more is running) before the CAS. // Otherwise the M from which we retake can exit the syscall, // increment nmidle and report deadlock. incidlelocked(-1) if atomic.Cas(&amp;_p_.status, s, _Pidle) &#123; if trace.enabled &#123; traceGoSysBlock(_p_) traceProcStop(_p_) &#125; n++ _p_.syscalltick++ handoffp(_p_) &#125; incidlelocked(1) lock(&amp;allpLock) &#125; else if s == _Prunning &#123; // 如果 G 运行时间太长，那么抢占它 t := int64(_p_.schedtick) if int64(pd.schedtick) != t &#123; pd.schedtick = uint32(t) pd.schedwhen = now continue &#125; if pd.schedwhen+forcePreemptNS &gt; now &#123; continue &#125; preemptone(_p_) &#125; &#125; unlock(&amp;allpLock) return uint32(n)&#125; 普通线程普通线程就是我们 G/P/M 模型里的 M 了，M 对应的就是操作系统的线程。 线程创建上面在创建 sysmon 线程的时候也看到了，创建线程的函数是 newm。 graph TD newm --> newm1 newm1 --> newosproc newosproc --> clone 最终会走到 linux 创建线程的系统调用 clone，代码里大段和 cgo 相关的内容我们就不关心了，摘掉 cgo 相关的逻辑后的代码如下: 12345678910// 创建一个新的 m。该 m 会在启动时调用函数 fn，或者 schedule 函数// fn 需要是 static 类型，且不能是在堆上分配的闭包。// 运行 m 时，m.p 是有可能为 nil 的，所以不允许 write barriers//go:nowritebarrierrecfunc newm(fn func(), _p_ *p) &#123; mp := allocm(_p_, fn) mp.nextp.set(_p_) mp.sigmask = initSigmask newm1(mp)&#125; 传入的 p 会被赋值给 m 的 nextp 成员，在 m 执行 schedule 时，会将 nextp 拿出来，进行之后真正的绑定操作(其实就是把 nextp 赋值为 nil，并把这个 nextp 赋值给 m.p，把 m 赋值给 p.m)。 12345func newm1(mp *m) &#123; execLock.rlock() // Prevent process clone. newosproc(mp, unsafe.Pointer(mp.g0.stack.hi)) execLock.runlock()&#125; 12345678910111213141516func newosproc(mp *m, stk unsafe.Pointer) &#123; // Disable signals during clone, so that the new thread starts // with signals disabled. It will enable them in minit. var oset sigset sigprocmask(_SIG_SETMASK, &amp;sigset_all, &amp;oset) ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart))) sigprocmask(_SIG_SETMASK, &amp;oset, nil) if ret &lt; 0 &#123; print("runtime: failed to create new OS thread (have ", mcount(), " already; errno=", -ret, ")\n") if ret == -_EAGAIN &#123; println("runtime: may need to increase max user processes (ulimit -u)") &#125; throw("newosproc") &#125;&#125; 工作流程首先空闲的 m 会被丢进全局调度器的 midle 队列中，在需要 m 的时候，会先从这里取: 123456789101112//go:nowritebarrierrec// 尝试从 midle 列表中获取一个 m// 必须锁全局的 sched// 可能在 STW 期间执行，所以不允许 write barriersfunc mget() *m &#123; mp := sched.midle.ptr() if mp != nil &#123; sched.midle = mp.schedlink sched.nmidle-- &#125; return mp&#125; 取不到的话就会调用之前提到的 newm 来创建新线程，创建的线程是不会被销毁的，哪怕之后不需要这么多 m 了，也就只是会把 m 放在 midle 中。 什么时候会创建线程呢，可以追踪一下 newm 的调用方: graph TD main --> |sysmon|newm startTheWorld --> startTheWorldWithSema gcMarkTermination --> startTheWorldWithSema gcStart--> startTheWorldWithSema startTheWorldWithSema --> |helpgc|newm startTheWorldWithSema --> |run p|newm startm --> mget mget --> |if no free m|newm startTemplateThread --> |templateThread|newm LockOsThread --> startTemplateThread main --> |iscgo|startTemplateThread handoffp --> startm wakep --> startm injectglist --> startm 基本上来讲，m 都是按需创建的。如果 sched.midle 中没有空闲的 m 了，现在又需要，那么就会去创建一个。 创建好的线程需要绑定到 p 之后才会开始执行，执行过程中也可能被剥夺掉 p。比如前面 retake 的流程，就会将 g 的 stackguard0 修改为 stackPreempt，待下一次进入 newstack 时，会判断是否有该抢占标记，有的话，就会放弃运行。这也就是所谓的协作式抢占。 工作线程执行的内容核心其实就只有俩: schedule() 和 findrunnable()。 schedulegraph TD schedule --> A[schedtick%61 == 0] A --> |yes|globrunqget A --> |no|runqget globrunqget --> C[gp == nil] C --> |no|execute C --> |yes|runqget runqget --> B[gp == nil] B --> |no|execute B --> |yes|findrunnable findrunnable --> execute 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// 调度器调度一轮要执行的函数: 寻找一个 runnable 状态的 goroutine，并 execute 它// 调度函数是循环，永远都不会返回func schedule() &#123; _g_ := getg() if _g_.m.locks != 0 &#123; throw("schedule: holding locks") &#125; if _g_.m.lockedg != 0 &#123; stoplockedm() execute(_g_.m.lockedg.ptr(), false) // Never returns. &#125; // 执行 cgo 调用的 g 不能被 schedule 走 // 因为 cgo 调用使用 m 的 g0 栈 if _g_.m.incgo &#123; throw("schedule: in cgo") &#125;top: if sched.gcwaiting != 0 &#123; gcstopm() goto top &#125; if _g_.m.p.ptr().runSafePointFn != 0 &#123; runSafePointFn() &#125; var gp *g var inheritTime bool if trace.enabled || trace.shutdown &#123; gp = traceReader() if gp != nil &#123; casgstatus(gp, _Gwaiting, _Grunnable) traceGoUnpark(gp, 0) &#125; &#125; if gp == nil &amp;&amp; gcBlackenEnabled != 0 &#123; gp = gcController.findRunnableGCWorker(_g_.m.p.ptr()) &#125; if gp == nil &#123; // 每调度几次就检查一下全局的 runq 来确保公平 // 否则两个 goroutine 就可以通过互相调用 // 完全占用本地的 runq 了 if _g_.m.p.ptr().schedtick%61 == 0 &amp;&amp; sched.runqsize &gt; 0 &#123; lock(&amp;sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) unlock(&amp;sched.lock) &#125; &#125; if gp == nil &#123; gp, inheritTime = runqget(_g_.m.p.ptr()) if gp != nil &amp;&amp; _g_.m.spinning &#123; throw("schedule: spinning with local work") &#125; &#125; if gp == nil &#123; gp, inheritTime = findrunnable() // 在找到 goroutine 之前会一直阻塞下去 &#125; // 当前线程将要执行 goroutine，并且不会再进入 spinning 状态 // 所以如果它被标记为 spinning，我们需要 reset 这个状态 // 可能会重启一个新的 spinning 状态的 M if _g_.m.spinning &#123; resetspinning() &#125; if gp.lockedm != 0 &#123; // Hands off own p to the locked m, // then blocks waiting for a new p. startlockedm(gp) goto top &#125; execute(gp, inheritTime)&#125; m 中所谓的调度循环实际上就是一直在执行下图中的 loop: graph TD schedule --> execute execute --> gogo gogo --> goexit goexit --> goexit1 goexit1 --> goexit0 goexit0 --> schedule execute123456789101112131415161718192021222324// Schedules gp to run on the current M.// If inheritTime is true, gp inherits the remaining time in the// current time slice. Otherwise, it starts a new time slice.// Never returns.//// Write barriers are allowed because this is called immediately after// acquiring a P in several places.////go:yeswritebarrierrecfunc execute(gp *g, inheritTime bool) &#123; _g_ := getg() // 这个可能是 m 的 g0 casgstatus(gp, _Grunnable, _Grunning) gp.waitsince = 0 gp.preempt = false gp.stackguard0 = gp.stack.lo + _StackGuard if !inheritTime &#123; _g_.m.p.ptr().schedtick++ &#125; _g_.m.curg = gp // 把当前 g 的位置让给 m gp.m = _g_.m // 把 gp 指向 m，建立双向关系 gogo(&amp;gp.sched)&#125; 比较简单，绑定 g 和 m，然后 gogo 执行绑定的 g 中的函数。 gogoruntime.gogo 是汇编完成的，功能就是执行 go func() 的这个 func()，可以看到功能主要是把 g 对象的 gobuf 里的内容搬到寄存器里。然后从 gobuf.pc 寄存器存储的指令位置开始继续向后执行。 123456789101112131415161718// void gogo(Gobuf*)// restore state from Gobuf; longjmpTEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ buf+0(FP), BX // gobuf MOVQ gobuf_g(BX), DX MOVQ 0(DX), CX // make sure g != nil get_tls(CX) MOVQ DX, g(CX) MOVQ gobuf_sp(BX), SP // restore SP MOVQ gobuf_ret(BX), AX MOVQ gobuf_ctxt(BX), DX MOVQ gobuf_bp(BX), BP MOVQ $0, gobuf_sp(BX) // clear to help garbage collector MOVQ $0, gobuf_ret(BX) MOVQ $0, gobuf_ctxt(BX) MOVQ $0, gobuf_bp(BX) MOVQ gobuf_pc(BX), BX JMP BX 当然，这里还是有一些和手写汇编不太一样的，看着比较奇怪的地方，gobuf_sp(BX) 这种写法按说标准 plan9 汇编中 gobuf_sp 只是个 symbol，没有任何偏移量的意思，但这里却用名字来代替了其偏移量，这是怎么回事呢？ 实际上这是 runtime 的特权，是需要链接器配合完成的，再来看看 gobuf 在 runtime 中的 struct 定义开头部分的注释: 1// The offsets of sp, pc, and g are known to (hard-coded in) libmach. 这下知道怎么回事了吧，链接器会帮助我们把这个换成偏移量。。 GoexitGoexit : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// Goexit terminates the goroutine that calls it. No other goroutine is affected.// Goexit runs all deferred calls before terminating the goroutine. Because Goexit// is not a panic, any recover calls in those deferred functions will return nil.//// Calling Goexit from the main goroutine terminates that goroutine// without func main returning. Since func main has not returned,// the program continues execution of other goroutines.// If all other goroutines exit, the program crashes.func Goexit() &#123; // Run all deferred functions for the current goroutine. // This code is similar to gopanic, see that implementation // for detailed comments. gp := getg() for &#123; d := gp._defer if d == nil &#123; break &#125; if d.started &#123; if d._panic != nil &#123; d._panic.aborted = true d._panic = nil &#125; d.fn = nil gp._defer = d.link freedefer(d) continue &#125; d.started = true reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz)) if gp._defer != d &#123; throw("bad defer entry in Goexit") &#125; d._panic = nil d.fn = nil gp._defer = d.link freedefer(d) // Note: we ignore recovers here because Goexit isn't a panic &#125; goexit1()&#125;// Finishes execution of the current goroutine.func goexit1() &#123; if raceenabled &#123; racegoend() &#125; if trace.enabled &#123; traceGoEnd() &#125; mcall(goexit0)&#125; 1234567// The top-most function running on a goroutine// returns to goexit+PCQuantum.TEXT runtime·goexit(SB),NOSPLIT,$0-0 BYTE $0x90 // NOP CALL runtime·goexit1(SB) // does not return // traceback from goexit1 must hit code range of goexit BYTE $0x90 // NOP mcall : 12345678910111213141516171819202122232425262728293031323334// func mcall(fn func(*g))// Switch to m-&gt;g0's stack, call fn(g).// Fn must never return. It should gogo(&amp;g-&gt;sched)// to keep running g.TEXT runtime·mcall(SB), NOSPLIT, $0-8 MOVQ fn+0(FP), DI get_tls(CX) MOVQ g(CX), AX // save state in g-&gt;sched MOVQ 0(SP), BX // caller's PC MOVQ BX, (g_sched+gobuf_pc)(AX) LEAQ fn+0(FP), BX // caller's SP MOVQ BX, (g_sched+gobuf_sp)(AX) MOVQ AX, (g_sched+gobuf_g)(AX) MOVQ BP, (g_sched+gobuf_bp)(AX) // switch to m-&gt;g0 &amp; its stack, call fn MOVQ g(CX), BX MOVQ g_m(BX), BX MOVQ m_g0(BX), SI CMPQ SI, AX // if g == m-&gt;g0 call badmcall JNE 3(PC) MOVQ $runtime·badmcall(SB), AX JMP AX MOVQ SI, g(CX) // g = m-&gt;g0 MOVQ (g_sched+gobuf_sp)(SI), SP // sp = m-&gt;g0-&gt;sched.sp PUSHQ AX MOVQ DI, DX MOVQ 0(DI), DI CALL DI POPQ AX MOVQ $runtime·badmcall2(SB), AX JMP AX RET wakep123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Tries to add one more P to execute G's.// Called when a G is made runnable (newproc, ready).func wakep() &#123; // be conservative about spinning threads if !atomic.Cas(&amp;sched.nmspinning, 0, 1) &#123; return &#125; startm(nil, true)&#125;// Schedules some M to run the p (creates an M if necessary).// If p==nil, tries to get an idle P, if no idle P's does nothing.// May run with m.p==nil, so write barriers are not allowed.// If spinning is set, the caller has incremented nmspinning and startm will// either decrement nmspinning or set m.spinning in the newly started M.//go:nowritebarrierrecfunc startm(_p_ *p, spinning bool) &#123; lock(&amp;sched.lock) if _p_ == nil &#123; _p_ = pidleget() if _p_ == nil &#123; unlock(&amp;sched.lock) if spinning &#123; // The caller incremented nmspinning, but there are no idle Ps, // so it's okay to just undo the increment and give up. if int32(atomic.Xadd(&amp;sched.nmspinning, -1)) &lt; 0 &#123; throw("startm: negative nmspinning") &#125; &#125; return &#125; &#125; mp := mget() unlock(&amp;sched.lock) if mp == nil &#123; var fn func() if spinning &#123; // The caller incremented nmspinning, so set m.spinning in the new M. fn = mspinning &#125; newm(fn, _p_) return &#125; if mp.spinning &#123; throw("startm: m is spinning") &#125; if mp.nextp != 0 &#123; throw("startm: m has p") &#125; if spinning &amp;&amp; !runqempty(_p_) &#123; throw("startm: p has runnable gs") &#125; // The caller incremented nmspinning, so set m.spinning in the new M. mp.spinning = spinning mp.nextp.set(_p_) notewakeup(&amp;mp.park)&#125; goroutine 挂起123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// Puts the current goroutine into a waiting state and calls unlockf.// If unlockf returns false, the goroutine is resumed.// unlockf must not access this G's stack, as it may be moved between// the call to gopark and the call to unlockf.func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason string, traceEv byte, traceskip int) &#123; mp := acquirem() gp := mp.curg status := readgstatus(gp) if status != _Grunning &amp;&amp; status != _Gscanrunning &#123; throw("gopark: bad g status") &#125; mp.waitlock = lock mp.waitunlockf = *(*unsafe.Pointer)(unsafe.Pointer(&amp;unlockf)) gp.waitreason = reason mp.waittraceev = traceEv mp.waittraceskip = traceskip releasem(mp) // can't do anything that might move the G between Ms here. mcall(park_m)&#125;func goready(gp *g, traceskip int) &#123; systemstack(func() &#123; ready(gp, traceskip, true) &#125;)&#125;// Mark gp ready to run.func ready(gp *g, traceskip int, next bool) &#123; if trace.enabled &#123; traceGoUnpark(gp, traceskip) &#125; status := readgstatus(gp) // Mark runnable. _g_ := getg() _g_.m.locks++ // disable preemption because it can be holding p in a local var if status&amp;^_Gscan != _Gwaiting &#123; dumpgstatus(gp) throw("bad g-&gt;status in ready") &#125; // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq casgstatus(gp, _Gwaiting, _Grunnable) runqput(_g_.m.p.ptr(), gp, next) if atomic.Load(&amp;sched.npidle) != 0 &amp;&amp; atomic.Load(&amp;sched.nmspinning) == 0 &#123; wakep() &#125; _g_.m.locks-- if _g_.m.locks == 0 &amp;&amp; _g_.preempt &#123; // restore the preemption request in Case we've cleared it in newstack _g_.stackguard0 = stackPreempt &#125;&#125; 123456789101112131415161718192021222324252627282930313233func notesleep(n *note) &#123; gp := getg() if gp != gp.m.g0 &#123; throw("notesleep not on g0") &#125; ns := int64(-1) if *cgo_yield != nil &#123; // Sleep for an arbitrary-but-moderate interval to poll libc interceptors. ns = 10e6 &#125; for atomic.Load(key32(&amp;n.key)) == 0 &#123; gp.m.blocked = true futexsleep(key32(&amp;n.key), 0, ns) if *cgo_yield != nil &#123; asmcgocall(*cgo_yield, nil) &#125; gp.m.blocked = false &#125;&#125;// One-time notifications.func noteclear(n *note) &#123; n.key = 0&#125;func notewakeup(n *note) &#123; old := atomic.Xchg(key32(&amp;n.key), 1) if old != 0 &#123; print("notewakeup - double wakeup (", old, ")\n") throw("notewakeup - double wakeup") &#125; futexwakeup(key32(&amp;n.key), 1)&#125; findrunnablefindrunnable 比较复杂，流程图先把 gc 相关的省略掉了: graph TD runqget --> A[gp == nil] A --> |no|return A --> |yes|globrunqget globrunqget --> B[gp == nil] B --> |no| return B --> |yes| C[netpollinited && lastpoll != 0] C --> |yes|netpoll netpoll --> K[gp == nil] K --> |no|return K --> |yes|runqsteal C --> |no|runqsteal runqsteal --> D[gp == nil] D --> |no|return D --> |yes|E[globrunqget] E --> F[gp == nil] F --> |no| return F --> |yes| G[check all p's runq] G --> H[runq is empty] H --> |no|runqget H --> |yes|I[netpoll] I --> J[gp == nil] J --> |no| return J --> |yes| stopm stopm --> runqget 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215// 找到一个可执行的 goroutine 来 execute// 会尝试从其它的 P 那里偷 g，从全局队列中拿，或者 network 中 pollfunc findrunnable() (gp *g, inheritTime bool) &#123; _g_ := getg() // The conditions here and in handoffp must agree: if // findrunnable would return a G to run, handoffp must start // an M.top: _p_ := _g_.m.p.ptr() if sched.gcwaiting != 0 &#123; gcstopm() goto top &#125; if _p_.runSafePointFn != 0 &#123; runSafePointFn() &#125; if fingwait &amp;&amp; fingwake &#123; if gp := wakefing(); gp != nil &#123; ready(gp, 0, true) &#125; &#125; if *cgo_yield != nil &#123; asmcgocall(*cgo_yield, nil) &#125; // 本地 runq if gp, inheritTime := runqget(_p_); gp != nil &#123; return gp, inheritTime &#125; // 全局 runq if sched.runqsize != 0 &#123; lock(&amp;sched.lock) gp := globrunqget(_p_, 0) unlock(&amp;sched.lock) if gp != nil &#123; return gp, false &#125; &#125; // Poll network. // netpoll 是我们执行 work-stealing 之前的一个优化 // 如果没有任何的 netpoll 等待者，或者线程被阻塞在 netpoll 中，我们可以安全地跳过这段逻辑 // 如果在阻塞的线程中存在任何逻辑上的竞争(e.g. 已经从 netpoll 中返回，但还没有设置 lastpoll) // 该线程还是会将下面的 netpoll 阻塞住 if netpollinited() &amp;&amp; atomic.Load(&amp;netpollWaiters) &gt; 0 &amp;&amp; atomic.Load64(&amp;sched.lastpoll) != 0 &#123; if gp := netpoll(false); gp != nil &#123; // 非阻塞 // netpoll 返回 goroutine 链表，用 schedlink 连接 injectglist(gp.schedlink.ptr()) casgstatus(gp, _Gwaiting, _Grunnable) if trace.enabled &#123; traceGoUnpark(gp, 0) &#125; return gp, false &#125; &#125; // 从其它 p 那里偷 g procs := uint32(gomaxprocs) if atomic.Load(&amp;sched.npidle) == procs-1 &#123; // GOMAXPROCS=1 或者除了我们其它的 p 都是 idle // 新的工作可能从 syscall/cgocall，网络或者定时器中来。 // 上面这些任务都不会被放到本地的 runq，所有没有可以 stealing 的点 goto stop &#125; // 如果正在自旋的 M 的数量 &gt;= 忙着的 P，那么阻塞 // 这是为了 // 当 GOMAXPROCS 远大于 1，但程序的并行度又很低的时候 // 防止过量的 CPU 消耗 if !_g_.m.spinning &amp;&amp; 2*atomic.Load(&amp;sched.nmspinning) &gt;= procs-atomic.Load(&amp;sched.npidle) &#123; goto stop &#125; if !_g_.m.spinning &#123; _g_.m.spinning = true atomic.Xadd(&amp;sched.nmspinning, 1) &#125; for i := 0; i &lt; 4; i++ &#123; for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() &#123; if sched.gcwaiting != 0 &#123; goto top &#125; stealRunNextG := i &gt; 2 // first look for ready queues with more than 1 g if gp := runqsteal(_p_, allp[enum.position()], stealRunNextG); gp != nil &#123; return gp, false &#125; &#125; &#125;stop: // 没有可以干的事情。如果我们正在 GC 的标记阶段，可以安全地扫描和加深对象的颜色， // 这样可以进行空闲时间的标记，而不是直接放弃 P if gcBlackenEnabled != 0 &amp;&amp; _p_.gcBgMarkWorker != 0 &amp;&amp; gcMarkWorkAvailable(_p_) &#123; _p_.gcMarkWorkerMode = gcMarkWorkerIdleMode gp := _p_.gcBgMarkWorker.ptr() casgstatus(gp, _Gwaiting, _Grunnable) if trace.enabled &#123; traceGoUnpark(gp, 0) &#125; return gp, false &#125; // Before we drop our P, make a snapshot of the allp slice, // which can change underfoot once we no longer block // safe-points. We don't need to snapshot the contents because // everything up to cap(allp) is immutable. allpSnapshot := allp // 返回 P 并阻塞 lock(&amp;sched.lock) if sched.gcwaiting != 0 || _p_.runSafePointFn != 0 &#123; unlock(&amp;sched.lock) goto top &#125; if sched.runqsize != 0 &#123; gp := globrunqget(_p_, 0) unlock(&amp;sched.lock) return gp, false &#125; if releasep() != _p_ &#123; throw("findrunnable: wrong p") &#125; pidleput(_p_) unlock(&amp;sched.lock) // Delicate dance: thread transitions from spinning to non-spinning state, // potentially concurrently with submission of new goroutines. We must // drop nmspinning first and then check all per-P queues again (with // #StoreLoad memory barrier in between). If we do it the other way around, // another thread can submit a goroutine after we've checked all run queues // but before we drop nmspinning; as the result nobody will unpark a thread // to run the goroutine. // If we discover new work below, we need to restore m.spinning as a signal // for resetspinning to unpark a new worker thread (because there can be more // than one starving goroutine). However, if after discovering new work // we also observe no idle Ps, it is OK to just park the current thread: // the system is fully loaded so no spinning threads are required. // Also see "Worker thread parking/unparking" comment at the top of the file. wasSpinning := _g_.m.spinning if _g_.m.spinning &#123; _g_.m.spinning = false if int32(atomic.Xadd(&amp;sched.nmspinning, -1)) &lt; 0 &#123; throw("findrunnable: negative nmspinning") &#125; &#125; // 再检查一下所有的 runq for _, _p_ := range allpSnapshot &#123; if !runqempty(_p_) &#123; lock(&amp;sched.lock) _p_ = pidleget() unlock(&amp;sched.lock) if _p_ != nil &#123; acquirep(_p_) if wasSpinning &#123; _g_.m.spinning = true atomic.Xadd(&amp;sched.nmspinning, 1) &#125; goto top &#125; break &#125; &#125; // 再检查 gc 空闲 g if gcBlackenEnabled != 0 &amp;&amp; gcMarkWorkAvailable(nil) &#123; lock(&amp;sched.lock) _p_ = pidleget() if _p_ != nil &amp;&amp; _p_.gcBgMarkWorker == 0 &#123; pidleput(_p_) _p_ = nil &#125; unlock(&amp;sched.lock) if _p_ != nil &#123; acquirep(_p_) if wasSpinning &#123; _g_.m.spinning = true atomic.Xadd(&amp;sched.nmspinning, 1) &#125; // Go back to idle GC check. goto stop &#125; &#125; // poll network if netpollinited() &amp;&amp; atomic.Load(&amp;netpollWaiters) &gt; 0 &amp;&amp; atomic.Xchg64(&amp;sched.lastpoll, 0) != 0 &#123; if _g_.m.p != 0 &#123; throw("findrunnable: netpoll with p") &#125; if _g_.m.spinning &#123; throw("findrunnable: netpoll with spinning") &#125; gp := netpoll(true) // 阻塞到返回为止 atomic.Store64(&amp;sched.lastpoll, uint64(nanotime())) if gp != nil &#123; lock(&amp;sched.lock) _p_ = pidleget() unlock(&amp;sched.lock) if _p_ != nil &#123; acquirep(_p_) injectglist(gp.schedlink.ptr()) casgstatus(gp, _Gwaiting, _Grunnable) if trace.enabled &#123; traceGoUnpark(gp, 0) &#125; return gp, false &#125; injectglist(gp) &#125; &#125; stopm() goto top&#125; m 和 p 解绑定handoffpgraph TD mexit --> A[is m0?] A --> |yes|B[handoffp] A --> |no| C[iterate allm] C --> |m found|handoffp C --> |m not found| throw forEachP --> |p status == syscall| handoffp stoplockedm --> handoffp entersyscallblock --> entersyscallblock_handoff entersyscallblock_handoff --> handoffp retake --> |p status == syscall| handoffp 最终会把 p 放回全局的 pidle 队列中: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Hands off P from syscall or locked M.// Always runs without a P, so write barriers are not allowed.//go:nowritebarrierrecfunc handoffp(_p_ *p) &#123; // handoffp must start an M in any situation where // findrunnable would return a G to run on _p_. // if it has local work, start it straight away if !runqempty(_p_) || sched.runqsize != 0 &#123; startm(_p_, false) return &#125; // if it has GC work, start it straight away if gcBlackenEnabled != 0 &amp;&amp; gcMarkWorkAvailable(_p_) &#123; startm(_p_, false) return &#125; // no local work, check that there are no spinning/idle M's, // otherwise our help is not required if atomic.Load(&amp;sched.nmspinning)+atomic.Load(&amp;sched.npidle) == 0 &amp;&amp; atomic.Cas(&amp;sched.nmspinning, 0, 1) &#123; // TODO: fast atomic startm(_p_, true) return &#125; lock(&amp;sched.lock) if sched.gcwaiting != 0 &#123; _p_.status = _Pgcstop sched.stopwait-- if sched.stopwait == 0 &#123; notewakeup(&amp;sched.stopnote) &#125; unlock(&amp;sched.lock) return &#125; if _p_.runSafePointFn != 0 &amp;&amp; atomic.Cas(&amp;_p_.runSafePointFn, 1, 0) &#123; sched.safePointFn(_p_) sched.safePointWait-- if sched.safePointWait == 0 &#123; notewakeup(&amp;sched.safePointNote) &#125; &#125; if sched.runqsize != 0 &#123; unlock(&amp;sched.lock) startm(_p_, false) return &#125; // If this is the last running P and nobody is polling network, // need to wakeup another M to poll network. if sched.npidle == uint32(gomaxprocs-1) &amp;&amp; atomic.Load64(&amp;sched.lastpoll) != 0 &#123; unlock(&amp;sched.lock) startm(_p_, false) return &#125; pidleput(_p_) unlock(&amp;sched.lock)&#125; g 的状态迁移graph LR start{newg} --> Gidle Gidle --> |oneNewExtraM|Gdead Gidle --> |newproc1|Gdead Gdead --> |newproc1|Grunnable Gdead --> |needm|Gsyscall Gscanrunning --> |scang|Grunning Grunnable --> |execute|Grunning Gany --> |casgcopystack|Gcopystack Gcopystack --> |todotodo|Grunning Gsyscall --> |dropm|Gdead Gsyscall --> |exitsyscall0|Grunnable Gsyscall --> |exitsyscall|Grunning Grunning --> |goschedImpl|Grunnable Grunning --> |goexit0|Gdead Grunning --> |newstack|Gcopystack Grunning --> |reentersyscall|Gsyscall Grunning --> |entersyscallblock|Gsyscall Grunning --> |markroot|Gwaiting Grunning --> |gcAssistAlloc1|Gwaiting Grunning --> |park_m|Gwaiting Grunning --> |gcMarkTermination|Gwaiting Grunning --> |gcBgMarkWorker|Gwaiting Grunning --> |newstack|Gwaiting Gwaiting --> |gcMarkTermination|Grunning Gwaiting --> |gcBgMarkWorker|Grunning Gwaiting --> |markroot|Grunning Gwaiting --> |gcAssistAlloc1|Grunning Gwaiting --> |newstack|Grunning Gwaiting --> |findRunnableGCWorker|Grunnable Gwaiting --> |ready|Grunnable Gwaiting --> |findrunnable|Grunnable Gwaiting --> |injectglist|Grunnable Gwaiting --> |schedule|Grunnable Gwaiting --> |park_m|Grunnable Gwaiting --> |procresize|Grunnable Gwaiting --> |checkdead|Grunnable 图上的 Gany 代表任意状态，GC 时的状态切换比较多，如果只关注正常情况下的状态转换，可以把 markroot、gcMark 之类的先忽略掉。 p 的状态迁移graph LR Pidle --> |acquirep1|Prunning Psyscall --> |retake|Pidle Psyscall --> |entersyscall_gcwait|Pgcstop Psyscall --> |exitsyscallfast|Prunning Pany --> |gcstopm|Pgcstop Pany --> |forEachP|Pidle Pany --> |releasep|Pidle Pany --> |handoffp|Pgcstop Pany --> |procresize release current p use allp 0|Pidle Pany --> |procresize when init|Pgcstop Pany --> |procresize when free old p| Pdead Pany --> |procresize after resize use current p|Prunning Pany --> |reentersyscall|Psyscall Pany --> |stopTheWorldWithSema|Pgcstop 抢占流程函数执行是在 goroutine 的栈上，这个栈在函数执行期间是有可能溢出的，我们前面也看到了，如果一个函数用到了栈，会将 stackguard0 和 sp 寄存器进行比较，如果 sp &lt; stackguard0，说明栈已经增长到溢出，因为栈是从内存高地址向低地址方向增长的。 那么这个比较过程是在哪里完成的呢？这一步是由编译器完成的，我们看看一个函数编译后的结果，这段代码来自 go-internals: 12345678910111213141516171819202122230x0000 TEXT "".main(SB), $24-0 ;; stack-split prologue 0x0000 MOVQ (TLS), CX 0x0009 CMPQ SP, 16(CX) 0x000d JLS 58 0x000f SUBQ $24, SP 0x0013 MOVQ BP, 16(SP) 0x0018 LEAQ 16(SP), BP ;; ...omitted FUNCDATA stuff... 0x001d MOVQ $137438953482, AX 0x0027 MOVQ AX, (SP) ;; ...omitted PCDATA stuff... 0x002b CALL "".add(SB) 0x0030 MOVQ 16(SP), BP 0x0035 ADDQ $24, SP 0x0039 RET ;; stack-split epilogue 0x003a NOP ;; ...omitted PCDATA stuff... 0x003a CALL runtime.morestack_noctxt(SB) 0x003f JMP 0 函数开头被插的这段指令，即是将 g struct 中的 stackguard 与 SP 寄存器进行对比，JLS 表示 SP &lt; 16(CX) 的话即跳转。 1234;; stack-split prologue0x0000 MOVQ (TLS), CX0x0009 CMPQ SP, 16(CX)0x000d JLS 58 这里因为 CX 寄存器存储的是 g 的起始地址，而 16(CX) 指的是 g 结构体偏移 16 个字节的位置，可以回顾一下 g 结构体定义，16 个字节恰好是跳过了第一个成员 stack(16字节) 之后的 stackguard0 的位置。 58 转为 16 进制即是 0x3a。 12345;; stack-split epilogue0x003a NOP;; ...omitted PCDATA stuff...0x003a CALL runtime.morestack_noctxt(SB)0x003f JMP 0 morestack_noctxt: 1234// morestack but not preserving ctxt.TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0 MOVL $0, DX JMP runtime·morestack(SB) morestack: 1234567891011121314151617181920212223242526272829303132333435363738394041424344TEXT runtime·morestack(SB),NOSPLIT,$0-0 // Cannot grow scheduler stack (m-&gt;g0). get_tls(CX) MOVQ g(CX), BX MOVQ g_m(BX), BX MOVQ m_g0(BX), SI CMPQ g(CX), SI JNE 3(PC) CALL runtime·badmorestackg0(SB) INT $3 // Cannot grow signal stack (m-&gt;gsignal). MOVQ m_gsignal(BX), SI CMPQ g(CX), SI JNE 3(PC) CALL runtime·badmorestackgsignal(SB) INT $3 // Called from f. // Set m-&gt;morebuf to f's caller. MOVQ 8(SP), AX // f's caller's PC MOVQ AX, (m_morebuf+gobuf_pc)(BX) LEAQ 16(SP), AX // f's caller's SP MOVQ AX, (m_morebuf+gobuf_sp)(BX) get_tls(CX) MOVQ g(CX), SI MOVQ SI, (m_morebuf+gobuf_g)(BX) // Set g-&gt;sched to context in f. MOVQ 0(SP), AX // f's PC MOVQ AX, (g_sched+gobuf_pc)(SI) MOVQ SI, (g_sched+gobuf_g)(SI) LEAQ 8(SP), AX // f's SP MOVQ AX, (g_sched+gobuf_sp)(SI) MOVQ BP, (g_sched+gobuf_bp)(SI) MOVQ DX, (g_sched+gobuf_ctxt)(SI) // Call newstack on m-&gt;g0's stack. MOVQ m_g0(BX), BX MOVQ BX, g(CX) MOVQ (g_sched+gobuf_sp)(BX), SP CALL runtime·newstack(SB) MOVQ $0, 0x1003 // crash if newstack returns RET newstack: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161// Called from runtime·morestack when more stack is needed.// Allocate larger stack and relocate to new stack.// Stack growth is multiplicative, for constant amortized cost.//// g-&gt;atomicstatus will be Grunning or Gscanrunning upon entry.// If the GC is trying to stop this g then it will set preemptscan to true.//// This must be nowritebarrierrec because it can be called as part of// stack growth from other nowritebarrierrec functions, but the// compiler doesn't check this.////go:nowritebarrierrecfunc newstack() &#123; thisg := getg() // TODO: double check all gp. shouldn't be getg(). if thisg.m.morebuf.g.ptr().stackguard0 == stackFork &#123; throw("stack growth after fork") &#125; if thisg.m.morebuf.g.ptr() != thisg.m.curg &#123; print("runtime: newstack called from g=", hex(thisg.m.morebuf.g), "\n"+"\tm=", thisg.m, " m-&gt;curg=", thisg.m.curg, " m-&gt;g0=", thisg.m.g0, " m-&gt;gsignal=", thisg.m.gsignal, "\n") morebuf := thisg.m.morebuf traceback(morebuf.pc, morebuf.sp, morebuf.lr, morebuf.g.ptr()) throw("runtime: wrong goroutine in newstack") &#125; gp := thisg.m.curg if thisg.m.curg.throwsplit &#123; // Update syscallsp, syscallpc in case traceback uses them. morebuf := thisg.m.morebuf gp.syscallsp = morebuf.sp gp.syscallpc = morebuf.pc pcname, pcoff := "(unknown)", uintptr(0) f := findfunc(gp.sched.pc) if f.valid() &#123; pcname = funcname(f) pcoff = gp.sched.pc - f.entry &#125; print("runtime: newstack at ", pcname, "+", hex(pcoff), " sp=", hex(gp.sched.sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n", "\tmorebuf=&#123;pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " lr:", hex(morebuf.lr), "&#125;\n", "\tsched=&#123;pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "&#125;\n") thisg.m.traceback = 2 // Include runtime frames traceback(morebuf.pc, morebuf.sp, morebuf.lr, gp) throw("runtime: stack split at bad time") &#125; morebuf := thisg.m.morebuf thisg.m.morebuf.pc = 0 thisg.m.morebuf.lr = 0 thisg.m.morebuf.sp = 0 thisg.m.morebuf.g = 0 // NOTE: stackguard0 may change underfoot, if another thread // is about to try to preempt gp. Read it just once and use that same // value now and below. preempt := atomic.Loaduintptr(&amp;gp.stackguard0) == stackPreempt // Be conservative about where we preempt. // We are interested in preempting user Go code, not runtime code. // If we're holding locks, mallocing, or preemption is disabled, don't // preempt. // This check is very early in newstack so that even the status change // from Grunning to Gwaiting and back doesn't happen in this case. // That status change by itself can be viewed as a small preemption, // because the GC might change Gwaiting to Gscanwaiting, and then // this goroutine has to wait for the GC to finish before continuing. // If the GC is in some way dependent on this goroutine (for example, // it needs a lock held by the goroutine), that small preemption turns // into a real deadlock. if preempt &#123; if thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != "" || thisg.m.p.ptr().status != _Prunning &#123; // Let the goroutine keep running for now. // gp-&gt;preempt is set, so it will be preempted next time. gp.stackguard0 = gp.stack.lo + _StackGuard gogo(&amp;gp.sched) // never return &#125; &#125; if gp.stack.lo == 0 &#123; throw("missing stack in newstack") &#125; sp := gp.sched.sp if sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 &#123; // The call to morestack cost a word. sp -= sys.PtrSize &#125; if stackDebug &gt;= 1 || sp &lt; gp.stack.lo &#123; print("runtime: newstack sp=", hex(sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n", "\tmorebuf=&#123;pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " lr:", hex(morebuf.lr), "&#125;\n", "\tsched=&#123;pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "&#125;\n") &#125; if sp &lt; gp.stack.lo &#123; print("runtime: gp=", gp, ", gp-&gt;status=", hex(readgstatus(gp)), "\n ") print("runtime: split stack overflow: ", hex(sp), " &lt; ", hex(gp.stack.lo), "\n") throw("runtime: split stack overflow") &#125; if preempt &#123; if gp == thisg.m.g0 &#123; throw("runtime: preempt g0") &#125; if thisg.m.p == 0 &amp;&amp; thisg.m.locks == 0 &#123; throw("runtime: g is running but p is not") &#125; // Synchronize with scang. casgstatus(gp, _Grunning, _Gwaiting) if gp.preemptscan &#123; for !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) &#123; // Likely to be racing with the GC as // it sees a _Gwaiting and does the // stack scan. If so, gcworkdone will // be set and gcphasework will simply // return. &#125; if !gp.gcscandone &#123; // gcw is safe because we're on the // system stack. gcw := &amp;gp.m.p.ptr().gcw scanstack(gp, gcw) if gcBlackenPromptly &#123; gcw.dispose() &#125; gp.gcscandone = true &#125; gp.preemptscan = false gp.preempt = false casfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting) // This clears gcscanvalid. casgstatus(gp, _Gwaiting, _Grunning) gp.stackguard0 = gp.stack.lo + _StackGuard gogo(&amp;gp.sched) // never return &#125; // Act like goroutine called runtime.Gosched. casgstatus(gp, _Gwaiting, _Grunning) gopreempt_m(gp) // never return &#125; // Allocate a bigger segment and move the stack. oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize * 2 if newsize &gt; maxstacksize &#123; print("runtime: goroutine stack exceeds ", maxstacksize, "-byte limit\n") throw("stack overflow") &#125; // The goroutine must be executing in order to call newstack, // so it must be Grunning (or Gscanrunning). casgstatus(gp, _Grunning, _Gcopystack) // The concurrent GC will not scan the stack while we are doing the copy since // the gp is in a Gcopystack status. copystack(gp, newsize, true) if stackDebug &gt;= 1 &#123; print("stack grow done\n") &#125; casgstatus(gp, _Gcopystack, _Grunning) gogo(&amp;gp.sched)&#125; 总结一下流程: graph TD start[entering func] --> cmp[sp < stackguard0] cmp --> |yes| morestack_noctxt cmp --> |no|final[execute func] morestack_noctxt --> morestack morestack --> newstack newstack --> preempt 抢占都是在 newstack 中完成，但抢占标记是在 Go 源代码中的其它位置来进行标记的: 我们来看看 stackPreempt 是在哪些位置赋值给 stackguard0 的: graph LR unlock --> |in case cleared in newstack|restorePreempt ready --> |in case cleared in newstack|restorePreempt startTheWorldWithSema --> |in case cleared in newstack|restorePreempt allocm --> |in case cleared in newstack|restorePreempt exitsyscall --> |in case cleared in newstack|restorePreempt newproc1--> |in case cleared in newstack|restorePreempt releasem --> |in case cleared in newstack|restorePreempt scang --> setPreempt reentersyscall --> setPreempt entersyscallblock --> setPreempt preemptone--> setPreempt enlistWorker --> preemptone retake --> preemptone preemptall --> preemptone freezetheworld --> preemptall stopTheWorldWithSema --> preemptall forEachP --> preemptall startpanic_m --> freezetheworld gcMarkDone --> forEachP 可见只有 gc 和 retake 才会去真正地抢占 g，并没有其它的入口，其它的地方就只是恢复一下可能在 newstack 中被清除掉的抢占标记。 当然，这里 entersyscall 和 entersyscallblock 比较特殊，虽然这俩函数的实现中有设置抢占标记，但实际上这两段逻辑是不会被走到的。因为 syscall 执行时是在 m 的 g0 栈上，如果在执行时被抢占，那么会直接 throw，而无法恢复。]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[追踪goroutine调度器[翻译自：Scheduler Tracing In Go]]]></title>
    <url>%2Fblog%2F2019%2F06%2F23%2F20190623003354.html</url>
    <content type="text"><![CDATA[能够生成分析和调试信息是我喜欢Go语言的原因之一。通过配置GODEBUG环境变量可以在程序运行期间打印出调试信息，包括gc和scheduler的详细或摘要。最棒的是你不必在程序里设置任何开关它就能够正常工作。 开始前 在这篇文章中，我将向您展示如何使用示例并发Go程序中解释调度程序跟踪信息。 如果您对调度程序有基本的了解，这将有所帮助。 我建议在继续之前阅读这两篇文章： Concurrency, Goroutines and GOMAXPROCS Go Scheduler 代码 以下是我们检查和解释GODEBUG结果的示例程序。 List1 123456789101112131415161718192021222324252627package mainimport ( "sync" "time")func main() &#123; workNum := 10 var wg sync.WaitGroup wg.Add(workNum) for i := 0; i &lt; workNum; i++ &#123; go work(&amp;wg) &#125; wg.Wait() // Wait to see the global run queue deplete. time.Sleep(3 * time.Second)&#125;func work(wg *sync.WaitGroup) &#123; time.Sleep(time.Second * 1) var counter int for i := 0; i &lt; 1e10; i++ &#123; counter++ &#125; wg.Done()&#125; List1中的代码旨在针对我们希望运行时发出的调试信息进行预测。 在第12行，声明一个for循环创建十个goroutines。 然后主函数在第16行等待所有goroutines完成他们的工作。 第22行的工作函数休眠一秒钟，然后将局部变量增加100亿次。 递增完成后，该函数在等待组上调用Done并返回。 在设置GODEBUG来运行程序之前首先构建程序是一个不错的主意，该变量由运行时被设置，因此应用go命令也会产生跟踪输出。如果你使用go run来跟踪程序，那么在程序正式运行之前构建时的追踪信息也将被打印出来。 分析 下边让我们来运行这个程序： 1go build example.go 追踪scheduler摘要信息 通过schedtrace 配置项可以让程序在运行时每隔 X 毫秒打印一行包括调度器状态和标准错误的摘要信息。让我们设置GODEBUG选项并运行这个程序吧： 1GOMAXPROCS=1 GODEBUG=schedtrace=1000 ./example 一旦程序开始运行，我们就可以看到追踪(tracing)信息开始打印了。程序本身不输出任何内容，因此控制台输出的内容均为追踪信息，让我们看看前两次tracing信息。 123SCHED 0ms: gomaxprocs=1 idleprocs=0 threads=2 spinningthreads=0 idlethreads=0 runqueue=0 [1]SCHED 1009ms: gomaxprocs=1 idleprocs=0 threads=3 spinningthreads=0 idlethreads=1 runqueue=0 [9] 让我们来分解一下每个字段的含义，并根据示例程序理解这些值的含义： 123456789101112131415161718192021222324251009ms : 从程序开始到打印该行信息经过的毫秒数. 此处是第1秒时打印的trace.gomaxprocs=1 : 配置的P数量(注：需了解 goroutine调度模型中 M P G 概念). 在此次试验中，仅设置为1.特别注意:考虑在这里的processor是一个逻辑processor，而不是物理processor。调度器在这些逻辑processors运行程序，而它们都是通过操作系统与内核线程(os thread)绑定的。操作系统会在可用的物理processor上再次调度这些内核线程。threads=3 : 运行时的线程数(注：我理解为goroutine调度模型中的M). 存在3个，一个用于processor，其他两个用于运行时idlethreads=1 : 空闲线程的数量. 1个线程空闲，2个在运行.idleprocs=0 : 空闲processor的数量. 0个空闲，1个运行.runqueue=0 : 全局运行队列(runq)中goroutine的数量. 所有可运行的goroutine被移动到 P 的local运行队列(注：p.runq).[9] : P 的local 运行队列中的 goroutine 数量（注：由于运行时配置了processor数量为1这里只打印1个，配置多个时会打印多个，对应每一个 P）. 在P的local 运行队列中有 9个goroutine. 程序在运行时给了我们很多非常棒的摘要信息。当我们在查看第一秒打印的追踪信息时，我们可以看到一个goroutine是如何运行而其他9个goroutine在 P的local 运行队列中等待。 示例图1 在示例图1中，用字母 P 来表示processor，用字母 M 来表示线程，用字母 G 来表示goroutine。我们可以根据 runqueue 的值为0而推断出全局的运行队列(Global Run Queue)是空的。prcessor 的 idleprocs 值为0，而它正在运行着一个 goroutine ，我们后边新建的goroutine则在这个 processor的local 运行队列中（根据process 的runqueue值为9判断的）。 当我们把processor个数配置多个时会有怎样的追踪信息呢？让我们把GOMAXPROCS的值设置为2，再看一次： 12345678910111213141516171819202122232425262728293031323334GOMAXPROCS=2 GODEBUG=schedtrace=1000 ./exampleSCHED 0ms: gomaxprocs=2 idleprocs=1 threads=2 spinningthreads=0idlethreads=0 runqueue=0 [0 0]SCHED 1002ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=1idlethreads=1 runqueue=0 [0 4]SCHED 2006ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0idlethreads=1 runqueue=0 [4 4]…SCHED 6024ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0idlethreads=1 runqueue=2 [3 3]…SCHED 10049ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0idlethreads=1 runqueue=4 [2 2]…SCHED 13067ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0idlethreads=1 runqueue=6 [1 1]…SCHED 17084ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0idlethreads=1 runqueue=8 [0 0]…SCHED 21100ms: gomaxprocs=2 idleprocs=2 threads=4 spinningthreads=0idlethreads=2 runqueue=0 [0 0] 运行新程序后，让我们着重看一下第二秒之后的信息: 123456789SCHED 2002ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0 idlethreads=1 runqueue=0 [4 4]2002ms : 标志这是第二秒追踪的信息.gomaxprocs=2 : 程序最大processor数是2.threads=4 : 4个线程，2个用于运行processor，2个用于运行时.idlethreads=1 : 1个空闲线程(3 threads running).idleprocs=0 : 0个空闲processor (2 processors busy).runqueue=0 : 所有可运行的goroutine被移动到 P的local运行队列(所以全局runqueue为0).[4 4] : 每个P的local 运行队列有4个 goroutine等待运行. 示例图2 让我们再看一下示例图2.我们能够看到goroutine是如何在两个 processor上运行的，其中有8个goroutine在runqueue中等待运行，它们分别在两个processor 的local 运行队列中(各4个)。 第6秒的时候，发生了一些变化 12345SCHED 6024ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0 idlethreads=1 runqueue=2 [3 3]idleprocs=0 : 0 processors are idle (2 processors busy).runqueue=2 : 2 goroutines returned and are waiting to be terminated.[3 3] : 3 goroutines are waiting inside each local run queue. 示例图3 在此图中，我们创建的goroutine在完成工作之后被移动到 Golbal Run Queue 中。我们可以看到仍然有2个goroutine正在运行，每个processor各运行一个，每个processor的local 运行队列中有3个goroutine. 特别注意： 在许多情况下，goroutine在终止之前不会移回全局运行队列。 这个程序创建了一个特殊情况，因为for循环执行的逻辑运行时间超过10毫秒并且没有调用任何函数。 10ms是调度程序中的调度量。 执行10ms后，调度程序尝试抢占goroutines。 这些goroutine不能被抢占，因为它们不会调用任何函数。 在这种情况下，一旦goroutine到达wg.Done调用，goroutines立即被抢占并移动到全局运行队列以终止。 17秒的时候，我们可以看到在processor中正在运行着最后的两个goroutine。 12345SCHED 17084ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0 idlethreads=1 runqueue=8 [0 0]idleprocs=0 : 0 processors are idle (2 processors busy).runqueue=8 : 8 goroutines returned and are waiting to be terminated.[0 0] : No goroutines are waiting inside any local run queue. 示例图4 在图4中，我们看到8个goroutine在全局运行队列中，剩下的最后2个goroutine正在运行。 这时每个processor的local 运行队列都为空。 第21秒的摘要信息： 12345SCHED 21100ms: gomaxprocs=2 idleprocs=2 threads=4 spinningthreads=0 idlethreads=2 runqueue=0 [0 0]idleprocs=2 : 2 processors are idle (0 processors busy).runqueue=0 : All the goroutines that were in the queue have been terminated.[0 0] : No goroutines are waiting inside any local run queue. 示例图5 此时，所有的goroutine都完成工作，并且被终结。 追踪详细信息 scheduler的摘要追踪信息非常有用，但是有时你需要更详细的信息。在这种情况下，我们可以增加shceddetail配置，它能够提供 thread、processor、goroutine的详细信息，让我们重新运行程序： 1GOMAXPROCS=2 GODEBUG=schedtrace=1000,scheddetail=1 ./example 以下是第4秒的输出： 12345678910111213141516171819202122SCHED 4028ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=0idlethreads=1 runqueue=2 gcwaiting=0 nmidlelocked=0 stopwait=0 sysmonwait=0P0: status=1 schedtick=10 syscalltick=0 m=3 runqsize=3 gfreecnt=0P1: status=1 schedtick=10 syscalltick=1 m=2 runqsize=3 gfreecnt=0M3: p=0 curg=4 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0 spinning=0 blocked=0 lockedg=-1M2: p=1 curg=10 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0 spinning=0 blocked=0 lockedg=-1M1: p=-1 curg=-1 mallocing=0 throwing=0 gcing=0 locks=1 dying=0 helpgc=0 spinning=0 blocked=0 lockedg=-1M0: p=-1 curg=-1 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0 spinning=0 blocked=0 lockedg=-1G1: status=4(semacquire) m=-1 lockedm=-1G2: status=4(force gc (idle)) m=-1 lockedm=-1G3: status=4(GC sweep wait) m=-1 lockedm=-1G4: status=2(sleep) m=3 lockedm=-1G5: status=1(sleep) m=-1 lockedm=-1G6: status=1(stack growth) m=-1 lockedm=-1G7: status=1(sleep) m=-1 lockedm=-1G8: status=1(sleep) m=-1 lockedm=-1G9: status=1(stack growth) m=-1 lockedm=-1G10: status=2(sleep) m=2 lockedm=-1G11: status=1(sleep) m=-1 lockedm=-1G12: status=1(sleep) m=-1 lockedm=-1G13: status=1(sleep) m=-1 lockedm=-1G17: status=4(timer goroutine (idle)) m=-1 lockedm=-1 摘要的部分与前边的实验输出是相似的，但是我们现在有更详细的 thread、processor、goroutine信息。从 processor开始看吧： 123P0: status=1 schedtick=10 syscalltick=0 m=3 runqsize=3 gfreecnt=0P1: status=1 schedtick=10 syscalltick=1 m=2 runqsize=3 gfreecnt=0 仍然用字母 P 表示 processor，由于GOMAXPROCS被设置为2，我们可以看到有两条关于P的信息。接下来我们看下线程： 1234567891011M3: p=0 curg=4 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0spinning=0 blocked=0 lockedg=-1M2: p=1 curg=10 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0spinning=0 blocked=0 lockedg=-1M1: p=-1 curg=-1 mallocing=0 throwing=0 gcing=0 locks=1 dying=0 helpgc=0spinning=0 blocked=0 lockedg=-1M0: p=-1 curg=-1 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0spinning=0 blocked=0 lockedg=-1 M 表示线程。由于线程数被设置为4(注：并没看到那个地方配置了这个。。。)，我们可以看到4条关于M的详细信息。从这些详细信息中可以看出哪个P与哪个M关联。 12345#在P的详情里P0的 m 值为3，表示P0与M3关联P0: status=1 schedtick=10 syscalltick=0 m=3 runqsize=3 gfreecnt=0# 反过来看M3，它的p值为0M3: p=0 curg=4 mallocing=0 throwing=0 gcing=0 locks=0 dying=0 helpgc=0spinning=0 blocked=0 lockedg=-1 这里看到了M3和P0关联到一起，P 和 M都会记录这个信息。 G表示一个goroutine。在4秒标记处，我们看到目前存在14个goroutine，自程序启动以来已创建了17个goroutine。 我们知道由于跟踪中列出的最后一个G附加的数字而创建的goroutine总数： 1G17: status=4(timer goroutine (idle)) m=-1 lockedm=-1 如果这个程序继续创建goroutines，我们会看到这个数字线性增加。 例如，如果该程序正在处理Web请求，我们可以使用这个数值来大致了解已处理的请求数。 如果程序在处理请求期间没有创建任何其他goroutine，则此估计值更接近真实的请求数。 接下来再看一下 main函数运行的 goroutine： 123G1: status=4(semacquire) m=-1 lockedm=-130 wg.Done() 我们可以看到，main函数所在的 goroutine状态是 4,阻塞在 semacquire，这是因为我们在代码中调用了 group.Wait。 为了更好地理解此跟踪中的其余goroutine，了解状态数字代表什么是有很有用的。 以下是在运行时头文件中声明的状态代码列表： 12345678910status: http://golang.org/src/runtime/Gidle, // 0Grunnable, // 1 runnable and on a run queueGrunning, // 2 runningGsyscall, // 3 performing a syscallGwaiting, // 4 waiting for the runtimeGmoribund_unused, // 5 currently unused, but hardcoded in gdb scriptsGdead, // 6 goroutine is deadGenqueue, // 7 only the Gscanenqueue is usedGcopystack, // 8 in this state when newstack is moving the stack 当我们查看我们创建的10个goroutines时，我们现在可以查看它们的状态并更好地了解每个goroutine正在做什么： 1234567891011121314151617// Goroutines running in a processor. (idleprocs=0)G4: status=2(sleep) m=3 lockedm=-1 – Thread M3 / Processor P0G10: status=2(sleep) m=2 lockedm=-1 – Thread M2 / Processor P1// Goroutines waiting to be run on a particular processor. (runqsize=3)G5: status=1(sleep) m=-1 lockedm=-1G7: status=1(sleep) m=-1 lockedm=-1G8: status=1(sleep) m=-1 lockedm=-1// Goroutines waiting to be run on a particular processor. (runqsize=3)G11: status=1(sleep) m=-1 lockedm=-1G12: status=1(sleep) m=-1 lockedm=-1G13: status=1(sleep) m=-1 lockedm=-1// Goroutines waiting on the global run queue. (runqueue=2)G6: status=1(stack growth) m=-1 lockedm=-1G9: status=1(stack growth) m=-1 lockedm=-1 通过对调度程序的基本了解以及了解程序的行为，我们可以详细了解事情的调度方式以及程序中每个处理器，线程和goroutine的状态。 结论 GODEBUG是在程序运行时窥视调度程序思维的好方法。 它可以告诉你很多关于程序的行为方式。 如果您想了解更多信息，请首先编写一些可用于预测来自调度程序的跟踪的简单程序。 在尝试查看更复杂程序的跟踪之前，了解期望的内容。 原文地址]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker基础之网络]]></title>
    <url>%2Fblog%2F2019%2F06%2F21%2F20190621005014.html</url>
    <content type="text"><![CDATA[网络基础 介绍部分docker的网络驱动 bridge网络docker0网桥 当在一台Linux系统的机器上装了Docker后，在宿主机上使用ifconfig命令可以看到多了一块名为docker0的网卡，我本地的docker0 IP是 172.17.0.0/16，宿主机上也多了一条路由： #route -n…172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0这条路由表示所有目的IP为 172.17.0.0/16的数据包都从docker0发出。使用docker创建一个容器，docker run -d busybox，进入到容器中查看网络设备信息ip addr: 123456781: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever77: eth0@if78: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 该容器网卡eth0连接在 if78这个设备上（不同测试环境可能不一样），在宿主机上执行ip addr，查看网络设备： 123456...78: veth3eb50a6@if77: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 86:5a:5e:ea:1c:60 brd ff:ff:ff:ff:ff:ff link-netnsid 3 inet6 fe80::845a:5eff:feea:1c60/64 scope link valid_lft forever preferred_lft forever... 78号设备的设备名称是 veth3eb50a6，然后再执行命令brctl show： 123...docker0 8000.0242f959ef4b no veth3eb50a6... 可以发现设备 veth3eb50a6 连在网桥docker0上。 网桥是一个二层设备，veth3eb50a6和容器中的eth0相当于一根网线的两个端口，整个过程可以看做是：用一根网线把容器和网桥docker0连了起来。当创建容器时如果不指定容器连接到的网络，容器都将连到docker0上，那么容器之间是二层互通的，可以之间相互访问。 至于上边主机上为什么添加路由信息：主机通过路由，使其能够访问容器IP，它工作在三层。 docker多网桥测试 本小节测试容器加入不同的网桥，以及它们之间的连通性。 123456789101112# 首先创建网络[root@node ~]# docker network create backend[root@node ~]# docker network create frontend# 创建容器 c1,c2,c3，其中，c1,c2接入网桥backend，c2,c3接入网桥 frontend[root@node ~]# docker run -d --name c1 --net backend busybox[root@node ~]# docker run -d --name c2 --net backend busybox[root@node ~]# docker run -d --name c3 --net frontend busybox[root@node ~]# docker network connect frontend c2# 从以上来看， c1,c2同时接入网桥backend两者二层连通，c2,c3同理接入网桥frontend。# c2上插了两根网线，分别接入了backend和frontend。 分别进入 c1 c2 c3测试另外两个容器连通性，结果如下： - c1 c2 c3 c1 - true false c2 true - true c3 false true - 符合预期。 此时在宿主机上使用 ip addr命令可以看到新增的两个网桥和ip，使用route命令可以看到对应的r转发到这两个容器的路由信息。 iptables 如果需要docker容器访问外部网络，则需要对宿主机进行配置： 允许Linux内核IP转发 sysctl net.ipv4.conf.all.forwarding=1 将iptables FORWARD的策略从DROP改为AACEPT 当访问外部时，对其做SNAT（源地址转化），对外部来说，就是宿主机在访问，外部感知不到容器的存在。 如果外部访问容器，则需要做端口映射，在启动容器时指定参数 -p 主机端口:容器端口，在iptables的nat和filter 的DOCKER链中会增加一条规则，将访问宿主机指定端口的流量转发到容器中，所以外界访问Docker容器是通过DNAT（目的地址转换）实现的。可以通过在filter的DOCKER链上添加规则来限制源IP的数据包访问容器。 overlay网络 docker中的overlay驱动使用IETF标准的VXLAN方式。 TODO 高级网络Linux NetworkNamespace TODO 容器跨主机访问方案 TODO]]></content>
      <categories>
        <category>docker</category>
        <category>docker基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s问题总结]]></title>
    <url>%2Fblog%2F2019%2F06%2F20%2F20190620225015.html</url>
    <content type="text"><![CDATA[总结遇到的问题及解决方案 设计类问题集群Master高可用 创建集群时，使用多个Master就不提了，这是基础。 一般做法是使用Keepalived陪VIP，结合health_check来做高可用，但这种做法存在一个问题：所有的rest请求都落到一个Master上，当客户端请求量比较大的时候，会导致apiserver hang死。 - 使用DNS动态解析：平安证券分享的方案（公众号上看的，公布细节有限，原话是：当Master挂掉，DNS解析可在十分钟内指向新的MasterIP），猜测他们的做法使用脚本去发现apiserver的运行状况，发现挂掉，就切换解析，这种做法在Master挂掉时，会存在无法访问apiserver服务的空窗期（十分钟左右，DNS解析生效时间），无法提供服务，而且所有请求也都落到一个Master上；另一种利用DNS的做法是DNS批量解析到所有Master上，当脚本检测到Master挂掉，然后解除到该Master的解析，虽然比前者好用一些，但在DNS删除解析生效期间，也会有一部分请求落到挂掉的Master上。 - 使用Nginx+Keepalived方案：首先所有Master上都使用docker启动一个NGINX，并配置自动重启和检查apiserver健康的策略，然后安装好keepalived并配置VIP和设置NGINX的监控检查规则。客户端向集群发送rest请求时使用的是VIP+NGINX端口，所有请求都落到VIP Master的NGINX上，然后由NGINX转发到所有Master的apiserver上。- NGINX配置apiserver健康检查，可以保证其转发的请求都能转发到正常运行的apiserver上，keepalived配置NGINX的健康检查可以保证，每次客户端请求时都能请求到NGINX，通过这么设置，实现了NGINX的高可用和apiserver的高可用，且所有到apiserver的请求不仅仅落到某一台机器上。 不同集群Pod间网络互通 正常情况下，不同集群的Pod之间是无法相互访问的 想要不同集群Pod之间网络互通，必要条件是，两者的网段不能够冲突，如果冲突，无法确定Pod要访问的地址到底是另一个集群的还是当前集群的。 使用网络插件。通过对Pod请求出来的数据进行重新封包（目标PodIP所在集群作为新包的目的IP），把数据包发送到目标Pod所在集群，然后网络插件再解包，把解包后的原始数据包发送到指定Pod，实现Pod网络互通的目地。 使用网关+路由的方式。在集群中弄几台机器用作网关，所有到该集群Pod的请求路由到这些机器，这些机器再利用kube-proxy将请求转到Pod内部，实现网络互通，但这种实现方式中，一个Pod请求到外部和外部请求进来的路径不一致。外部请求进来：proxyNode-&gt;Pod；请求到外部：Pod-&gt;Pod所在Node-&gt;外部。这种方式存在问题：对于Pod来说，每次请求的来源IP都是ProxyNode，当应用根据来源IP来做一些策略时可能存在问题。 踩坑系列apiserver请求过多，直接hang死 使用集群Master高可用方案里的第3点（参照设计类问题中的集群Master高可用部分）。 k8s 1.12以下版本kubelet使用configmap后无法删除pod 原因：kubelet目录使用了软连接，umount时报错。解决：1.升级1.12后修复bug 2. /etc/systemd/system/kubelet.service.d/10-kubeadm.conf中启动命令加上参数 –root-dir=/app/…kubelet 直接将kubelet目录改成实际目录]]></content>
      <categories>
        <category>k8s</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s问题总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker基础之资源隔离]]></title>
    <url>%2Fblog%2F2019%2F06%2F20%2F20190620225014.html</url>
    <content type="text"><![CDATA[Linux Namespace概念 Linux Namespace是Linux内核提供的一个功能，它可以隔离一系列的系统资源，比如：PID,UserID,NetWork等。 当前Linux内核一共实现了6种不同类型的Namespace: Namespace类型 系统调用参数 内核版本 Mount Namespace CLONE_NEWNS 2.4.19 UTS Namespace CLONE_NEWUTS 2.6.19 IPC Namespace CLONE_NEWIPC 2.6.19 PID Namespace CLONE_NEWPID 2.6.24 Network Namespace CLONE_NEWNET 2.6.29 User Namespace CLONE_NEWUSER 3.8 Namespace 的API主要使用如下三个系统调用： clone() 创建新进程。根据系统调用参数来判断哪些Namespace被创建，而且它们的紫禁城也会被包含到这些Namespace中 unshare() 将进程移除某个Namespace setns（） 将进程加入到Namespace中 UTS Namespace UTS Namespace主要用来隔离 nodename和domainname两个系统标识，在UTS Namespace中，每个Namespace允许有自己的hostname。 golang测试代码： 1234567891011121314151617181920212223242526// file : uts.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 在代码目录下执行命令：go run uts.go，然后进入shell交互页面。 然后执行命令echo $$，打印出当前shell的进程id，然后再执行命令 readlink /proc/$pid/ns/uts，打印出的结果与直接在terminal中执行该命令结果是不一样的，说明已实现了uts隔离。 在该shell中修改hostname：hostname -b test，然后同时在该shell中和直接在terminal中打印hostname，发现两者不一致，说明在uts隔离的shell中对uts修改不会影响宿主机。 IPC Namespace IPC Namespace 用来隔离 信号量、消息队列和共享内存，每一个IPC Namespace的这些资源都是隔离的。在上一版本的代码中略作修改： 1234567891011121314151617181920212223242526//file: ipc.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC, // 增加一个flag &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; IPC隔离测试： 首先在宿主机上打开一个shell 123456789101112131415161718192021# 查看现有 ipc message queuesroot@master:~# ipcs -q--------- 消息队列 -----------键 msqid 拥有者 权限 已用字节数 消息# 创建一个message queueroot@master:~# ipcmk -Q消息队列 id：0# 然后在查看一下root@master:~# ipcs -q--------- 消息队列 -----------键 msqid 拥有者 权限 已用字节数 消息 0xfee7b418 0 root 644 0 0 # 此时能看到一个queue了，再使用另一个shell去运行程序： go run ipc.go# ipcs -q--------- 消息队列 -----------键 msqid 拥有者 权限 已用字节数 消息 由以上实验可以发现，在新建的Namespace里，看不到宿主机上创建的queue，说明IPC Namespace创建成功，IPC已经被隔离。 PID Namespace PID Namespace是用来隔离进程ID的，同一个进程在不同的PID Namespace里可以有不同的PID，在docker容器里面，使用 ps -ef就会发现，容器内，前台运行的进程PID是1，但在容器外使用ps -ef却发现同一进程有不同的PID，这就是PID Namespace的功劳。 测试代码，在上一节代码中增加一个CloneFlag： 123456789101112131415161718192021222324252627// file: pid.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID, // add &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 同时打开两个shell，其中一个运行： go run pid.go 在宿主机shell中执行命令： ps -ef|grep pid.go可以查到另一个shell中运行的go程序的pid，然后在另一个运行go程序的shell中执行命令: echo $$可以得到当前sh的pid，可以发现，在运行go程序的shell中其PID为1，而在宿主机看到该程序的PID不为1。说明PID已隔离。 Mount Namespace Mount Namespace 用来隔离各个进程看到的挂载点视图，在不同的Namespace中，看到的文件层次是不一样的，在Mount Namespace中调用mount() umount()仅影响当前Namespace的文件系统（注：Mount有传播模式，如果是share模式还是会影响到的，测试时设置private模式可避免影响） 测试代码，在上一节基础上再增加一个CloneFlag： 123456789101112131415161718192021222324252627// file: ns.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 运行程序： go run ns.go 在程序shell中执行命令mount --make-private /proc，先修改挂载模式为private 修改 mount 之前，查看 /proc (ls /proc)目录发现里边文件有很多，然后执行命令：mount -t proc proc /proc，在查看 /proc目录，发现里边文件少了很多。此时使用 ps -ef查看系统进程，可以看到只有 sh进程和 ps -ef进程。说明Mount隔离成功。(注：/proc目录下有很多数字的目录，这些每个目录都代表一个进程，数字为进程的PID，里边存储着跟进程相关数据，Mount隔离和PID隔离配合，使得ps -ef命令查看到当前Namespace下的进程，且PID是从1开始编号的) User Namespace User Namespace主要是隔离用户的用户组ID，可以在宿主机上以一个非root用户运行创建一个User Namespace，并且在这个Namespace下创建root用户，且在namespace下有root权限。 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142// file: user.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER, UidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getuid(), Size: 1, &#125;, &#125;, GidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getgid(), Size: 1, &#125;, &#125;, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 以普通用户执行：go run user.go，在shell中输入命令id，打印出：uid=0(root) gid=0(root) 组=0(root),65534(nogroup)，但当尝试列出 /root目录时，会提示权限不够。 Network Namespace Network Namespace是用来隔离网络设备，IP地址端口等网络栈的Namespace。 测试代码：在上一节代码基础上增加CloneFlag： 123456789101112131415161718192021222324252627282930313233343536373839404142// file: network.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNET, UidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getuid(), Size: 1, &#125;, &#125;, GidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getgid(), Size: 1, &#125;, &#125;, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 在宿主机上查看网络设备，ifconfig，可以看到很多个网络设备。 执行命令：go run network.go后，在查看网络设备，发现只有 lo 一个网络设备了，说明Network Namespace隔离成功。 Linux Cgroup Linux Cgroup提供了对一组进程及将来子进程的资源限制、控制和统计的能力。 Cgroup的三个组件： cgroup是对进程分组管理的一种机制，一个cgroup包含一组进程，并可以在这个cgroup上增加subsystem的各种参数配置，将一组进程和一组subsystem的系统参数关联起来 subsystem是一组资源控制的模块，一般包含以下几项： blkio：设置对块设备输入输出的访问控制 cpu：设置cgroup中进程的cpu被调度的策略 cpuacct：可以统计cgroup中进程的cpu占用 cpuset：在多核机器上设置cgroup中进程可以使用的cpu和内存 devices：控制cgroup中进程对设备的访问 freezer：用于挂起和恢复cgroup中的进程 memory：用于控制cgroup中进程的内存占用 net_cls：用于将cgroup中进程产生的网络包分类，以便Linux的tc(traffic controller)可以根据分类区分出来自某个cgroup的包并做限流或监控 net_prio：设置cgroup中进程产生的网络流量的优先级 ns：使cgroup中的进程在新的Namespace中fork新进程时，创建一个新的cgroup，这个cgroup包含新的Namespace中的进程 hierarchy的功能是把一组cgroup串成一个树状结构，通过树状结构Cgroup可以做到集成 三个组件相互关系 系统在创建了新的hierarchy后，系统中所有进程都会加入这个hierarchy的cgroup根节点，这个cgroup根节点是hierarchy默认创建的 一个subsystem只能附加到一个hierarchy上 一个hierarchy能附加多个subsystem 一个进程可以作为多个cgroup成员，但这些cgroup必须在不同的hierarchy中 一个进程fork出子进程时，子进程和父进程是在同一个cgroup中，也可移到其他cgroup中]]></content>
      <categories>
        <category>docker</category>
        <category>docker基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker资源隔离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker基础之镜像存储]]></title>
    <url>%2Fblog%2F2019%2F06%2F16%2F20190616225014.html</url>
    <content type="text"><![CDATA[Union File System什么是Union File System Union File System简称 UnionFS，是一种为Linux，FreeBSD，NetBSD操作系统设计的，把其他文件系统联合到一个挂载点的文件系统服务。它使用branch将不同文件和目录透明地覆盖，形成一个单一一致的文件系统。这些branch都是readonly或read-write的，当对这个虚拟后的联合文件系统进行写操作时，系统是真正写到了一个新的文件中，看起来这个虚拟的联合文件系统是对任何文件进行操作的，但实际上并未改变原来的文件，unionFS使用了写时复制技术。 写时复制（copy on write，简称CoW），也叫隐式共享，是一种对可修改资源实现高效复制的资源管理技术。主要思想：如果一个资源是重复的，没有任何修改，这时并不需要立即创建一个新资源，这个资源可以被共享，一旦有写操作发生时，就会将该资源复制一份，而且写操作发生在新copy的资源上。通过这种方式，可以减少未修改资源复制带来的消耗和硬盘存储的消耗。 Aufs简介 aufs(Advanced multi layered unification filesystem)是一种支持联合挂载的文件系统，aufs将不同目录分层挂载到同一个目录下，每一层都是一个普通的文件系统。 读写操作 当需要读写一个文件时，会从最顶层开始向下寻找，本层没有，则根据层之间的关系到下一层找，直到找到第一个文件并打开它。 当写入一个文件时，如果文件不存在，则在读写层新建一个，否则向上边一个过程一样从顶层开始查找，直到找到最近的文件，aufs会将其复制到读写层进行修改。 当删除一个文件时，若文件仅存在于读写层，则可以直接删除，否则删除其在读写层的北方，再在读写层创建一个whiteout文件来标识文件不存在。 当新建一个文件时，若文件在读写层存在whiteout文件，则先删除之再新建，否则直接新建。 Device Mapper简介 Device Mapper是一种从逻辑设备到物理设备的映射框架，通过使用它，用户可以很方便地根据自己的需要制定实现存储资源的管理策略。 它主要包括3个概念：映射设备、映射表、目标设备。 说明 docker使用Device Mapper的CopyOnWrite发生在块存储级别。Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。 DeviceMapper 根据使用的基础块设备是真正的块设备哈斯稀疏文件挂载的loop设备分为两种模式：direct-lvm模式和loop-lvm模式，两者性能差别很大。 Overlay简介 Overlay主要目录结构：lower，upper，work，merge： lower：在docker中用于存放镜像分层文件，只读。可以有多个 upper：在docker中用于存放修改的文件，新增、修改、删除等，对应docker存储的 diff目录 work： merge：存放多个lower、upper合并后的文件和目录，对应docker的 merge目录 对联合挂载在merge目录下的文件操作规则： 新增：新增文件会直接将新增的文件写入到upper目录下 修改：如果对lower层中的文件修改，则会引起copy，文件copy到upper目录下，所有修改操作在upper目录下完成；如果对upper目录文件修改（包括新增的文件和已经从底层copy出的文件），则直接修改 删除：在upper目录中会对文件标记删除（使用ll查看文件标识为c --- --- ---，正常文件第一个标识位为-，目录为d），合并到merge目录下的效果为删除。 大文件修改 对lower层大文件修改时，会产生一个.file.swp（.前缀和.swp后缀）文件，修改文件在该文件操作，当执行保存时，首先将文件从lower层该文件复制到upper层中，命名为file~（在文件后加~后缀），然后将file~与.file.swp文件合并生成最终文件并删除file~。 本地测试显示，当保存文件时，新file和file~与lower层file曾同时共存。当文件从file~ 完全copy到file时，会被删除。 本地测试记录：当使用vim打开大文件时，对应的upper目录中生成了 .file.swp文件，当对其编辑时，所有修改操作都在该文件中记录（随着新增内容，swp文件大小随之改变）；当执行保存时，upper中生成了file~文件，当该文件大小不再变动时查看大小，发现与lower层中文件大小一致，说明该文件为lower层文件的副本。 此过程与Linux写大文件操作基本一致：本地生成.swp文件，然后直接将源文件重命名为file~，再合并两者为最终文件，然后删除file~。在OverlayFS中区别为，file~文件在OverlayFS中是从底层复制的[当修改upper层文件时与Linux中写大文件操作一致]，而在Linux中写大文件时file~是直接把源文件重命名的 自己动手使用Overlay 实验环境：centos7，内核：4.4.169-1.el7.elrepo.x86_64 12345678910111213141516171819202122文件路径： /app/overlayfs-test目录结构：.├── layer1│ ├── 1.txt│ └── tmp.txt├── layer2│ ├── 2.txt│ └── tmp.txt├── layer3│ ├── 3.txt│ └── tmp.txt├── layer4│ ├── 4.txt│ └── tmp.txt├── mnt├── upper└── work └── work文件内容： 1. layer&#123;n&#125;/&#123;n&#125;.txt 内容里的内容为 n2. layer&#123;n&#125;/tmp.txt 内容里的内容为 n 执行命令： 1mount -t overlay -o lowerdir=/app/overlayfs-test/layer1:/app/overlayfs-test/layer2:/app/overlayfs-test/layer3:/app/overlayfs-test/layer4,upperdir=/app/overlayfs-test/upper,workdir=/app/overlayfs-test/work overlay /app/overlayfs-test/mnt 命令格式： (格式：mount -t overlay -o lowerdir=pwd1:pwd2,upperdir=pwd3,workdir=pwd4 overlay pwd5) 格式说明：命令中 pwd{n}部分均为变量，根据自己挂载情况修改，lowerdir=pwd1:pwd2，当pwd1和pwd2中有同名文件，则显示的是pwd1中的文件。upperdir指定了upper文件的目录，workdir指定了work目录，最后一个参数指定了merge目录。 执行后的目录结构： 12345678910111213141516171819202122.├── layer1│ ├── 1.txt│ └── tmp.txt├── layer2│ ├── 2.txt│ └── tmp.txt├── layer3│ ├── 3.txt│ └── tmp.txt├── layer4│ ├── 4.txt│ └── tmp.txt├── mnt│ ├── 1.txt│ ├── 2.txt│ ├── 3.txt│ ├── 4.txt│ └── tmp.txt├── upper└── work └── work 如上， {n}.txt都挂载到了 mnt目录下，tmp.txt文件则只有一个，cat mnt/tmp.txt输出1 在mnt目录新增文件x.txt输入x，删除1.txt，编辑2.txt输入x2，新的目录结构： 1234567891011121314151617181920212223242526272829303132333435363738.├── layer1│ ├── 1.txt│ └── tmp.txt├── layer2│ ├── 2.txt│ └── tmp.txt├── layer3│ ├── 3.txt│ └── tmp.txt├── layer4│ ├── 4.txt│ └── tmp.txt├── mnt│ ├── 2.txt│ ├── 3.txt│ ├── 4.txt│ ├── tmp.txt│ └── x.txt├── upper│ ├── 1.txt│ ├── 2.txt│ └── x.txt└── work └── work``` &gt; upper目录中，执行命令：&gt; ```bash[root@node2 upper]# ll总用量 8c--------- 1 root root 0, 0 6月 16 23:36 1.txt-rw-r--r-- 1 root root 3 6月 16 23:36 2.txt-rw-r--r-- 1 root root 2 6月 16 23:36 x.txt[root@node2 upper]# cat 2.txt x2[root@node2 upper]# cat x.txt x 其中，1.txt标记为删除，2.txt内容改为x2，x.txt是新增的文件 在mnt目录下，文件1.txt也确实被删除了，x.txt文件也添加进来了，2.txt内容也改为x2。反观layer1目录下，1.txt文件还存在，layer2目录中2.txt内容仍然为2]]></content>
      <categories>
        <category>docker</category>
        <category>docker基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker镜像存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[client-go源码阅读之-controller(+简单实现)]]></title>
    <url>%2Fblog%2F2019%2F06%2F06%2F20190606154010.html</url>
    <content type="text"><![CDATA[本篇文章简单介绍controller实现过程中涉及到的client-go中的几个组件，并尝试使用简单的方式手撸一个可以实现一个类似client-go中controller功能的demo。 注：本文client-go是从kubernetes 1.13的项目源码中拷贝出来的，文中标记的代码行数以此为准。 简介 NewIndexerInformer()是定义在client-go/tools/cache/shared_informer中的一个函数，返回值为indexer,controller，本文主要讲controller的功能实现(也会简单提到indexer)，它用来监听k8s集群中的某一种资源，针对资源对象的不同事件(add/update/delete)执行用户自定义的事件处理函数。 其在k8s中的应用十分广泛 k8s里的系统组件：k8s调度器里通过监听pod资源对象来对其进行调度、k8s的kube-proxy通过监听service/endpoints资源对象的变化来配置各个节点的网络等等； CRD 使用该组件对k8s中的资源进行状态监控(deployment、daemonSet、pod等) 使用该组件对k8s中的资源配额进行监控 其他 controller 基本组成部分介绍 先讲几个在controller中引用到的几个结构体 Indexer 首先看一下如何New一个Indexer对象： 12345678910111213141516171819// client-go/tools/cache/store.go line: 112type cache struct &#123; // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc&#125;// ...// client-go/tools/cache/store.go line: 239func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer &#123; return &amp;cache&#123; cacheStorage: NewThreadSafeStore(indexers, Indices&#123;&#125;), keyFunc: keyFunc, &#125;&#125;// 注KeyFunc定义： type KeyFunc func(obj interface&#123;&#125;) (string, error) 可以看到，NewIndexer返回的是一个cache对象，而cache实现了Indexer接口。再结合cache的定义可以知道，此处的Indexer就是一个线程安全的存储，keyFunc的作用就是给定一个对象，然后返回该对象的key值。暂时简单地理解Indexer不去深究其内部其他逻辑，将其视为一个线程安全的map存储即可，这个map的key值可以通过调用keyFunc(val)获得。 ListWatch 首先看下 ListWatch的定义： 123456789// ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface.// It is a convenience function for users of NewReflector, etc.// ListFunc and WatchFunc must not be niltype ListWatch struct &#123; ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool &#125; 主要包括两个函数对象(先不去理会DisableChunking)，ListFunc作用主要是列出k8s集群中的资源对象，WatchFunc作用主要是监听k8s集群中的资源对象。 先看下创建ListWatch的代码吧： 123456789101112131415161718192021func NewFilteredListWatchFromClient(c Getter, resource string, namespace string, optionsModifier func(options *metav1.ListOptions)) *ListWatch &#123; listFunc := func(options metav1.ListOptions) (runtime.Object, error) &#123; optionsModifier(&amp;options) return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(&amp;options, metav1.ParameterCodec). Do(). Get() &#125; watchFunc := func(options metav1.ListOptions) (watch.Interface, error) &#123; options.Watch = true optionsModifier(&amp;options) return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(&amp;options, metav1.ParameterCodec). Watch() &#125; return &amp;ListWatch&#123;ListFunc: listFunc, WatchFunc: watchFunc&#125;&#125; 从代码中可用看到，这里创建了两个函数对象，ListFunc和WatchFunc，这两个函数返回的都是向apiserver发请求的对象，不过其发送请求的Action不同，一个是Get，一个是Watch，那ListWatch对象如何知道去Get/Watch哪个集群的资源对象呢，这个就是通过参数c来指定了，k8s的ClientSet实现了Getter接口，可以作为参数传进来。 到此处是不是感觉接触到了controller的核心部分了呢？到这里就算是比较接近底层的部分了，再向底层分析就是client-go向apiserver发请求和解析请求结果的部分了，深度上先到此为止（在本篇中，我们不去关心更底层是如何Watch到资源的变化情况的，也不关心数据是怎么在client和apiserver中传输和解析的只需知道ListWatch已经能够Get和Watch到集群的资源就够了）。 DeltaFIFO DeltaFIFO，从名字可以看出这是一个变化的队列，且是先进先出的队列。稍微解释一下，这个队列实际包含两层队列。先看下定义的这部分： 12345678910111213141516// client-go/tools/cache/delta_fifo.go 96行type DeltaFIFO struct &#123; //... // line：104 items map[string]Deltas queue []string // ... keyFunc KeyFunc // ...&#125;// 补充一下Deltas的定义：type Deltas []Deltatype Delta struct &#123; Type DeltaType // 字符串，Added,Updated,Deleted等 Object interface&#123;&#125;&#125; 先介绍一下 keyFunc，keyFunc和上边的Indexer里的keyFunc作用类似，都是根据对象返回一个key值，这里求得的key值主要用于 queue中的值，还有items这个map中的键值，从而，当从queue中取出队列头部元素时，可以根据这个元素值从items中取出对应的具体value 以下分析DeltaFIFO的两层队列实现：首先，queue是一个队列，数组保存队列，也很容易实现先进先出，queue队列中元素出队时，从item中取出items[queue[0]]元素并删除，queue也删除0号元素即可完成队列的Pop操作。从Deltas的定义看到，这是一个Delta数组，items[queue[0]]也是一个队列，这个队列存的是Delta对象，既然它们存储在同一个items[key]下，那么表示它们是同一个资源对象的Delta，即：这个队列表示的是同一个对象的事件队列。 综上： queue表示的是不同资源对象的队列，items的value值表示的是同一资源对象的事件队列。 有点拗口，想象一下这个场景： 假设有3个pod资源对象，记为 a,b,c， a发生变化deltaA1，此时a加入到queue队列，a的变化事件加到items[a]这个事件队列 b发生变化deltaB1，此时b假如到queue队列，b的变化事件加到items[b]这个事件队列 a又发生变化deltaA2，此时items中已经存在items[a]了，表示a已经在队列queue中了，此时，获取到items[a]事件队列，然后将这次变化加入队列此时，queue和items中的数据为： 12queue : [a,b]items : &#123;a:[deltaA1,deltaA2], b: [deltaB1]&#125; 如果上边步骤3发生时，queue已经执行过一次出队操作，那么：a发生变化deltaA2时，items中items[a]不存在，表示a不在队列中，便将a加到queue队列，deltaA2加到items[a]这个事件队列中去。此时，queue和items中的数据为： 123queue: [b,a]items: &#123;a:[deltaA2], b: [deltaB1]&#125;// 注意：items是map，其内部的`k-v`键值对不分先后顺序，只有v的值分先后顺序，因为v是数组 讲了这么多，大概应该能明白DeltaFIFO的作用了吧，简单点说就是存储k8s资源对象变化事件的队列。 controllercontroller初始化 在controller里，存储了ListWatch，objType(监听的资源对象类型)，ResourceEventHandler(资源事件handler)，DeltaFIFO，以及Process 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// client-go/tools/cache/controller.go line:345func NewIndexerInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, indexers Indexers,) (Indexer, Controller) &#123; // This will hold the client state, as we know it. // 定义Indexer（线程安全的存储） clientState := NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers) // This will hold incoming changes. Note how we pass clientState in as a // KeyLister, that way resync operations will result in the correct set // of update/delete deltas. // 定义FIFO（事件队列） fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, clientState) cfg := &amp;Config&#123; Queue: fifo, // 将ListWatch对象赋值到cfg，传递给controller ListerWatcher: lw, ObjectType: objType, FullResyncPeriod: resyncPeriod, RetryOnError: false, // 资源对象变化处理函数，obj类型是Deltas，就是上边降到DeltaFIFO中 items这个map对象的value的数据类型 // 这个函数是在 fifo.queue这个队列出队的时候调用 Process: func(obj interface&#123;&#125;) error &#123; // from oldest to newest for _, d := range obj.(Deltas) &#123; switch d.Type &#123; case Sync, Added, Updated: // 判断Indexer中是否有该元素，有则表示是更新操作，没有表示是新增操作 // 该判断逻辑一定对的前提：下边Add事件时，Indexer同步add，Delete事件时，同步delete // 此时更新Indexer中的数据，并调用 h.OnUpdate // Indexer中存储数据的更重要的一点是，本地存储新更新到来之前的最新版本资源对象，那么当更新事件到来时，客户端可以有新旧对象供handler使用 if old, exists, err := clientState.Get(d.Object); err == nil &amp;&amp; exists &#123; // update Indexer if err := clientState.Update(d.Object); err != nil &#123; return err &#125; // 调用handler事件 h.OnUpdate(old, d.Object) &#125; else &#123; // add to Indexer if err := clientState.Add(d.Object); err != nil &#123; return err &#125; // 调用handler事件 h.OnAdd(d.Object) &#125; case Deleted: // delete from Indexer if err := clientState.Delete(d.Object); err != nil &#123; return err &#125; // 调用handler事件 h.OnDelete(d.Object) &#125; &#125; return nil &#125;, &#125; return clientState, New(cfg)&#125; controller运行 controller启动方法是 controller.Run()，首先看下源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// client-go/tools/cache/controller.go line:100func (c *controller) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() go func() &#123; &lt;-stopCh c.config.Queue.Close() &#125;() r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.clock = c.clock c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group defer wg.Wait() // 此处启动一个协程，运行 r.Run函数 // r.Run即Reflector对象的Run方法，定义在 client-go/tools/cache/reflector.go line:121 // 通过查看 r.Run可以知道，它又运行了 r.ListAndWatch // r.ListAndWatch里的逻辑就是真正获取k8s资源对象 // 在该函数中又调用r.watchHandler 来监控着资源的事件，并将其加到r.store中(在本文描述场景中，r.store即为DeltaFIFO) wg.StartWithChannel(stopCh, r.Run) // 此处启动processLoop循环处理事件 wait.Until(c.processLoop, time.Second, stopCh)&#125;// 从队列(DeltaFIFO)中取出数据，执行 process（前边在NewIndexerInformer中定义的process）// c.config.Queue.Pop方法定义在：client-go/tools/cache/delta_fifo.go line:411func (c *controller) processLoop() &#123; for &#123; obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil &#123; if err == FIFOClosedError &#123; return &#125; if c.config.RetryOnError &#123; // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) &#125; &#125; &#125;&#125; 通过 controller.Run方法，整个资源状态监控的过程就完成了。 总结： 通过以上讲informer中使用到的各个组件的功能及作用，整个informer工作流程大概如下： 首先调用ListWatch中的List方法，初步将k8s中待监听资源拉到本地，将其加到本地存储Indexer和事件队列DeltaFIFO中(初始化时将这些资源的变化事件看做是ADD) 异步1：使用ListWatch中Watch方法不断去监听事件，监听到后将其加到 DeltaFIFO中 异步1：无限循环： 阻塞方法去从队列中取出数据（使用到了sync.cond） 取到的数据类型是 Deltas，调用 controller中的Process方法（上边的NewIndexerInformer里定义的） // 在Process方法里调用用户自定义的handler 以上过程讲的比较简单，实际client-go的代码中很多出错逻辑处理、同步数据、以及多协程时的数据存储优化。 手撸实现 client-go中，controller 部分代码使用了很多接口，而且封装的太深，看它的源码时没那么容易。 基于以上对 controller 的分析，自己实现了一个比较简单的 controller，弄懂这个后再去深究 client-go的实现会事半功倍。 代码地址：https://github.com/geedchin/client-go-src-learning核心部分在 02_watch中]]></content>
      <categories>
        <category>k8s</category>
        <category>功能组件</category>
        <category>client-go</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>client-go</tag>
        <tag>controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆的理解与运用]]></title>
    <url>%2Fblog%2F2019%2F05%2F23%2F20190523000154.html</url>
    <content type="text"><![CDATA[什么是堆 堆是一种完全二叉树，且每个节点都大于等于（小于等于）其子节点的值。 大顶堆：每个节点都大于等于其子节点的堆小顶堆：每个节点都小于等于其子节点的堆 如何存储堆 根据堆的定义，其为完全二叉树，则可以使用数组来存储堆。使用数组存储堆相比于链式存储有以下优点： 访问任意位置元素时间复杂度为O(1) 访问每个节点的子(父)节点耗时与链式存储几乎无差异。假如某节点在数组的索引是x,则其父节点索引为 (x+1)/2-1，左右子节点分别为(x+1)*2-1与(x+1)*2 数组在内存中按顺序存储，对CPU更友好，缓存命中高 堆的操作 以下涉及堆均默认为小顶堆，大顶堆与其类似，只是判断大小逻辑需调整 向堆中增加元素 把新的元素放至堆的末位，为使其仍然符合堆的定义，则需要对其进行调整。这个过程叫做堆化(heapify)： 将新元素与其父元素做比较，如果比父节点值小，则与父节点交换位置，以此类推，通过不断的与新父节点作比较与交换，最终到达合适的位置，正好满足堆的定义。 Go语言实现自下而上的堆化过程代码如下： 1234567891011// 自下而上的堆化func heapify(arr []int, x int) &#123; for x &gt; 0 &#123; parent := (x+1)/2 - 1 if arr[x] &lt; arr[parent] &#123; arr[x], arr[parent] = arr[parent], arr[x] &#125; x = parent &#125;&#125; 删除堆顶元素 从堆的定义中可知，对顶元素即为堆中的最小值，当删除该值时，需将剩下元素的最小值放到堆顶，而且其最小值必定是堆顶元素的子节点;将最小值移上去后，还需填充该位置以满足其完全二叉树的性质，再去从该位置的子节点中找最小元素… 最终将堆中最后一行中的某个元素移到上一层，但此时可能不满足满二叉树的性质，又要将最后一个位置的值移到该位置（移了之后又可能不满足堆的性质，又要堆化），此过程还是比较麻烦的。 如果考虑将堆顶元素删除后，直接将最后一个元素移到此位置，则只需对其自上而下堆化即可: 将堆顶元素删除，并将堆中最后一个元素放到堆顶 从堆顶开始，将堆顶视为当前元素 比较当前元素与子节点元素的大小，取出最小的，如果最小的为本身，则直接结束； 否则，交换最小的子节点元素与当前元素值， 将最小子节点作为当前节点，重复2，直到当前节点为叶子节点 Go语言实现自上而下的堆化过程代码如下： 1234567891011121314151617181920212223242526func heapify2(arr []int, n int) &#123; arrLen := len(arr) for &#123; right := (n + 1) * 2 left := right - 1 if left &gt;= arrLen &#123; break &#125; mini := n // 判断左右子节点，找出最小节点 if right &lt; arrLen &amp;&amp; arr[right] &lt; arr[mini] &#123; mini = right &#125; if arr[left] &lt; arr[mini] &#123; mini = left &#125; // 如果最小节点是当前节点，则退出 if mini == n &#123; break &#125; // 将最小位置的值与当前位置交换 arr[mini], arr[n] = arr[n], arr[mini] n = mini &#125;&#125; 如何基于堆实现排序 大致过程分为两步：建堆和排序 建堆 建堆有两种方法： 堆中元素从1个不断增加，增加到指定长度，这个过程使用了自下向上的堆化过程。时间复杂度为O(nlg(n)) 从数组中间开始遍历到数组0索引位置，对每个位置实行自上而下的堆化过程。时间复杂度O(n) 第二种建堆方法Go实现： 1234567func InitHeap(arr []int) &#123; arrLen := len(arr) for i := arrLen/2 + 1; i &gt;= 0; i-- &#123; heapify(arr, i) // 调前边的实现 &#125;&#125; 排序 不断从堆中取出最小值，取得数据的序列即为从小到大的排序结果 12345678910// 删除堆顶元素func RemoveTop(arr []int) int &#123; ret := arr[0] len := len(arr) arr[0] = arr[len-1] arr = arr[0 : len-1] heapify(arr, 0) // 调前边的实现 return ret&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>堆</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go接口中的‘陷阱’]]></title>
    <url>%2Fblog%2F2019%2F04%2F10%2F20190410140732.html</url>
    <content type="text"><![CDATA[接口值陷阱概念 接口值共有两部分组成：具体的类型、该类型的值。它们称之为接口值的动态类型和动态值。 接口值判断陷阱 既然接口值有两部分组成，那么，接口值的相等性判断是否也要类型及值均相等才算相等呢？ 答案是肯定的，以下两个示例： 123456// 公共部分type People interface &#123; &#125;type Student struct &#123; &#125;type Teacher struct &#123; &#125;// 均无方法，方便测试// 可以看作Student Teacher都实现了 People接口 12345678910// 示例1var p1, p2 Peoplevar s *Studentvar t *Teacherfmt.Println(s == nil, t == nil) // true truep1 = sp2 = tfmt.Println(p1 == p2) // false// p1 p2 值均为空，但其类型不同，因此判断相等结果为false 12345678910// 示例2var p1 Peoplevar s *Studentfmt.Println(p1 == nil) // truefmt.Println(s == nil) // truep1 = sfmt.Println(p1 == nil) // false// p1 值为nil，但类型不为nil，所以p1与nil做相等比较结果false// 此处要注意，在函数中用接口类型做为形参时，如果直接将其与nil做比较来判断其是否为空，可能会有意想不到的结果，且不易排查 接口不同实现方式 使用指针接收者实现 使用类型接收者实现 指针接收者实现 使用指针接收者实现，则只有该实体类型的指针实现了该接口。不能将实体类型的值赋值给该接口类型变量。 12345678910111213141516171819package mainimport "fmt"type People interface &#123; SayHello()&#125;type Student struct &#123;&#125;func (*Student) SayHello() &#123; fmt.Println("hello")&#125;func main() &#123; var p People // p = Student&#123;&#125; // 编译报错 p = &amp;Student&#123;&#125; p.SayHello()&#125; 类型接收者实现 能够将实体类型和实体类型的指针赋值给接口类型变量。 1234567891011121314151617181920package mainimport "fmt"type People interface &#123; SayHello()&#125;type Student struct &#123;&#125;func (Student) SayHello() &#123; fmt.Println("hello")&#125;func main() &#123; var p People p = Student&#123;&#125; p.SayHello() p = &amp;Student&#123;&#125; p.SayHello()&#125;]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>接口</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>go接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言接口简介]]></title>
    <url>%2Fblog%2F2019%2F04%2F10%2F20190410133551.html</url>
    <content type="text"><![CDATA[什么是接口 接口是一种描述一类事物具有相同操作的抽象：即某一类的事物都具有相同的操作合集。 go接口 在大多数语言中，实现某接口必须用 implement 显示地声明自己实现的是声明接口，而在go语言中，则无需声明实现接口，这种特性可称之为 duck typing。 比如，假设一只鸭子类型的接口有 游泳 呱呱叫 走路 等方法，任何一个对象如果包含这些方法，我们可以认为它是鸭子。 go接口类型 在go语言中，可以定义接口类型，它描述了一系列方法的集合，一个实现了这些方法的具体类型是这个接口类型的实例。有点抽象，可以这么理解：现在定义一个接口类型叫 Animal，其包含方法 Breath，那么所有包含 Breath方法的具体类型可视为一个Animal的实例。 接口类型还可以嵌套，例如，go自带io包里的接口 io.Writer、io.Reader、io.ReadWriter 12345678910type Writer interface&#123; Write(p []byte) (n int, err error)&#125;type Reader interface &#123; Read(p []byte) (n int, err error)&#125;type ReadWriter interface &#123; Reader Writer&#125; 实现接口的条件 一个类型如果拥有一个接口的所有方法，那么这个类型就实现了这个接口。]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>接口</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>go接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2Fblog%2F2019%2F04%2F01%2F20190401023114.html</url>
    <content type="text"><![CDATA[#逐步介绍 Dockerfile里的命令 ENV 格式： ENV &lt;key&gt; &lt;val&gt; 或 ENV &lt;key&gt;=&lt;val&gt; ... ENV指令可以为镜像创建出来的容器声明环境变量，可被后边的指令解释使用，使用格式为 $variable_name或${variable_name}。如果是带$的字符串，可使用\来转义。 FROM 格式： FROM &lt;image&gt; 或 FROM &lt;image&gt;:&lt;tag&gt; FROM指令为后边的指令提供基础镜像 COPY 格式: COPY &lt;src&gt; &lt;dest&gt; COPY指令复制&lt;src&gt;所指向的文件或目录，将它添加到新镜像中，src所指向的源必须在上下文中。 context上下文： 当构建镜像时，会执行 docker build PATH|-|URL命令，最后的PATH|-|URL即为上下文环境，在构建过程中，docker程序会将指定目录的所有文件(可用.dockerignore文件来忽略文件)复制到构建环境(实际是由docker-client提交到daemon去构建镜像)，这些文件作为构建镜像的上下文。 可同时拷贝多个文件，如 COPY src1 src2 workdir/，如果src1,src2是目录，则执行时会将src1,src2两个目录下的文件复制到 workdir/中去。 ADD 格式：ADD &lt;src&gt; &lt;dest&gt;ADD与COPY 功能上很相似，都支持将本地文件复制到镜像中，但ADD还支持其他功能，src可以是一个指向网络文件的 URL，此时若dest指向目录，则URL必须是完全路径；还可以是本地压缩归档文件，该文件复制到容器时会被解压提取，但URL中的归档文件不会被解压。 推荐使用COPY，相比之下它更透明 RUN 两种格式： RUN &lt;command&gt; shell格式 RUN [&quot;exec&quot;,&quot;param1&quot;,&quot;param2&quot;] exec格式 RUN命令会在前一条命令创建的镜像基础上创建一个容器，并在容器中运行命令，在命令结束后提交容器为新镜像，新镜像被Dockerfile中的下一条指令使用。 当使用shell格式，时，命令通过 /bin/sh -c运行，当使用exec格式时，命令是直接运行的，exec中参数会被当成json数组解析，因此必须使用双引号。因为其不在shell中执行，因此环境变量不会被替换。如果希望运行shell程序，可写成CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;progress&quot;] CMD 三种格式： CMD &lt;command&gt; shell格式 CMD [&quot;exec&quot;,&quot;param1&quot;,&quot;param2&quot;] exec格式，推荐 CMD [&quot;param1&quot;,&quot;param2&quot;] 为ENTRYPOINT指令提供参数 可以有多条CMD命令，但只有一条会被执行。前两种执行方式一致，但执行时机不通：RUN 是在构建过程中执行命令并生成新镜像，CMD 在构建时不执行而是在容器启动时作为第一条命令执行。使用第三种格式时，会将值作为参数传给ENTRYPOINT指令 ENTRYPOINT 两种格式： ENTRYPOINT &lt;command&gt; shell格式 ENTRYPOINT [&quot;exec&quot;,&quot;param&quot;] exec格式，推荐 ENTRYPOINT与CMD命令类似，都可以让容器在启动时执行相同的命令，但又有区别。ENTRYPOINT指令可以有多条，但只有最后一条有效，当使用shell格式时，会忽略所有CMD指令和docker run参数，并运行在 /bin/sh -c中，这意味着该指令进程是 /bin/sh -c的子进程，在容器中的PID将不再是1，且不能接受UNIX信号，即使使用docker stop命令，也收不到SIGTERM信号。 ONBUILD 格式： ONBUILD [INSTRUCTION] 添加一个将来执行的触发器指令到镜像中，当该镜像作为FROM指令的参数时，这些触发器指令就会在FROM指令执行时假如到构建过程中。]]></content>
      <categories>
        <category>docker</category>
        <category>docker基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s网络插件——flannel]]></title>
    <url>%2Fblog%2F2019%2F01%2F10%2F20190110150506.html</url>
    <content type="text"><![CDATA[pod容器间通信]]></content>
      <categories>
        <category>k8s</category>
        <category>功能组件</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s网络</tag>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-ha 搭建]]></title>
    <url>%2Fblog%2F2019%2F01%2F07%2F20190107123609.html</url>
    <content type="text"><![CDATA[机器准备 6台虚拟机IP地址： 10.1.1.10~12 10.1.1.20~22 内核：4.4.169-1.el7.elrepo.x86_64（3.10以上能装docker都可） 安装包晚点奉上 基本软件安装keepalived安装 略 ssh实现集群机器间互相访问 略 docker 安装 本文用的docker版本：17.06.0-ce进入 step1-docker 文件目录下，执行 install.sh然后执行以下命令测试docker是否安装成功 12systemctl start dockerdocker version 安装成功后继续下一步 基本配置 进入 step2-conf 目录下，执行 basic_conf.sh（脚本代码如下，加了注释）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/sh# 配置docker开机启动systemctl daemon-reloadsystemctl restart dockersystemctl enable docker# 关闭防火墙并关闭开机启动systemctl disable firewalldsystemctl stop firewalld# 永久关闭 selinuxsetenforce 0sed "s/^SELINUX=enforcing$/SELINUX=disabled/g" /etc/selinux/config# 关闭swap# 网络设置swapoff -ayes|cp /etc/fstab /etc/fstab.bakcat /etc/fstab.bak |grep -v swap &gt; /etc/fstabecho '''net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness=0''' &gt; /etc/sysctl.d/k8s.confmodprobe br_netfiltersysctl -p /etc/sysctl.d/k8s.conf# 配置 ipvs的依赖模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4# 安装ipset 和ipvsadmyum install -y ipsetyum install -y ipvsadm 导入系统镜像 进入 step3-images 目录下，执行脚本 loadImages.sh，主要镜像及版本如下：(貌似weave镜像还少一个，不过这个未被墙可以联网下载) 1234567891011cloud-controller-manager-1.13.1.tarkube-controller-manager-1.13.1.tarkube-proxy-1.13.1.tarkube-scheduler-1.13.1.tarkube-apiserver-1.13.1.tarcoredns-1.2.6.taretcd-3.2.24.tarpause-3.1.tarweave-kube-npc-2.5.0.tar 安装软件 进入 step4-rpm 目录下，执行脚本 install.sh ，主要软件及版本如下：master机要全装，node机器只需装 kubelet和kubeadm即可(这两个有依赖以下其他的包) 123456cri-tools-1.12.0-0.x86_64.rpmkubeadm-1.13.1-0.x86_64.rpmkubectl-1.13.1-0.x86_64.rpmkubelet-1.13.1-0.x86_64.rpmkubernetes-cni-0.6.0-0.x86_64.rpmsocat-1.7.3.2-2.el7.x86_64.rpm 配置开机启动 kubelet 12systemctl start kubeletsystemctl enable kubelet 初始化集群 进入 /root目录下，配置集群的IP信息，新建文件 cluster-info： 123456CP0_IP=10.1.1.10CP1_IP=10.1.1.11CP2_IP=10.1.1.12VIP=10.1.1.8NET_IF=ens33CIDR=10.244.0.0/16 NET_IF 是网卡，这里需要根据实际情况修改，还有主机IP地址、VIP、网段 复制 kubeha-gen.sh 到 /root 目录下，并执行等待初始化成功，三台 master 即完成初始化。界面会输出加入集群的命令：kubeadm join xxxxxx node 节点机器执行 master 机器执行成功输出的命令 kubeadm join ....，即可加入集群 kubeha-gen.sh脚本如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177#/bin/bashfunction check_parm()&#123; if [ "$&#123;2&#125;" == "" ]; then echo -n "$&#123;1&#125;" return 1 else return 0 fi&#125;if [ -f ./cluster-info ]; then source ./cluster-info ficheck_parm "Enter the IP address of master-01: " $&#123;CP0_IP&#125; if [ $? -eq 1 ]; then read CP0_IPficheck_parm "Enter the IP address of master-02: " $&#123;CP1_IP&#125;if [ $? -eq 1 ]; then read CP1_IPficheck_parm "Enter the IP address of master-03: " $&#123;CP2_IP&#125;if [ $? -eq 1 ]; then read CP2_IPficheck_parm "Enter the VIP: " $&#123;VIP&#125;if [ $? -eq 1 ]; then read VIPficheck_parm "Enter the Net Interface: " $&#123;NET_IF&#125;if [ $? -eq 1 ]; then read NET_IFficheck_parm "Enter the cluster CIDR: " $&#123;CIDR&#125;if [ $? -eq 1 ]; then read CIDRfiecho """cluster-info: master-01: $&#123;CP0_IP&#125; master-02: $&#123;CP1_IP&#125; master-03: $&#123;CP2_IP&#125; VIP: $&#123;VIP&#125; Net Interface: $&#123;NET_IF&#125; CIDR: $&#123;CIDR&#125;"""echo -n 'Please print "yes" to continue or "no" to cancel: 'read AGREEwhile [ "$&#123;AGREE&#125;" != "yes" ]; do if [ "$&#123;AGREE&#125;" == "no" ]; then exit 0; else echo -n 'Please print "yes" to continue or "no" to cancel: ' read AGREE fidonemkdir -p ~/ikube/tlsIPS=($&#123;CP0_IP&#125; $&#123;CP1_IP&#125; $&#123;CP2_IP&#125;)PRIORITY=(100 50 30)STATE=("MASTER" "BACKUP" "BACKUP")HEALTH_CHECK=""for index in 0 1 2; do HEALTH_CHECK=$&#123;HEALTH_CHECK&#125;""" real_server $&#123;IPS[$index]&#125; 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;"""donefor index in 0 1 2; do ip=$&#123;IPS[$&#123;index&#125;]&#125; echo """global_defs &#123; router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state $&#123;STATE[$&#123;index&#125;]&#125; interface $&#123;NET_IF&#125; virtual_router_id 80 priority $&#123;PRIORITY[$&#123;index&#125;]&#125; advert_int 1 authentication &#123; auth_type PASS auth_pass just0kk &#125; virtual_ipaddress &#123; $&#123;VIP&#125;/24 &#125;&#125;virtual_server $&#123;VIP&#125; 6443 &#123; delay_loop 6 lb_algo loadbalance lb_kind DR nat_mask 255.255.255.0 persistence_timeout 0 protocol TCP$&#123;HEALTH_CHECK&#125;&#125;""" &gt; ~/ikube/keepalived-$&#123;index&#125;.conf scp ~/ikube/keepalived-$&#123;index&#125;.conf $&#123;ip&#125;:/etc/keepalived/keepalived.conf ssh $&#123;ip&#125; " systemctl stop keepalived systemctl enable keepalived systemctl start keepalived kubeadm reset -f rm -rf /etc/kubernetes/pki/"doneecho """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.1controlPlaneEndpoint: "$&#123;VIP&#125;:6443"apiServer: certSANs: - $&#123;CP0_IP&#125; - $&#123;CP1_IP&#125; - $&#123;CP2_IP&#125; - $&#123;VIP&#125;networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: $&#123;CIDR&#125;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs""" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yamlmkdir -p $HOME/.kubecp -f /etc/kubernetes/admin.conf $&#123;HOME&#125;/.kube/config# 加入其它节点JOIN_CMD=`kubeadm token create --print-join-command`for index in 1 2; do ip=$&#123;IPS[$&#123;index&#125;]&#125; ssh $ip "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/" scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $ip:~/.kube/config ssh $&#123;ip&#125; "$&#123;JOIN_CMD&#125; --experimental-control-plane"doneecho "Cluster create finished." 网络插件安装 进入weave目录下，执行 kubectl apply -f weave.yml]]></content>
      <categories>
        <category>k8s</category>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s高可用搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine(1)]]></title>
    <url>%2Fblog%2F2019%2F01%2F06%2F20190106083354.html</url>
    <content type="text"><![CDATA[什么是协程 介绍协程之前，首先回顾一下进程与线程 进程与线程 进程： 通俗点讲，进程就是一个程序运行的实例，进程拥有自己运行时打开的各种资源和独立的内存空间。每个进程通过PCB（process control block）来保存自己的基本信息（在Linux中，该结构叫task_struct）PCB包含了进程运行的基本信息：标示符、状态、优先级、程序计数器、内存指针、上下文、IO信息、记账信息等 线程： 线程属于进程，是程序的实际执行者，一个程序可以包含多个线程，线程具有自己的栈空间，其基本信息存储在TCB（thread control block） 对OS来说，进程是最小的资源管理单元，线程是最小的执行单元 线程切换比进程切换更加轻量：线程进程切换时，都会调用内核切换上下文，不同进程之间内存空间相互独立，而不同线程之间共享进程的内存空间，因此线程上下文切换效率更高 进程和线程的切换都是抢占式的，由OS管理 go语言协程 协程,是一种比线程更轻量级的运行单元。主要特点： 轻量级“线程” 非抢占式多任务处理，由协程调度器来调度任务也可主动交出控制权 编译器、解释器、虚拟机层面的多任务 多个协程可以在一个或多个线程上运行 非抢占式测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import ( "testing" "fmt" "time")// 结束时间大于2秒，因为是非抢占式，main的协程无法及时得到执行// 打印属于IO操作，执行打印时会释放CPU资源给其他协程使用func Test_01(t *testing.T) &#123; for i := 0; i &lt; 100; i++ &#123; go func(x int) &#123; for &#123; fmt.Println(x) &#125; &#125;(i) &#125; time.Sleep(time.Second * 2)&#125;// 该程序不会退出，执行到最后一行打印之前，启动了4个协程，这些协程均不会释放CPU资源，没有空闲的线程用于主协程执行，因此不会退出// 当启动的协程数 &lt; runtime.NumCPU() ，可以正常退出func Test_02(t *testing.T) &#123; // N值要大于等于机器支持线程数（大于等于runtime.NumCPU()） // 测试机为2核4线程，这里设置成4 const N = 4 var a [N]int for i := 0; i &lt; N; i++ &#123; go func(x int) &#123; for &#123; a[x]++ &#125; &#125;(i) &#125; // 此处休眠1秒是为了让前边循环中的协程都启动 time.Sleep(time.Second) fmt.Println(a)&#125;// 主动交出控制权func Test_03(t *testing.T) &#123; // const N = 10 var a [N]int for i := 0; i &lt; N; i++ &#123; go func(x int) &#123; for &#123; a[x]++ // 主动交出控制权 runtime.Gosched() &#125; &#125;(i) &#125; time.Sleep(time.Second) fmt.Println(a)&#125; 任何函数只要加上go就可以交给调度器执行 调度器会在指定的点进行切换 可能切换点：（可能切换） IO,select channel 等待锁 函数调用(当协程运行过长时会打标记，在函数调用时检测标记并根据标记决定是否调度) runtime.Gosched() 系统调用 可以通过使用 -race参数来进行数据多线程的读写竞争进行检测 go语言协程模型 模型简介 如下图，go语言协程的模型 M：系统线程（内核线程） P：执行Go协程所必须的资源（上下文环境，逻辑处理器） G：Go协程 可以把Go协程视为“用户线程”，它由用户态的Go调度器来实现在P上的调度，由于P中的协程队列同处于一个上下文环境（在同一个线程）没有内核切换的开销。调度器通过将Go协程映射到内核线程中，使程序运行时“看起来”是在并发执行。 由于Go协程是非抢占式的，而且基本在指定的几种情况下才会释放CPU，所以在协程之间切换时程序上下文复制开销较小且不用进行内核调用，因此更轻量级。 模型 在并发执行程序时，调度器除了正常从协程队列里取出协程放到自己的执行队里中外，还有其他的执行调度行为： Work-sharing：当处理器生成新线程时，它会尝试着将一些工作任务交给其他闲置的线程执行，充分利用资源 Work-stealing：调度器主动“偷取”其他线程上执行任务来执行 在Go语言中有一个global goroutine queue，每个内核线程又有一个local goroutine queue， 对于每一个内核线程来说，在该线程上等待执行的协程是串行的，而对于外部来说，这些协程是并发的。 协程之于内核线程，有点像进程之于CPU（单核CPU上对于CPU来说进程是串行的，对于外部来说是并行的）]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[列表(list)——可快速增删的非连续空间的容器]]></title>
    <url>%2Fblog%2F2018%2F12%2F23%2F20181223104815.html</url>
    <content type="text"><![CDATA[基础基础结构12345678910111213141516171819202122// Element is an element of a linked list.type Element struct &#123; // Next and previous pointers in the doubly-linked list of elements. // To simplify the implementation, internally a list l is implemented // as a ring, such that &amp;l.root is both the next element of the last // list element (l.Back()) and the previous element of the first list // element (l.Front()). next, prev *Element // The list to which this element belongs. list *List // The value stored with this element. Value interface&#123;&#125;&#125;// List represents a doubly linked list.// The zero value for List is an empty list ready to use.type List struct &#123; root Element // sentinel list element, only &amp;root, root.prev, and root.next are used len int // current list length excluding (this) sentinel element&#125; Element，封装保存在list中的值， Element中的变量next,prev是指针，分别指向当前元素的下一个元素与前一个元素 Element中的变量list是指针，指向该Element所属的list Value存储的为List中存放的值 List，list本身 List中的变量root变量存储着List中的第一个元素，该元素Value为nil，只做为根，不存储数据 List中的变量len变量表示List的长度 本质就是数据结构中的双向循环链表 主要操作：Remove(e *Element) 移除元素 PushFront(v interface{}) 在列表头部增加元素 PushBack(v interface{}) 在列表尾部增加元素 InsertBefore(v interface{},mark *Element)*Element 在mark前增加元素 InsertAfter(v interface{},mark *Element)*Element 在mark后增加元素 MoveToFront(e *Element) 将元素移至列表头部 MoveToBack(e *Element) 将元素移至列表尾部 MoveBefore(e mark *Element) 将元素e移至mark前边 MoveAfter(e mark *Element) 将元素e移至mark后边 PushBackList(other *List) 将列表other放入当前列表尾部 PushFrontList(other *List) 将列表other放至当前列表头部]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sync.Map 底层实现分析]]></title>
    <url>%2Fblog%2F2018%2F12%2F17%2F20181217141509.html</url>
    <content type="text"><![CDATA[sync.Map的数据结构 首先看一下sync.Map的底层数据结构 123456789101112131415161718192021222324252627282930type Map struct &#123; // 互斥锁，锁定 dirty mu Mutex // 优先读 read atomic.Value // 保存最新map dirty map[interface&#123;&#125;]*entry // 记录从read中读取不到数据次数 // 当misses&gt;=len(dirty)时，会将dirty作为read,然后dirty=nil // (源码中 missLocked函数) misses int &#125;type readOnly struct &#123; // 主要用于存储 m map[interface&#123;&#125;]*entry // 如果数据在dirty中但没有在read中，该值为true,作为修改标识 amended bool &#125;type entry struct &#123; // nil: 表示为被删除，调用Delete()可以将read map中的元素置为nil // expunged: 也是表示被删除，但是该键只在read而没有在dirty中，这种情况出现在将read复制到dirty中，即复制的过程会先将nil标记为expunged，然后不将其复制到dirty // 其他: 表示存着真正的数据 p unsafe.Pointer // *interface&#123;&#125;&#125; mu 用于数据存取数据加锁 read dirty misses sync.Map的操作存数据，Store(key,val)1234567891011121314151617181920212223242526272829// Store sets the value for a key.func (m *Map) Store(key, value interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123; return &#125; m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok &#123; if e.unexpungeLocked() &#123; // The entry was previously expunged, which implies that there is a // non-nil dirty map and this entry is not in it. m.dirty[key] = e &#125; e.storeLocked(&amp;value) &#125; else if e, ok := m.dirty[key]; ok &#123; e.storeLocked(&amp;value) &#125; else &#123; if !read.amended &#123; // We're adding the first new key to the dirty map. // Make sure it is allocated and mark the read-only map as incomplete. m.dirtyLocked() m.read.Store(readOnly&#123;m: read.m, amended: true&#125;) &#125; m.dirty[key] = newEntry(value) &#125; m.mu.Unlock()&#125; 首先判断是否在map.read中，如果在，则调用entry的tryStore方法，tryStore方法使用了atomic.CompareAndSwapPointer操作，保证了操作原子性。如果成功，则直接返回①。当该key值已删去时tryStore方法返回false继续执行② 加锁，重新获取map.read值保证锁定操作期间数据最新且不会发生变化。 如果key存在于map.read中： 如果read中数据被删除③，则将k-v放到dirty中(dirty中没有该key) 如果read中数据未被删除④不处理 将v放到read中 如果key不存在于map.read中，存在于dirty中⑤，更新dirty中的值 如果key既不存在于map.read中，也不存在于map.dirty中： read与dirty中key一致（amended为false）⑥，初始化read，并标记amended参数为true，将数据存到dirty中 否则不做处理 ⑦ 将数据存入dirty中 解锁返回 删除数据 Delete(key)1234567891011121314151617// Delete deletes the value for a key.func (m *Map) Delete(key interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; delete(m.dirty, key) &#125; m.mu.Unlock() &#125; if ok &#123; e.delete() &#125;&#125; 判断如果待删除的key不在map.read中且map.dirty中含有map.read中不存在的key，则： 锁定，重新判断map.read中是否有key，如果同上，则调用从map.dirty中删除的函数，然后解锁 否则，若key存在于map.read中，则调用entry中的delete（entry里只有一个参数，应该是val的指针） 访问map数据 Load(key)123456789101112131415161718192021222324252627// Load returns the value stored in the map for a key, or nil if no// value is present.// The ok result indicates whether value was found in the map.func (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; m.mu.Lock() // Avoid reporting a spurious miss if m.dirty got promoted while we were // blocked on m.mu. (If further loads of the same key will not miss, it's // not worth copying the dirty map for this key.) read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; e, ok = m.dirty[key] // Regardless of whether the entry was present, record a miss: this key // will take the slow path until the dirty map is promoted to the read // map. m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125; 判断如果待访问的key不在map.read中且map.dirty中含有map.read中不存在的key，则 对map加锁，再次判断为真，则调用map.missLocked()，（通过判断在map.read中miss的次数来决定是否将map.dirty替换掉map.read） 解锁 判断前边是否获得到val值，未获取到，则返回 nil,false 若前边取到val，则返回 LoadOrStore(key,val) 如果map中存在key，则返回对应的val 否则，保存参数中的key,val，并返回参数中的val 遍历Map: range123456789101112131415161718package mainimport ( "fmt" "sync")func main() &#123; m := sync.Map&#123;&#125; m.Store("k1", "v1") m.Store("k2", "v2") m.Range(func(k, v interface&#123;&#125;) bool &#123; fmt.Println(k, v) return true &#125;)&#125; 总结map.read与map.dirty中数据一致性： map.dirty包含map.read中没有的key，map.read.amended为false，表明两者底层val引用是一致的，此时对数据的 更新、删除（此时删除操作实际是将底层val的指针置位nil，相当于只删除了val未删除key） 做cas操作即可 否则，map.read.amended为true，此时操作数据要考虑map.read与map.dirty中数据一致性 load数据时，在map.read中访问不到次数大于等于map.dirty长度时，会用map.dirty替换map.read，并将map.dirty置为nil，此时map.read.amended为false（因为map.dirty中不包含map.read中不存在的key） store数据时，当map.dirty数据为空时，会将map.read中的key的val引用复制过来（此时会判断是否标记未删除，标记删除则不复制，删除了val的key也删除掉），再在map.dirty中增加k-v，此时map.read.amended为true]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[map基础]]></title>
    <url>%2Fblog%2F2018%2F12%2F16%2F20181216153850.html</url>
    <content type="text"><![CDATA[mapmap实现方式 go语言中map使用hash（散列表）实现 定义方法12345678// 声明一个 map 变量，其值为nil，(m1==nil)为truevar m1 map[int]int// 创建一个空map （可在&#123;&#125;中填写map中的元素）m2 := map[int]int&#123;&#125;// 创建一个空mapm3 := make(map[int]int) map的key类型限制 map使用hash表存储实现，key类型必须可比较相等。map的key类型必须不为function,slice,map，在定义时，如果使用这些类型会报错。struct类型也可作为key类型，但如果使用带有这些类型属性的struct不可以作为key。可能编译不会报错，但运行时会报错。 Map操作map长度 获取长度方法：len(map) 访问测试代码1234567891011121314151617181920212223package mainimport "fmt"func main() &#123; m := map[int]interface&#123;&#125;&#123; 0: 0, 1: 2, 2: 4, 3: 6, 5: nil, &#125; // 直接访问不存在的元素 m4v := m[4] fmt.Println(m4v) // 使用ok参数接收是否取得正确的值 m4v, ok := m[4] fmt.Println(m4v, ok) // 使用ok参数接收是否取得正确的值 m5v, ok := m[5] fmt.Println(m5v, ok)&#125; 测试结果：123&lt;nil&gt;&lt;nil&gt; false&lt;nil&gt; true 注意： 不使用ok参数接收是否取得值时，如果不存在key值对应的val，则返回对应类型的空：int返回0，string返回&quot;&quot;，引用类型返回nil等。 使用ok参数可以通过判断ok知道map中是否存在key 不使用ok时可能存在的问题，如上代码，m[4]不存在于map中，取得结果为nil，m[5]存在于map中，其本身对应的值为nil所以取得结果也为nil，这种情况下可能会存在问题 遍历测试代码12345678910111213141516package mainimport "fmt"func main() &#123; m := map[int]int&#123; 0: 0, 1: 2, 2: 4, 3: 6, &#125; for k, v := range m &#123; fmt.Println(k, v) &#125;&#125; 注意： 每次运行程序输出顺序可能不一致，map不能保证遍历的顺序性 删除元素 删除方法 delete 测试代码1234567891011121314151617package mainimport "fmt"func main() &#123; m := map[int]int&#123; 0: 0, 1: 2, 2: 4, &#125; delete(m, 0) _, ok := m[0] fmt.Println(ok) // 测试删除不存在的元素，不会报错 delete(m,5)&#125; 测试结果 输出：false 安全map——sync.Map 普通map类型在只读的情况下是线程安全的，但同时读写时非线程安全。 普通线程安全性普通map线程安全测试12345678910111213141516171819202122package mainfunc main() &#123; m := make(map[int]int) // 无限向map写数据 go func() &#123; for &#123; m[1] = 1 &#125; &#125;() // 无限从map读数据 go func() &#123; for &#123; _ = m[1] &#125; &#125;() // 让主线程不退出，并发程序后台执行 for true &#123; _ = 1 &#125;&#125; 测试结果:报错1fatal error: concurrent map read and map write sync.Map操作添加元素 方法：Store(key,val)；LoadOrStore(key,val) Store(key,val)方法，将key:val对存入map中； LoadOrStore(key,val)方法，返回值有两个 (val,ok)，val表示在map中key对应的值（如果已存在，则为原map中的值，如果不存在，则先将key:val对存入map，再返回val）；ok表示map中是否已存在key值及其对应的val。 删除元素 方法： Delete(key) 删除map中key及其对应的val 获取元素 方法： Load(key)；LoadOrStore(key,val) 直接调用Load方法，返回值有两个(val,ok)，值以及是否存在 LoadOrStore方法在添加元素部分已解释 遍历元素 方法： Range(f func(key, value interface{}) bool) 参数是一个函数，在函数中执行遍历操作]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-slice操作]]></title>
    <url>%2Fblog%2F2018%2F12%2F16%2F20181216012715.html</url>
    <content type="text"><![CDATA[添加元素方法：append(slice,elems...) 测试12345678910111213141516171819202122package mainimport "fmt"func main() &#123; // 未触发扩容 arr := []int&#123;0,1,2,3,4,5,6,7,8,9&#125; s := arr[0:2] news := append(s, 100) fmt.Printf("news ptr : %p\ts ptr : %p\n",news,s) fmt.Println("s\t",s) fmt.Println("news\t",news) fmt.Println("arr\t",arr) fmt.Println("====================") // 触发扩容 arr = []int&#123;0,1,2&#125; s = arr[:] news = append(s,100) fmt.Printf("news ptr : %p\ts ptr : %p\n",news,s) fmt.Println("s\t",s) fmt.Println("news\t",news) fmt.Println("arr\t",arr)&#125; 测试结果：123456789news ptr : 0xc00007a0a0 s ptr : 0xc00007a0a0s [0 1]news [0 1 100]arr [0 1 100 3 4 5 6 7 8 9]====================news ptr : 0xc000072030 s ptr : 0xc00004a0e0s [0 1 2]news [0 1 2 100]arr [0 1 2] 结论： append方法会创建一个新slice，原slice不变。 调用append方法如果未触发扩容，则底层array对应位置会做出相应改变 调用append方法如果触发扩容，则原底层array对应位置不发生变化，因为新的slice底层array也已变化 复制元素方法：copy(dst []type,src []type) 测试：12345678910111213141516171819package mainimport "fmt"func main() &#123; s1 := make([]int,1) s2 := []int&#123;1,2,3&#125; // len(src) &gt; len(dst) fmt.Println("before\tdst :",s1,"src :",s2) copy(s1,s2) fmt.Println("after\tdst :",s1,"src :",s2) fmt.Println("=====================") // len(src) &lt; len(dst) s1 = make([]int,1) fmt.Println("before\tdst :",s2,"src :",s1) copy(s2,s1) fmt.Println("after\tdst :",s2,"src :",s1)&#125; 测试结果：12345before dst : [0] src : [1 2 3]after dst : [1] src : [1 2 3]=====================before dst : [1 2 3] src : [0]after dst : [0 2 3] src : [0] 结论： 将src数据拷贝到dst中 如果src长度大于dst长度，则只会复制len(dst)个数据到dst中 如果src长度小于dst长度，则dst的前len(src)个数据会被src的数据覆盖，后边不变 删除操作 无默认删除操作，可通过append实现 测试123456789101112131415package mainimport "fmt"func main() &#123; s1 := []int&#123;0,1,2,3,4,5&#125; fmt.Printf("before del s1's ptr :\t%p\n",s1) fmt.Println(s1) fmt.Println("=============") // do delete s1 = append(s1[:2],s1[3:]...) fmt.Printf("after del s1's ptr :\t%p\n",s1) fmt.Println(s1)&#125; 测试结果：12345before del s1&apos;s ptr : 0xc000072030[0 1 2 3 4 5]=============after del s1&apos;s ptr : 0xc000072030[0 1 3 4 5] 结论： 删除元素slice长度只会减小不会增大，因此底层数组不会发生变化。 修改方法：无方法，直接通过 slice[i] = val 修改测试： 略 遍历方法： for i,v := range slice遍历测试：1234567891011121314151617181920212223package mainimport "fmt"func main() &#123; s := []int&#123;0, 2, 4, 6, 8, 10&#125; // 只用 index for i := range s &#123; fmt.Printf("%d,", i) &#125; fmt.Println() // index,value都用 for i, v := range s &#123; fmt.Printf("%d:%d,\t", i, v) &#125; fmt.Println() // 只用value for _, v := range s &#123; fmt.Printf("%d,", v) &#125; fmt.Println()&#125; 输出1230,1,2,3,4,5,0:0, 1:2, 2:4, 3:6, 4:8, 5:10, 0,2,4,6,8,10,]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go slice原理]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2F20181215204437.html</url>
    <content type="text"><![CDATA[原理 go语言slice是底层数组的一个视图。slice主要参数有3个: ptr指向slice的首个元素； len表示slice长度； cap表示slice容量。 当向slice里append元素的时候，如果元素个数大于容量的某个百分比，将会扩容。其ptr值也会变。slice扩容的原理其实就是更换该slice底层array数组（因为slice只是某一数组的视图） 定义时与数组的区别 array与slice的定义 12345678910// array定义array1 := [3]int&#123;1,2,3&#125;array2 := [...]int&#123;1,2,3&#125;// slice定义slice1 := []int&#123;1,2,3&#125;slice2 := make([]int,0,10)slice3 := array1[:] // array1 := [3]int&#123;1,2,3&#125;slice4 := slice1[:]var slice5 = []int 测试测试1 测试slice与array定义区别 12345678910111213141516171819202122232425package mainimport ( "fmt" "reflect")func main() &#123; array1 := [3]int&#123;1,2,3&#125; array2 := [...]int&#123;1,2,3&#125; slice1 := []int&#123;1,2,3&#125; slice2 := make([]int,5,10) slice3 := array1[:] slice4 := slice1[:] var slice5 []int fmt.Printf("array1 type : %v\n",reflect.TypeOf(array1).Kind()) fmt.Printf("array2 type : %v\n",reflect.TypeOf(array2).Kind()) fmt.Println("===================") fmt.Printf("slice1 type : %v\n",reflect.TypeOf(slice1).Kind()) fmt.Printf("slice2 type : %v\n",reflect.TypeOf(slice2).Kind()) fmt.Printf("slice3 type : %v\n",reflect.TypeOf(slice3).Kind()) fmt.Printf("slice4 type : %v\n",reflect.TypeOf(slice4).Kind()) fmt.Printf("slice5 type : %v\n",reflect.TypeOf(slice5).Kind())&#125; 输出结果： 12345678array1 type : arrayarray2 type : array===================slice1 type : sliceslice2 type : sliceslice3 type : sliceslice4 type : sliceslice5 type : slice 测试2 测试slice扩容条件 123456789101112131415161718package mainimport "fmt"func main() &#123; tmp := [...]int&#123;0, 1, 2, 3&#125; arr := tmp[1:] Cap := cap(arr) lastCap := Cap for i := 0; i &lt; 20000; i++ &#123; arr = append(arr, 0) Cap = cap(arr) if Cap == lastCap &#123; continue &#125; fmt.Printf("%p\tlen:%v\t\tlastCap:%v\t\tcap:%v\n", arr, len(arr), lastCap, Cap) lastCap = Cap &#125;&#125; 输出结果 123456789101112131415161718190xc000072030 len:4 lastCap:3 cap:60xc00003e060 len:7 lastCap:6 cap:120xc00008a000 len:13 lastCap:12 cap:240xc00008c000 len:25 lastCap:24 cap:480xc00008e000 len:49 lastCap:48 cap:960xc000090000 len:97 lastCap:96 cap:1920xc000092000 len:193 lastCap:192 cap:3840xc000098000 len:385 lastCap:384 cap:7680xc00009e000 len:769 lastCap:768 cap:15360xc0000a4000 len:1537 lastCap:1536 cap:20480xc0000a8000 len:2049 lastCap:2048 cap:25600xc0000b2000 len:2561 lastCap:2560 cap:34080xc0000c6000 len:3409 lastCap:3408 cap:51200xc0000d0000 len:5121 lastCap:5120 cap:71680xc0000de000 len:7169 lastCap:7168 cap:92160xc0000f0000 len:9217 lastCap:9216 cap:122880xc000108000 len:12289 lastCap:12288 cap:153600xc000126000 len:15361 lastCap:15360 cap:194560xc00014c000 len:19457 lastCap:19456 cap:24576 结论： 当扩容时，ptr指针变（即slice这个视图所在的array发生变化） 只有当cap大小等于len的时候才会扩容，扩容大小视cap大小而定：cap较小时，直接扩容一倍，稍大时，扩容比例较小 测试3 测试扩容前后的slice与原slice有什么不同以及两个相同的slice执行某些操作后，另一个的变化情况 12345678910111213141516171819202122232425package mainimport "fmt"func main() &#123; arr := make([]int,2,16) arr2 := arr // 扩容前两者地址一样，修改一个，另一个值也修改 arr[0] = 5 fmt.Printf("arr\t%p\t[0] : %v\tlen : %v\tcap : %v\n", arr, arr[0], len(arr), cap(arr)) fmt.Printf("ar2\t%p\t[0] : %v\tlen : %v\tcap : %v\n\n", arr2, arr2[0], len(arr2), cap(arr2)) // 扩容前，当一个增加元素时，另一个slice长度不变 arr = append(arr, 0) fmt.Printf("arr\t%p\t[0] : %v\tlen : %v\tcap : %v\n", arr, arr[0], len(arr), cap(arr)) fmt.Printf("ar2\t%p\t[0] : %v\tlen : %v\tcap : %v\n\n", arr2, arr2[0], len(arr2), cap(arr2)) // 循环，令arr扩容 for i := 0; i &lt; 100; i++ &#123; arr = append(arr, 0) &#125; // 扩容后，arr变成一个新的slice，修改其中一个slice的值，另一个不会变 fmt.Printf("arr\tptr : %p\tlen : %v\tcap : %v\n", arr, len(arr), cap(arr)) fmt.Printf("ar2\tptr : %p\tlen : %v\tcap : %v\n\n", arr2, len(arr2), cap(arr2)) arr[0] = 100 fmt.Printf("arr\t[0] : %v\n", arr[0]) fmt.Printf("ar2\t[0] : %v\n", arr2[0])&#125; 输出结果 1234567891011arr 0xc00007e080 [0] : 5 len : 2 cap : 16ar2 0xc00007e080 [0] : 5 len : 2 cap : 16arr 0xc00007e080 [0] : 5 len : 3 cap : 16ar2 0xc00007e080 [0] : 5 len : 2 cap : 16arr ptr : 0xc00008c000 len : 103 cap : 128ar2 ptr : 0xc00007e080 len : 2 cap : 16arr [0] : 100ar2 [0] : 5 结论 未发生扩容，两个slice相同，改变其中一个slice，另一个也变 未发生扩容，向其中一个增加元素，另一个len不变 发生扩容，新生成slice，地址变，改变其中一个slice另一个不变 测试4 测试对于两个引用同一个数组但len不同的slice，能否取到超出自身len的元素 1234567891011package mainimport "fmt"func main() &#123; tmp := [...]int&#123;0,1,2,3,4,5,6,7&#125; s1 := tmp[2:6] // s1 : [2,3,4,5] fmt.Println(len(s1)) // s1 长度只有1 s2 := s1[3:6] // s2 : [5,6,7] // fmt.Println(s2[3]) // 报错 fmt.Println(s2)&#125; 输出 124[5 6 7] 结论： 如实例，s1长度为1，使用[:]操作，能取到后边的数字 如实例，s1长度为1，无法使用s1[1]取到对应位置的数据]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客next主题侧边菜单栏设置]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2F20181215023114.html</url>
    <content type="text"><![CDATA[增加菜单 增加菜单 tags、categories、等 123# 创建页面hexo new page tagshexo new page categories 增加链接123456# 在主题config里去掉menu里相关注释menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th 修改新建的tags、categories目录下的md文件 在md文件中增加type 属性 123456---title: tagsdate: 2018-12-15 02:22:50type: &quot;tags&quot;--- 使用标签和分类 在post里增加tags和categories属性 123456789---title: 博客next主题侧边菜单栏设置date: 2018-12-15 02:31:14tags: - github博客categories: - github博客--- 注意：tags、categories均可用数组[1,2,3]的形式代替列表形式。tags列表里内容同级，categories列表里内容不同级 永久链接 主要在blog根目录下的_config.yml文件中配置，默认配置是 年/月/日/标题/，用标题作为url一部分不是很合理，如果标题发生改变，那链接也跟着变化了。 所以要使用一个唯一标示且不会发生改变的值作为url。hexo自带的有id，可使用id，但id生成规则不是很清楚，测试过修改题目后，id也发生变化。 本网站使用的方法是：给每个md文件增加一个属性，记为pid，然后使用pid作为url一部分。 首先，修改blog根目录下的_config.yml文件如下： 1234# 永久化链接。（最后加 .html 有利于搜索引擎收录）permalink: blog/:year/:month/:day/:pid.htmlpermalink_defaults: pid: default 然后，修改新增post时的模板文件blog/scaffolds/post.md，增加pid属性，这里把pid设置成时间了，直接用date在转化后不是我们想要的格式，这里就用这种笨方法。或者留白，新建文件后自己填写也可。 使用这种方法，pid是与文章内容无任何关系的属性，只要我们不修改pid和永久链接规则，文章的链接就不会变。 12345678---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;pid: &#123;&#123;date[0]&#125;&#125;&#123;&#123;date[1]&#125;&#125;&#123;&#123;date[2]&#125;&#125;&#123;&#123;date[3]&#125;&#125;&#123;&#123;date[5]&#125;&#125;&#123;&#123;date[6]&#125;&#125;&#123;&#123;date[8]&#125;&#125;&#123;&#123;date[9]&#125;&#125;&#123;&#123;date[11]&#125;&#125;&#123;&#123;date[12]&#125;&#125;&#123;&#123;date[14]&#125;&#125;&#123;&#123;date[15]&#125;&#125;&#123;&#123;date[17]&#125;&#125;&#123;&#123;date[18]&#125;&#125;tags:categories:---]]></content>
      <categories>
        <category>github博客</category>
      </categories>
      <tags>
        <tag>github博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在GitHub上创建博客]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2F20181215005754.html</url>
    <content type="text"><![CDATA[GitHub搭建博客安装node.js 安装完成后测试 1234npm -v# 6.4.1node -v# v10.14.2 安装hexo 全局安装hexo 123456789101112131415161718192021npm install hexo -g# 安装完查看hexo -v# 输出hexo-cli: 1.1.0os: Windows_NT 10.0.17763 win32 x64http_parser: 2.8.0node: 10.14.2v8: 6.8.275.32-node.45uv: 1.23.2zlib: 1.2.11ares: 1.15.0modules: 64nghttp2: 1.34.0napi: 3openssl: 1.1.0jicu: 62.1unicode: 11.0cldr: 33.1tz: 2018e GitHub创建项目，并拉至本地 注意：项目名格式必须为 用户名.github.io 初始化hexo环境123456# 新建blog目录，在目录下执行命令hexo init# 完成后安装依赖npm install # npm install hexo-deployer-git --save 配置ssh，让本地可免密push到GitHub 略 新建博客，并上传到git新建博客1hexo new post "博客名" 配置12345# 在_config.yml最后配置deploy: type: git repo: git地址 branch: master 部署1hexo d -g 增加搜索功能安装插件：1npm install hexo-generator-searchdb --save 修改配置文件123456789# blog配置文件search: path: search.xml field: post format: html limit: 10000# 主题配置文件 themes/主题名/_config.ymllocal_search: enable: true 安装mermaid插件：安装命令1npm install hexo-filter-mermaid-diagrams -u 配置_config.yml中增加配置： 123456# mermaid chartmermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: "7.1.2" # default v7.1.2 options: # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js #startOnload: true // default true 在页面加入引用：github地址 12345678&lt;% if (theme.mermaid.enable) &#123; %&gt; &lt;script src=&apos;https://unpkg.com/mermaid@&lt;%= theme.mermaid.version %&gt;/dist/mermaid.min.js&apos;&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: &apos;forest&apos;&#125;); &#125; &lt;/script&gt;&lt;% &#125; %&gt; 报错 ERROR Deployer not found: git1npm install --save hexo-deployer-git]]></content>
      <categories>
        <category>github博客</category>
      </categories>
      <tags>
        <tag>github博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言与其他语言不一样的地方]]></title>
    <url>%2Fblog%2F2018%2F10%2F23%2F20181210104815.html</url>
    <content type="text"><![CDATA[没有：类、继承、多态、重载 的概念 不同世界观： go语言中使用 duck typing，面向接口编程、函数式编程 面向对象的世界里，也流行变继承为组合的思维 面向对象的元素容易被滥用 为组合提供了便捷的支持 没有：try/catch/finally 太多错误被当作异常 很多C++项目组禁用try/catch 正确的使用try/catch处理错误，导致代码混乱 在产品代码中try/catch并不能减小开发人员负担 真正异常情况有 defer/panic/recover 模式处理 没有构造/析构/RAII 大型项目很少使用构造函数，多使用工厂函数，如果需要，可直接使用结构体初始化语法实现 go有gc，不需要析构 没有泛型 泛型实际想实现duck typing，go语言提供了duck typing及接口组合支持 使用来约束参数类型：本身复杂，但go自带slice,map,channel 类似泛型参数 type assertion、go generation来实现自己的泛型 泛型支持是作者唯一态度不强硬的点 todo]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
</search>
