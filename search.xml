<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k8s问题总结]]></title>
    <url>%2Fblog%2F2019%2F06%2F20%2F20190620225015.html</url>
    <content type="text"><![CDATA[总结遇到的问题及解决方案 设计类问题集群Master高可用 创建集群时，使用多个Master就不提了，这是基础。 一般做法是使用Keepalived陪VIP，结合health_check来做高可用，但这种做法存在一个问题：所有的rest请求都落到一个Master上，当客户端请求量比较大的时候，会导致apiserver hang死。 - 使用DNS动态解析：平安证券分享的方案（公众号上看的，公布细节有限，原话是：当Master挂掉，DNS解析可在十分钟内指向新的MasterIP），猜测他们的做法使用脚本去发现apiserver的运行状况，发现挂掉，就切换解析，这种做法在Master挂掉时，会存在无法访问apiserver服务的空窗期（十分钟左右，DNS解析生效时间），无法提供服务，而且所有请求也都落到一个Master上；另一种利用DNS的做法是DNS批量解析到所有Master上，当脚本检测到Master挂掉，然后解除到该Master的解析，虽然比前者好用一些，但在DNS删除解析生效期间，也会有一部分请求落到挂掉的Master上。 - 使用Nginx+Keepalived方案：首先所有Master上都使用docker启动一个NGINX，并配置自动重启和检查apiserver健康的策略，然后安装好keepalived并配置VIP和设置NGINX的监控检查规则。客户端向集群发送rest请求时使用的是VIP+NGINX端口，所有请求都落到VIP Master的NGINX上，然后由NGINX转发到所有Master的apiserver上。- NGINX配置apiserver健康检查，可以保证其转发的请求都能转发到正常运行的apiserver上，keepalived配置NGINX的健康检查可以保证，每次客户端请求时都能请求到NGINX，通过这么设置，实现了NGINX的高可用和apiserver的高可用，且所有到apiserver的请求不仅仅落到某一台机器上。 不同集群Pod间网络互通 正常情况下，不同集群的Pod之间是无法相互访问的 想要不同集群Pod之间网络互通，必要条件是，两者的网段不能够冲突，如果冲突，无法确定Pod要访问的地址到底是另一个集群的还是当前集群的。 使用网络插件。通过对Pod请求出来的数据进行重新封包（目标PodIP所在集群作为新包的目的IP），把数据包发送到目标Pod所在集群，然后网络插件再解包，把解包后的原始数据包发送到指定Pod，实现Pod网络互通的目地。 使用网关+路由的方式。在集群中弄几台机器用作网关，所有到该集群Pod的请求路由到这些机器，这些机器再利用kube-proxy将请求转到Pod内部，实现网络互通，但这种实现方式中，一个Pod请求到外部和外部请求进来的路径不一致。外部请求进来：proxyNode-&gt;Pod；请求到外部：Pod-&gt;Pod所在Node-&gt;外部。这种方式存在问题：对于Pod来说，每次请求的来源IP都是ProxyNode，当应用根据来源IP来做一些策略时可能存在问题。 踩坑系列apiserver请求过多，直接hang死 使用集群Master高可用方案里的第3点（参照设计类问题中的集群Master高可用部分）。 ##]]></content>
      <categories>
        <category>k8s</category>
        <category>总结</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s问题总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker基础之资源隔离]]></title>
    <url>%2Fblog%2F2019%2F06%2F20%2F20190620225014.html</url>
    <content type="text"><![CDATA[Linux Namespace概念 Linux Namespace是Linux内核提供的一个功能，它可以隔离一系列的系统资源，比如：PID,UserID,NetWork等。 当前Linux内核一共实现了6种不同类型的Namespace: Namespace类型 系统调用参数 内核版本 Mount Namespace CLONE_NEWNS 2.4.19 UTS Namespace CLONE_NEWUTS 2.6.19 IPC Namespace CLONE_NEWIPC 2.6.19 PID Namespace CLONE_NEWPID 2.6.24 Network Namespace CLONE_NEWNET 2.6.29 User Namespace CLONE_NEWUSER 3.8 Namespace 的API主要使用如下三个系统调用： clone() 创建新进程。根据系统调用参数来判断哪些Namespace被创建，而且它们的紫禁城也会被包含到这些Namespace中 unshare() 将进程移除某个Namespace setns（） 将进程加入到Namespace中 UTS Namespace UTS Namespace主要用来隔离 nodename和domainname两个系统标识，在UTS Namespace中，每个Namespace允许有自己的hostname。 golang测试代码： 1234567891011121314151617181920212223242526// file : uts.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 在代码目录下执行命令：go run uts.go，然后进入shell交互页面。 然后执行命令echo $$，打印出当前shell的进程id，然后再执行命令 readlink /proc/$pid/ns/uts，打印出的结果与直接在terminal中执行该命令结果是不一样的，说明已实现了uts隔离。 在该shell中修改hostname：hostname -b test，然后同时在该shell中和直接在terminal中打印hostname，发现两者不一致，说明在uts隔离的shell中对uts修改不会影响宿主机。 IPC Namespace IPC Namespace 用来隔离 信号量、消息队列和共享内存，每一个IPC Namespace的这些资源都是隔离的。在上一版本的代码中略作修改： 1234567891011121314151617181920212223242526//file: ipc.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC, // 增加一个flag &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; IPC隔离测试： 首先在宿主机上打开一个shell 123456789101112131415161718192021# 查看现有 ipc message queuesroot@master:~# ipcs -q--------- 消息队列 -----------键 msqid 拥有者 权限 已用字节数 消息# 创建一个message queueroot@master:~# ipcmk -Q消息队列 id：0# 然后在查看一下root@master:~# ipcs -q--------- 消息队列 -----------键 msqid 拥有者 权限 已用字节数 消息 0xfee7b418 0 root 644 0 0 # 此时能看到一个queue了，再使用另一个shell去运行程序： go run ipc.go# ipcs -q--------- 消息队列 -----------键 msqid 拥有者 权限 已用字节数 消息 由以上实验可以发现，在新建的Namespace里，看不到宿主机上创建的queue，说明IPC Namespace创建成功，IPC已经被隔离。 PID Namespace PID Namespace是用来隔离进程ID的，同一个进程在不同的PID Namespace里可以有不同的PID，在docker容器里面，使用 ps -ef就会发现，容器内，前台运行的进程PID是1，但在容器外使用ps -ef却发现同一进程有不同的PID，这就是PID Namespace的功劳。 测试代码，在上一节代码中增加一个CloneFlag： 123456789101112131415161718192021222324252627// file: pid.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID, // add &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 同时打开两个shell，其中一个运行： go run pid.go 在宿主机shell中执行命令： ps -ef|grep pid.go可以查到另一个shell中运行的go程序的pid，然后在另一个运行go程序的shell中执行命令: echo $$可以得到当前sh的pid，可以发现，在运行go程序的shell中其PID为1，而在宿主机看到该程序的PID不为1。说明PID已隔离。 Mount Namespace Mount Namespace 用来隔离各个进程看到的挂载点视图，在不同的Namespace中，看到的文件层次是不一样的，在Mount Namespace中调用mount() umount()仅影响当前Namespace的文件系统（注：Mount有传播模式，如果是share模式还是会影响到的，测试时设置private模式可避免影响） 测试代码，在上一节基础上再增加一个CloneFlag： 123456789101112131415161718192021222324252627// file: ns.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 运行程序： go run ns.go 在程序shell中执行命令mount --make-private /proc，先修改挂载模式为private 修改 mount 之前，查看 /proc (ls /proc)目录发现里边文件有很多，然后执行命令：mount -t proc proc /proc，在查看 /proc目录，发现里边文件少了很多。此时使用 ps -ef查看系统进程，可以看到只有 sh进程和 ps -ef进程。说明Mount隔离成功。(注：/proc目录下有很多数字的目录，这些每个目录都代表一个进程，数字为进程的PID，里边存储着跟进程相关数据，Mount隔离和PID隔离配合，使得ps -ef命令查看到当前Namespace下的进程，且PID是从1开始编号的) User Namespace User Namespace主要是隔离用户的用户组ID，可以在宿主机上以一个非root用户运行创建一个User Namespace，并且在这个Namespace下创建root用户，且在namespace下有root权限。 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142// file: user.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER, UidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getuid(), Size: 1, &#125;, &#125;, GidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getgid(), Size: 1, &#125;, &#125;, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 以普通用户执行：go run user.go，在shell中输入命令id，打印出：uid=0(root) gid=0(root) 组=0(root),65534(nogroup)，但当尝试列出 /root目录时，会提示权限不够。 Network Namespace Network Namespace是用来隔离网络设备，IP地址端口等网络栈的Namespace。 测试代码：在上一节代码基础上增加CloneFlag： 123456789101112131415161718192021222324252627282930313233343536373839404142// file: network.gopackage mainimport ( "mydocker/tools/logging" "os" "os/exec" "syscall")func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNET, UidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getuid(), Size: 1, &#125;, &#125;, GidMappings: []syscall.SysProcIDMap&#123; &#123; ContainerID: 0, HostID: syscall.Getgid(), Size: 1, &#125;, &#125;, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; logging.Fatal(err) &#125;&#125; 在宿主机上查看网络设备，ifconfig，可以看到很多个网络设备。 执行命令：go run network.go后，在查看网络设备，发现只有 lo 一个网络设备了，说明Network Namespace隔离成功。 Linux Cgroup Linux Cgroup提供了对一组进程及将来子进程的资源限制、控制和统计的能力。 Cgroup的三个组件： cgroup是对进程分组管理的一种机制，一个cgroup包含一组进程，并可以在这个cgroup上增加subsystem的各种参数配置，将一组进程和一组subsystem的系统参数关联起来 subsystem是一组资源控制的模块，一般包含以下几项： blkio：设置对块设备输入输出的访问控制 cpu：设置cgroup中进程的cpu被调度的策略 cpuacct：可以统计cgroup中进程的cpu占用 cpuset：在多核机器上设置cgroup中进程可以使用的cpu和内存 devices：控制cgroup中进程对设备的访问 freezer：用于挂起和恢复cgroup中的进程 memory：用于控制cgroup中进程的内存占用 net_cls：用于将cgroup中进程产生的网络包分类，以便Linux的tc(traffic controller)可以根据分类区分出来自某个cgroup的包并做限流或监控 net_prio：设置cgroup中进程产生的网络流量的优先级 ns：使cgroup中的进程在新的Namespace中fork新进程时，创建一个新的cgroup，这个cgroup包含新的Namespace中的进程 hierarchy的功能是把一组cgroup串成一个树状结构，通过树状结构Cgroup可以做到集成 三个组件相互关系 系统在创建了新的hierarchy后，系统中所有进程都会加入这个hierarchy的cgroup根节点，这个cgroup根节点是hierarchy默认创建的 一个subsystem只能附加到一个hierarchy上 一个hierarchy能附加多个subsystem 一个进程可以作为多个cgroup成员，但这些cgroup必须在不同的hierarchy中 一个进程fork出子进程时，子进程和父进程是在同一个cgroup中，也可移到其他cgroup中]]></content>
      <categories>
        <category>docker</category>
        <category>docker基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker资源隔离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker基础之镜像存储]]></title>
    <url>%2Fblog%2F2019%2F06%2F16%2F20190616225014.html</url>
    <content type="text"><![CDATA[Union File System什么是Union File System Union File System简称 UnionFS，是一种为Linux，FreeBSD，NetBSD操作系统设计的，把其他文件系统联合到一个挂载点的文件系统服务。它使用branch将不同文件和目录透明地覆盖，形成一个单一一致的文件系统。这些branch都是readonly或read-write的，当对这个虚拟后的联合文件系统进行写操作时，系统是真正写到了一个新的文件中，看起来这个虚拟的联合文件系统是对任何文件进行操作的，但实际上并未改变原来的文件，unionFS使用了写时复制技术。 写时复制（copy on write，简称CoW），也叫隐式共享，是一种对可修改资源实现高效复制的资源管理技术。主要思想：如果一个资源是重复的，没有任何修改，这时并不需要立即创建一个新资源，这个资源可以被共享，一旦有写操作发生时，就会将该资源复制一份，而且写操作发生在新copy的资源上。通过这种方式，可以减少未修改资源复制带来的消耗和硬盘存储的消耗。 Aufs简介 aufs(Advanced multi layered unification filesystem)是一种支持联合挂载的文件系统，aufs将不同目录分层挂载到同一个目录下，每一层都是一个普通的文件系统。 读写操作 当需要读写一个文件时，会从最顶层开始向下寻找，本层没有，则根据层之间的关系到下一层找，直到找到第一个文件并打开它。 当写入一个文件时，如果文件不存在，则在读写层新建一个，否则向上边一个过程一样从顶层开始查找，直到找到最近的文件，aufs会将其复制到读写层进行修改。 当删除一个文件时，若文件仅存在于读写层，则可以直接删除，否则删除其在读写层的北方，再在读写层创建一个whiteout文件来标识文件不存在。 当新建一个文件时，若文件在读写层存在whiteout文件，则先删除之再新建，否则直接新建。 Device Mapper简介 Device Mapper是一种从逻辑设备到物理设备的映射框架，通过使用它，用户可以很方便地根据自己的需要制定实现存储资源的管理策略。 它主要包括3个概念：映射设备、映射表、目标设备。 说明 docker使用Device Mapper的CopyOnWrite发生在块存储级别。Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。 DeviceMapper 根据使用的基础块设备是真正的块设备哈斯稀疏文件挂载的loop设备分为两种模式：direct-lvm模式和loop-lvm模式，两者性能差别很大。 Overlay简介 Overlay主要目录结构：lower，upper，work，merge： lower：在docker中用于存放镜像分层文件，只读。可以有多个 upper：在docker中用于存放修改的文件，新增、修改、删除等，对应docker存储的 diff目录 work： merge：存放多个lower、upper合并后的文件和目录，对应docker的 merge目录 对联合挂载在merge目录下的文件操作规则： 新增：新增文件会直接将新增的文件写入到upper目录下 修改：如果对lower层中的文件修改，则会引起copy，文件copy到upper目录下，所有修改操作在upper目录下完成；如果对upper目录文件修改（包括新增的文件和已经从底层copy出的文件），则直接修改 删除：在upper目录中会对文件标记删除（使用ll查看文件标识为c --- --- ---，正常文件第一个标识位为-，目录为d），合并到merge目录下的效果为删除。 大文件修改 对lower层大文件修改时，会产生一个.file.swp（.前缀和.swp后缀）文件，修改文件在该文件操作，当执行保存时，首先将文件从lower层该文件复制到upper层中，命名为file~（在文件后加~后缀），然后将file~与.file.swp文件合并生成最终文件并删除file~。 本地测试显示，当保存文件时，新file和file~与lower层file曾同时共存。当文件从file~ 完全copy到file时，会被删除。 本地测试记录：当使用vim打开大文件时，对应的upper目录中生成了 .file.swp文件，当对其编辑时，所有修改操作都在该文件中记录（随着新增内容，swp文件大小随之改变）；当执行保存时，upper中生成了file~文件，当该文件大小不再变动时查看大小，发现与lower层中文件大小一致，说明该文件为lower层文件的副本。 此过程与Linux写大文件操作基本一致：本地生成.swp文件，然后直接将源文件重命名为file~，再合并两者为最终文件，然后删除file~。在OverlayFS中区别为，file~文件在OverlayFS中是从底层复制的[当修改upper层文件时与Linux中写大文件操作一致]，而在Linux中写大文件时file~是直接把源文件重命名的 自己动手使用Overlay 实验环境：centos7，内核：4.4.169-1.el7.elrepo.x86_64 12345678910111213141516171819202122文件路径： /app/overlayfs-test目录结构：.├── layer1│ ├── 1.txt│ └── tmp.txt├── layer2│ ├── 2.txt│ └── tmp.txt├── layer3│ ├── 3.txt│ └── tmp.txt├── layer4│ ├── 4.txt│ └── tmp.txt├── mnt├── upper└── work └── work文件内容： 1. layer&#123;n&#125;/&#123;n&#125;.txt 内容里的内容为 n2. layer&#123;n&#125;/tmp.txt 内容里的内容为 n 执行命令： 1mount -t overlay -o lowerdir=/app/overlayfs-test/layer1:/app/overlayfs-test/layer2:/app/overlayfs-test/layer3:/app/overlayfs-test/layer4,upperdir=/app/overlayfs-test/upper,workdir=/app/overlayfs-test/work overlay /app/overlayfs-test/mnt 命令格式： (格式：mount -t overlay -o lowerdir=pwd1:pwd2,upperdir=pwd3,workdir=pwd4 overlay pwd5) 格式说明：命令中 pwd{n}部分均为变量，根据自己挂载情况修改，lowerdir=pwd1:pwd2，当pwd1和pwd2中有同名文件，则显示的是pwd1中的文件。upperdir指定了upper文件的目录，workdir指定了work目录，最后一个参数指定了merge目录。 执行后的目录结构： 12345678910111213141516171819202122.├── layer1│ ├── 1.txt│ └── tmp.txt├── layer2│ ├── 2.txt│ └── tmp.txt├── layer3│ ├── 3.txt│ └── tmp.txt├── layer4│ ├── 4.txt│ └── tmp.txt├── mnt│ ├── 1.txt│ ├── 2.txt│ ├── 3.txt│ ├── 4.txt│ └── tmp.txt├── upper└── work └── work 如上， {n}.txt都挂载到了 mnt目录下，tmp.txt文件则只有一个，cat mnt/tmp.txt输出1 在mnt目录新增文件x.txt输入x，删除1.txt，编辑2.txt输入x2，新的目录结构： 1234567891011121314151617181920212223242526272829303132333435363738.├── layer1│ ├── 1.txt│ └── tmp.txt├── layer2│ ├── 2.txt│ └── tmp.txt├── layer3│ ├── 3.txt│ └── tmp.txt├── layer4│ ├── 4.txt│ └── tmp.txt├── mnt│ ├── 2.txt│ ├── 3.txt│ ├── 4.txt│ ├── tmp.txt│ └── x.txt├── upper│ ├── 1.txt│ ├── 2.txt│ └── x.txt└── work └── work``` &gt; upper目录中，执行命令：&gt; ```bash[root@node2 upper]# ll总用量 8c--------- 1 root root 0, 0 6月 16 23:36 1.txt-rw-r--r-- 1 root root 3 6月 16 23:36 2.txt-rw-r--r-- 1 root root 2 6月 16 23:36 x.txt[root@node2 upper]# cat 2.txt x2[root@node2 upper]# cat x.txt x 其中，1.txt标记为删除，2.txt内容改为x2，x.txt是新增的文件 在mnt目录下，文件1.txt也确实被删除了，x.txt文件也添加进来了，2.txt内容也改为x2。反观layer1目录下，1.txt文件还存在，layer2目录中2.txt内容仍然为2]]></content>
      <categories>
        <category>docker</category>
        <category>docker基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker镜像存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[client-go源码阅读之-controller(+简单实现)]]></title>
    <url>%2Fblog%2F2019%2F06%2F06%2F20190606154010.html</url>
    <content type="text"><![CDATA[本篇文章简单介绍controller实现过程中涉及到的client-go中的几个组件，并尝试使用简单的方式手撸一个可以实现一个类似client-go中controller功能的demo。 注：本文client-go是从kubernetes 1.13的项目源码中拷贝出来的，文中标记的代码行数以此为准。 简介 NewIndexerInformer()是定义在client-go/tools/cache/shared_informer中的一个函数，返回值为indexer,controller，本文主要讲controller的功能实现(也会简单提到indexer)，它用来监听k8s集群中的某一种资源，针对资源对象的不同事件(add/update/delete)执行用户自定义的事件处理函数。 其在k8s中的应用十分广泛 k8s里的系统组件：k8s调度器里通过监听pod资源对象来对其进行调度、k8s的kube-proxy通过监听service/endpoints资源对象的变化来配置各个节点的网络等等； CRD 使用该组件对k8s中的资源进行状态监控(deployment、daemonSet、pod等) 使用该组件对k8s中的资源配额进行监控 其他 controller 基本组成部分介绍 先讲几个在controller中引用到的几个结构体 Indexer 首先看一下如何New一个Indexer对象： 12345678910111213141516171819// client-go/tools/cache/store.go line: 112type cache struct &#123; // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc&#125;// ...// client-go/tools/cache/store.go line: 239func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer &#123; return &amp;cache&#123; cacheStorage: NewThreadSafeStore(indexers, Indices&#123;&#125;), keyFunc: keyFunc, &#125;&#125;// 注KeyFunc定义： type KeyFunc func(obj interface&#123;&#125;) (string, error) 可以看到，NewIndexer返回的是一个cache对象，而cache实现了Indexer接口。再结合cache的定义可以知道，此处的Indexer就是一个线程安全的存储，keyFunc的作用就是给定一个对象，然后返回该对象的key值。暂时简单地理解Indexer不去深究其内部其他逻辑，将其视为一个线程安全的map存储即可，这个map的key值可以通过调用keyFunc(val)获得。 ListWatch 首先看下 ListWatch的定义： 123456789// ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface.// It is a convenience function for users of NewReflector, etc.// ListFunc and WatchFunc must not be niltype ListWatch struct &#123; ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool &#125; 主要包括两个函数对象(先不去理会DisableChunking)，ListFunc作用主要是列出k8s集群中的资源对象，WatchFunc作用主要是监听k8s集群中的资源对象。 先看下创建ListWatch的代码吧： 123456789101112131415161718192021func NewFilteredListWatchFromClient(c Getter, resource string, namespace string, optionsModifier func(options *metav1.ListOptions)) *ListWatch &#123; listFunc := func(options metav1.ListOptions) (runtime.Object, error) &#123; optionsModifier(&amp;options) return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(&amp;options, metav1.ParameterCodec). Do(). Get() &#125; watchFunc := func(options metav1.ListOptions) (watch.Interface, error) &#123; options.Watch = true optionsModifier(&amp;options) return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(&amp;options, metav1.ParameterCodec). Watch() &#125; return &amp;ListWatch&#123;ListFunc: listFunc, WatchFunc: watchFunc&#125;&#125; 从代码中可用看到，这里创建了两个函数对象，ListFunc和WatchFunc，这两个函数返回的都是向apiserver发请求的对象，不过其发送请求的Action不同，一个是Get，一个是Watch，那ListWatch对象如何知道去Get/Watch哪个集群的资源对象呢，这个就是通过参数c来指定了，k8s的ClientSet实现了Getter接口，可以作为参数传进来。 到此处是不是感觉接触到了controller的核心部分了呢？到这里就算是比较接近底层的部分了，再向底层分析就是client-go向apiserver发请求和解析请求结果的部分了，深度上先到此为止（在本篇中，我们不去关心更底层是如何Watch到资源的变化情况的，也不关心数据是怎么在client和apiserver中传输和解析的只需知道ListWatch已经能够Get和Watch到集群的资源就够了）。 DeltaFIFO DeltaFIFO，从名字可以看出这是一个变化的队列，且是先进先出的队列。稍微解释一下，这个队列实际包含两层队列。先看下定义的这部分： 12345678910111213141516// client-go/tools/cache/delta_fifo.go 96行type DeltaFIFO struct &#123; //... // line：104 items map[string]Deltas queue []string // ... keyFunc KeyFunc // ...&#125;// 补充一下Deltas的定义：type Deltas []Deltatype Delta struct &#123; Type DeltaType // 字符串，Added,Updated,Deleted等 Object interface&#123;&#125;&#125; 先介绍一下 keyFunc，keyFunc和上边的Indexer里的keyFunc作用类似，都是根据对象返回一个key值，这里求得的key值主要用于 queue中的值，还有items这个map中的键值，从而，当从queue中取出队列头部元素时，可以根据这个元素值从items中取出对应的具体value 以下分析DeltaFIFO的两层队列实现：首先，queue是一个队列，数组保存队列，也很容易实现先进先出，queue队列中元素出队时，从item中取出items[queue[0]]元素并删除，queue也删除0号元素即可完成队列的Pop操作。从Deltas的定义看到，这是一个Delta数组，items[queue[0]]也是一个队列，这个队列存的是Delta对象，既然它们存储在同一个items[key]下，那么表示它们是同一个资源对象的Delta，即：这个队列表示的是同一个对象的事件队列。 综上： queue表示的是不同资源对象的队列，items的value值表示的是同一资源对象的事件队列。 有点拗口，想象一下这个场景： 假设有3个pod资源对象，记为 a,b,c， a发生变化deltaA1，此时a加入到queue队列，a的变化事件加到items[a]这个事件队列 b发生变化deltaB1，此时b假如到queue队列，b的变化事件加到items[b]这个事件队列 a又发生变化deltaA2，此时items中已经存在items[a]了，表示a已经在队列queue中了，此时，获取到items[a]事件队列，然后将这次变化加入队列此时，queue和items中的数据为： 12queue : [a,b]items : &#123;a:[deltaA1,deltaA2], b: [deltaB1]&#125; 如果上边步骤3发生时，queue已经执行过一次出队操作，那么：a发生变化deltaA2时，items中items[a]不存在，表示a不在队列中，便将a加到queue队列，deltaA2加到items[a]这个事件队列中去。此时，queue和items中的数据为： 123queue: [b,a]items: &#123;a:[deltaA2], b: [deltaB1]&#125;// 注意：items是map，其内部的`k-v`键值对不分先后顺序，只有v的值分先后顺序，因为v是数组 讲了这么多，大概应该能明白DeltaFIFO的作用了吧，简单点说就是存储k8s资源对象变化事件的队列。 controllercontroller初始化 在controller里，存储了ListWatch，objType(监听的资源对象类型)，ResourceEventHandler(资源事件handler)，DeltaFIFO，以及Process 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// client-go/tools/cache/controller.go line:345func NewIndexerInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, indexers Indexers,) (Indexer, Controller) &#123; // This will hold the client state, as we know it. // 定义Indexer（线程安全的存储） clientState := NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers) // This will hold incoming changes. Note how we pass clientState in as a // KeyLister, that way resync operations will result in the correct set // of update/delete deltas. // 定义FIFO（事件队列） fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, clientState) cfg := &amp;Config&#123; Queue: fifo, // 将ListWatch对象赋值到cfg，传递给controller ListerWatcher: lw, ObjectType: objType, FullResyncPeriod: resyncPeriod, RetryOnError: false, // 资源对象变化处理函数，obj类型是Deltas，就是上边降到DeltaFIFO中 items这个map对象的value的数据类型 // 这个函数是在 fifo.queue这个队列出队的时候调用 Process: func(obj interface&#123;&#125;) error &#123; // from oldest to newest for _, d := range obj.(Deltas) &#123; switch d.Type &#123; case Sync, Added, Updated: // 判断Indexer中是否有该元素，有则表示是更新操作，没有表示是新增操作 // 该判断逻辑一定对的前提：下边Add事件时，Indexer同步add，Delete事件时，同步delete // 此时更新Indexer中的数据，并调用 h.OnUpdate // Indexer中存储数据的更重要的一点是，本地存储新更新到来之前的最新版本资源对象，那么当更新事件到来时，客户端可以有新旧对象供handler使用 if old, exists, err := clientState.Get(d.Object); err == nil &amp;&amp; exists &#123; // update Indexer if err := clientState.Update(d.Object); err != nil &#123; return err &#125; // 调用handler事件 h.OnUpdate(old, d.Object) &#125; else &#123; // add to Indexer if err := clientState.Add(d.Object); err != nil &#123; return err &#125; // 调用handler事件 h.OnAdd(d.Object) &#125; case Deleted: // delete from Indexer if err := clientState.Delete(d.Object); err != nil &#123; return err &#125; // 调用handler事件 h.OnDelete(d.Object) &#125; &#125; return nil &#125;, &#125; return clientState, New(cfg)&#125; controller运行 controller启动方法是 controller.Run()，首先看下源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// client-go/tools/cache/controller.go line:100func (c *controller) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() go func() &#123; &lt;-stopCh c.config.Queue.Close() &#125;() r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.clock = c.clock c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group defer wg.Wait() // 此处启动一个协程，运行 r.Run函数 // r.Run即Reflector对象的Run方法，定义在 client-go/tools/cache/reflector.go line:121 // 通过查看 r.Run可以知道，它又运行了 r.ListAndWatch // r.ListAndWatch里的逻辑就是真正获取k8s资源对象 // 在该函数中又调用r.watchHandler 来监控着资源的事件，并将其加到r.store中(在本文描述场景中，r.store即为DeltaFIFO) wg.StartWithChannel(stopCh, r.Run) // 此处启动processLoop循环处理事件 wait.Until(c.processLoop, time.Second, stopCh)&#125;// 从队列(DeltaFIFO)中取出数据，执行 process（前边在NewIndexerInformer中定义的process）// c.config.Queue.Pop方法定义在：client-go/tools/cache/delta_fifo.go line:411func (c *controller) processLoop() &#123; for &#123; obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil &#123; if err == FIFOClosedError &#123; return &#125; if c.config.RetryOnError &#123; // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) &#125; &#125; &#125;&#125; 通过 controller.Run方法，整个资源状态监控的过程就完成了。 总结： 通过以上讲informer中使用到的各个组件的功能及作用，整个informer工作流程大概如下： 首先调用ListWatch中的List方法，初步将k8s中待监听资源拉到本地，将其加到本地存储Indexer和事件队列DeltaFIFO中(初始化时将这些资源的变化事件看做是ADD) 异步1：使用ListWatch中Watch方法不断去监听事件，监听到后将其加到 DeltaFIFO中 异步1：无限循环： 阻塞方法去从队列中取出数据（使用到了sync.cond） 取到的数据类型是 Deltas，调用 controller中的Process方法（上边的NewIndexerInformer里定义的） // 在Process方法里调用用户自定义的handler 以上过程讲的比较简单，实际client-go的代码中很多出错逻辑处理、同步数据、以及多协程时的数据存储优化。 手撸实现 client-go中，controller 部分代码使用了很多接口，而且封装的太深，看它的源码时没那么容易。 基于以上对 controller 的分析，自己实现了一个比较简单的 controller，弄懂这个后再去深究 client-go的实现会事半功倍。 代码地址：https://github.com/geedchin/client-go-src-learning核心部分在 02_watch中]]></content>
      <categories>
        <category>k8s</category>
        <category>功能组件</category>
        <category>client-go</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>client-go</tag>
        <tag>controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆的理解与运用]]></title>
    <url>%2Fblog%2F2019%2F05%2F23%2F20190523000154.html</url>
    <content type="text"><![CDATA[什么是堆 堆是一种完全二叉树，且每个节点都大于等于（小于等于）其子节点的值。 大顶堆：每个节点都大于等于其子节点的堆小顶堆：每个节点都小于等于其子节点的堆 如何存储堆 根据堆的定义，其为完全二叉树，则可以使用数组来存储堆。使用数组存储堆相比于链式存储有以下优点： 访问任意位置元素时间复杂度为O(1) 访问每个节点的子(父)节点耗时与链式存储几乎无差异。假如某节点在数组的索引是x,则其父节点索引为 (x+1)/2-1，左右子节点分别为(x+1)*2-1与(x+1)*2 数组在内存中按顺序存储，对CPU更友好，缓存命中高 堆的操作 以下涉及堆均默认为小顶堆，大顶堆与其类似，只是判断大小逻辑需调整 向堆中增加元素 把新的元素放至堆的末位，为使其仍然符合堆的定义，则需要对其进行调整。这个过程叫做堆化(heapify)： 将新元素与其父元素做比较，如果比父节点值小，则与父节点交换位置，以此类推，通过不断的与新父节点作比较与交换，最终到达合适的位置，正好满足堆的定义。 Go语言实现自下而上的堆化过程代码如下： 1234567891011// 自下而上的堆化func heapify(arr []int, x int) &#123; for x &gt; 0 &#123; parent := (x+1)/2 - 1 if arr[x] &lt; arr[parent] &#123; arr[x], arr[parent] = arr[parent], arr[x] &#125; x = parent &#125;&#125; 删除堆顶元素 从堆的定义中可知，对顶元素即为堆中的最小值，当删除该值时，需将剩下元素的最小值放到堆顶，而且其最小值必定是堆顶元素的子节点;将最小值移上去后，还需填充该位置以满足其完全二叉树的性质，再去从该位置的子节点中找最小元素… 最终将堆中最后一行中的某个元素移到上一层，但此时可能不满足满二叉树的性质，又要将最后一个位置的值移到该位置（移了之后又可能不满足堆的性质，又要堆化），此过程还是比较麻烦的。 如果考虑将堆顶元素删除后，直接将最后一个元素移到此位置，则只需对其自上而下堆化即可: 将堆顶元素删除，并将堆中最后一个元素放到堆顶 从堆顶开始，将堆顶视为当前元素 比较当前元素与子节点元素的大小，取出最小的，如果最小的为本身，则直接结束； 否则，交换最小的子节点元素与当前元素值， 将最小子节点作为当前节点，重复2，直到当前节点为叶子节点 Go语言实现自上而下的堆化过程代码如下： 1234567891011121314151617181920212223242526func heapify2(arr []int, n int) &#123; arrLen := len(arr) for &#123; right := (n + 1) * 2 left := right - 1 if left &gt;= arrLen &#123; break &#125; mini := n // 判断左右子节点，找出最小节点 if right &lt; arrLen &amp;&amp; arr[right] &lt; arr[mini] &#123; mini = right &#125; if arr[left] &lt; arr[mini] &#123; mini = left &#125; // 如果最小节点是当前节点，则退出 if mini == n &#123; break &#125; // 将最小位置的值与当前位置交换 arr[mini], arr[n] = arr[n], arr[mini] n = mini &#125;&#125; 如何基于堆实现排序 大致过程分为两步：建堆和排序 建堆 建堆有两种方法： 堆中元素从1个不断增加，增加到指定长度，这个过程使用了自下向上的堆化过程。时间复杂度为O(nlg(n)) 从数组中间开始遍历到数组0索引位置，对每个位置实行自上而下的堆化过程。时间复杂度O(n) 第二种建堆方法Go实现： 1234567func InitHeap(arr []int) &#123; arrLen := len(arr) for i := arrLen/2 + 1; i &gt;= 0; i-- &#123; heapify(arr, i) // 调前边的实现 &#125;&#125; 排序 不断从堆中取出最小值，取得数据的序列即为从小到大的排序结果 12345678910// 删除堆顶元素func RemoveTop(arr []int) int &#123; ret := arr[0] len := len(arr) arr[0] = arr[len-1] arr = arr[0 : len-1] heapify(arr, 0) // 调前边的实现 return ret&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>堆</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go接口中的‘陷阱’]]></title>
    <url>%2Fblog%2F2019%2F04%2F10%2F20190410140732.html</url>
    <content type="text"><![CDATA[接口值陷阱概念 接口值共有两部分组成：具体的类型、该类型的值。它们称之为接口值的动态类型和动态值。 接口值判断陷阱 既然接口值有两部分组成，那么，接口值的相等性判断是否也要类型及值均相等才算相等呢？ 答案是肯定的，以下两个示例： 123456// 公共部分type People interface &#123; &#125;type Student struct &#123; &#125;type Teacher struct &#123; &#125;// 均无方法，方便测试// 可以看作Student Teacher都实现了 People接口 12345678910// 示例1var p1, p2 Peoplevar s *Studentvar t *Teacherfmt.Println(s == nil, t == nil) // true truep1 = sp2 = tfmt.Println(p1 == p2) // false// p1 p2 值均为空，但其类型不同，因此判断相等结果为false 12345678910// 示例2var p1 Peoplevar s *Studentfmt.Println(p1 == nil) // truefmt.Println(s == nil) // truep1 = sfmt.Println(p1 == nil) // false// p1 值为nil，但类型不为nil，所以p1与nil做相等比较结果false// 此处要注意，在函数中用接口类型做为形参时，如果直接将其与nil做比较来判断其是否为空，可能会有意想不到的结果，且不易排查 接口不同实现方式 使用指针接收者实现 使用类型接收者实现 指针接收者实现 使用指针接收者实现，则只有该实体类型的指针实现了该接口。不能将实体类型的值赋值给该接口类型变量。 12345678910111213141516171819package mainimport "fmt"type People interface &#123; SayHello()&#125;type Student struct &#123;&#125;func (*Student) SayHello() &#123; fmt.Println("hello")&#125;func main() &#123; var p People // p = Student&#123;&#125; // 编译报错 p = &amp;Student&#123;&#125; p.SayHello()&#125; 类型接收者实现 能够将实体类型和实体类型的指针赋值给接口类型变量。 1234567891011121314151617181920package mainimport "fmt"type People interface &#123; SayHello()&#125;type Student struct &#123;&#125;func (Student) SayHello() &#123; fmt.Println("hello")&#125;func main() &#123; var p People p = Student&#123;&#125; p.SayHello() p = &amp;Student&#123;&#125; p.SayHello()&#125;]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>接口</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>go接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言接口简介]]></title>
    <url>%2Fblog%2F2019%2F04%2F10%2F20190410133551.html</url>
    <content type="text"><![CDATA[什么是接口 接口是一种描述一类事物具有相同操作的抽象：即某一类的事物都具有相同的操作合集。 go接口 在大多数语言中，实现某接口必须用 implement 显示地声明自己实现的是声明接口，而在go语言中，则无需声明实现接口，这种特性可称之为 duck typing。 比如，假设一只鸭子类型的接口有 游泳 呱呱叫 走路 等方法，任何一个对象如果包含这些方法，我们可以认为它是鸭子。 go接口类型 在go语言中，可以定义接口类型，它描述了一系列方法的集合，一个实现了这些方法的具体类型是这个接口类型的实例。有点抽象，可以这么理解：现在定义一个接口类型叫 Animal，其包含方法 Breath，那么所有包含 Breath方法的具体类型可视为一个Animal的实例。 接口类型还可以嵌套，例如，go自带io包里的接口 io.Writer、io.Reader、io.ReadWriter 12345678910type Writer interface&#123; Write(p []byte) (n int, err error)&#125;type Reader interface &#123; Read(p []byte) (n int, err error)&#125;type ReadWriter interface &#123; Reader Writer&#125; 实现接口的条件 一个类型如果拥有一个接口的所有方法，那么这个类型就实现了这个接口。]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>接口</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>go接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2Fblog%2F2019%2F04%2F01%2F20190401023114.html</url>
    <content type="text"><![CDATA[#逐步介绍 Dockerfile里的命令 ENV 格式： ENV &lt;key&gt; &lt;val&gt; 或 ENV &lt;key&gt;=&lt;val&gt; ... ENV指令可以为镜像创建出来的容器声明环境变量，可被后边的指令解释使用，使用格式为 $variable_name或${variable_name}。如果是带$的字符串，可使用\来转义。 FROM 格式： FROM &lt;image&gt; 或 FROM &lt;image&gt;:&lt;tag&gt; FROM指令为后边的指令提供基础镜像 COPY 格式: COPY &lt;src&gt; &lt;dest&gt; COPY指令复制&lt;src&gt;所指向的文件或目录，将它添加到新镜像中，src所指向的源必须在上下文中。 context上下文： 当构建镜像时，会执行 docker build PATH|-|URL命令，最后的PATH|-|URL即为上下文环境，在构建过程中，docker程序会将指定目录的所有文件(可用.dockerignore文件来忽略文件)复制到构建环境(实际是由docker-client提交到daemon去构建镜像)，这些文件作为构建镜像的上下文。 可同时拷贝多个文件，如 COPY src1 src2 workdir/，如果src1,src2是目录，则执行时会将src1,src2两个目录下的文件复制到 workdir/中去。 ADD 格式：ADD &lt;src&gt; &lt;dest&gt;ADD与COPY 功能上很相似，都支持将本地文件复制到镜像中，但ADD还支持其他功能，src可以是一个指向网络文件的 URL，此时若dest指向目录，则URL必须是完全路径；还可以是本地压缩归档文件，该文件复制到容器时会被解压提取，但URL中的归档文件不会被解压。 推荐使用COPY，相比之下它更透明 RUN 两种格式： RUN &lt;command&gt; shell格式 RUN [&quot;exec&quot;,&quot;param1&quot;,&quot;param2&quot;] exec格式 RUN命令会在前一条命令创建的镜像基础上创建一个容器，并在容器中运行命令，在命令结束后提交容器为新镜像，新镜像被Dockerfile中的下一条指令使用。 当使用shell格式，时，命令通过 /bin/sh -c运行，当使用exec格式时，命令是直接运行的，exec中参数会被当成json数组解析，因此必须使用双引号。因为其不在shell中执行，因此环境变量不会被替换。如果希望运行shell程序，可写成CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;progress&quot;] CMD 三种格式： CMD &lt;command&gt; shell格式 CMD [&quot;exec&quot;,&quot;param1&quot;,&quot;param2&quot;] exec格式，推荐 CMD [&quot;param1&quot;,&quot;param2&quot;] 为ENTRYPOINT指令提供参数 可以有多条CMD命令，但只有一条会被执行。前两种执行方式一致，但执行时机不通：RUN 是在构建过程中执行命令并生成新镜像，CMD 在构建时不执行而是在容器启动时作为第一条命令执行。使用第三种格式时，会将值作为参数传给ENTRYPOINT指令 ENTRYPOINT 两种格式： ENTRYPOINT &lt;command&gt; shell格式 ENTRYPOINT [&quot;exec&quot;,&quot;param&quot;] exec格式，推荐 ENTRYPOINT与CMD命令类似，都可以让容器在启动时执行相同的命令，但又有区别。ENTRYPOINT指令可以有多条，但只有最后一条有效，当使用shell格式时，会忽略所有CMD指令和docker run参数，并运行在 /bin/sh -c中，这意味着该指令进程是 /bin/sh -c的子进程，在容器中的PID将不再是1，且不能接受UNIX信号，即使使用docker stop命令，也收不到SIGTERM信号。 ONBUILD 格式： ONBUILD [INSTRUCTION] 添加一个将来执行的触发器指令到镜像中，当该镜像作为FROM指令的参数时，这些触发器指令就会在FROM指令执行时假如到构建过程中。]]></content>
      <categories>
        <category>docker</category>
        <category>镜像构建</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s网络插件——flannel]]></title>
    <url>%2Fblog%2F2019%2F01%2F10%2F20190110150506.html</url>
    <content type="text"><![CDATA[pod容器间通信]]></content>
      <categories>
        <category>k8s</category>
        <category>功能组件</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s网络</tag>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-ha 搭建]]></title>
    <url>%2Fblog%2F2019%2F01%2F07%2F20190107123609.html</url>
    <content type="text"><![CDATA[机器准备 6台虚拟机IP地址： 10.1.1.10~12 10.1.1.20~22 内核：4.4.169-1.el7.elrepo.x86_64（3.10以上能装docker都可） 安装包晚点奉上 基本软件安装keepalived安装 略 ssh实现集群机器间互相访问 略 docker 安装 本文用的docker版本：17.06.0-ce进入 step1-docker 文件目录下，执行 install.sh然后执行以下命令测试docker是否安装成功 12systemctl start dockerdocker version 安装成功后继续下一步 基本配置 进入 step2-conf 目录下，执行 basic_conf.sh（脚本代码如下，加了注释）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/sh# 配置docker开机启动systemctl daemon-reloadsystemctl restart dockersystemctl enable docker# 关闭防火墙并关闭开机启动systemctl disable firewalldsystemctl stop firewalld# 永久关闭 selinuxsetenforce 0sed "s/^SELINUX=enforcing$/SELINUX=disabled/g" /etc/selinux/config# 关闭swap# 网络设置swapoff -ayes|cp /etc/fstab /etc/fstab.bakcat /etc/fstab.bak |grep -v swap &gt; /etc/fstabecho '''net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness=0''' &gt; /etc/sysctl.d/k8s.confmodprobe br_netfiltersysctl -p /etc/sysctl.d/k8s.conf# 配置 ipvs的依赖模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4# 安装ipset 和ipvsadmyum install -y ipsetyum install -y ipvsadm 导入系统镜像 进入 step3-images 目录下，执行脚本 loadImages.sh，主要镜像及版本如下：(貌似weave镜像还少一个，不过这个未被墙可以联网下载) 1234567891011cloud-controller-manager-1.13.1.tarkube-controller-manager-1.13.1.tarkube-proxy-1.13.1.tarkube-scheduler-1.13.1.tarkube-apiserver-1.13.1.tarcoredns-1.2.6.taretcd-3.2.24.tarpause-3.1.tarweave-kube-npc-2.5.0.tar 安装软件 进入 step4-rpm 目录下，执行脚本 install.sh ，主要软件及版本如下：master机要全装，node机器只需装 kubelet和kubeadm即可(这两个有依赖以下其他的包) 123456cri-tools-1.12.0-0.x86_64.rpmkubeadm-1.13.1-0.x86_64.rpmkubectl-1.13.1-0.x86_64.rpmkubelet-1.13.1-0.x86_64.rpmkubernetes-cni-0.6.0-0.x86_64.rpmsocat-1.7.3.2-2.el7.x86_64.rpm 配置开机启动 kubelet 12systemctl start kubeletsystemctl enable kubelet 初始化集群 进入 /root目录下，配置集群的IP信息，新建文件 cluster-info： 123456CP0_IP=10.1.1.10CP1_IP=10.1.1.11CP2_IP=10.1.1.12VIP=10.1.1.8NET_IF=ens33CIDR=10.244.0.0/16 NET_IF 是网卡，这里需要根据实际情况修改，还有主机IP地址、VIP、网段 复制 kubeha-gen.sh 到 /root 目录下，并执行等待初始化成功，三台 master 即完成初始化。界面会输出加入集群的命令：kubeadm join xxxxxx node 节点机器执行 master 机器执行成功输出的命令 kubeadm join ....，即可加入集群 kubeha-gen.sh脚本如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177#/bin/bashfunction check_parm()&#123; if [ "$&#123;2&#125;" == "" ]; then echo -n "$&#123;1&#125;" return 1 else return 0 fi&#125;if [ -f ./cluster-info ]; then source ./cluster-info ficheck_parm "Enter the IP address of master-01: " $&#123;CP0_IP&#125; if [ $? -eq 1 ]; then read CP0_IPficheck_parm "Enter the IP address of master-02: " $&#123;CP1_IP&#125;if [ $? -eq 1 ]; then read CP1_IPficheck_parm "Enter the IP address of master-03: " $&#123;CP2_IP&#125;if [ $? -eq 1 ]; then read CP2_IPficheck_parm "Enter the VIP: " $&#123;VIP&#125;if [ $? -eq 1 ]; then read VIPficheck_parm "Enter the Net Interface: " $&#123;NET_IF&#125;if [ $? -eq 1 ]; then read NET_IFficheck_parm "Enter the cluster CIDR: " $&#123;CIDR&#125;if [ $? -eq 1 ]; then read CIDRfiecho """cluster-info: master-01: $&#123;CP0_IP&#125; master-02: $&#123;CP1_IP&#125; master-03: $&#123;CP2_IP&#125; VIP: $&#123;VIP&#125; Net Interface: $&#123;NET_IF&#125; CIDR: $&#123;CIDR&#125;"""echo -n 'Please print "yes" to continue or "no" to cancel: 'read AGREEwhile [ "$&#123;AGREE&#125;" != "yes" ]; do if [ "$&#123;AGREE&#125;" == "no" ]; then exit 0; else echo -n 'Please print "yes" to continue or "no" to cancel: ' read AGREE fidonemkdir -p ~/ikube/tlsIPS=($&#123;CP0_IP&#125; $&#123;CP1_IP&#125; $&#123;CP2_IP&#125;)PRIORITY=(100 50 30)STATE=("MASTER" "BACKUP" "BACKUP")HEALTH_CHECK=""for index in 0 1 2; do HEALTH_CHECK=$&#123;HEALTH_CHECK&#125;""" real_server $&#123;IPS[$index]&#125; 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;"""donefor index in 0 1 2; do ip=$&#123;IPS[$&#123;index&#125;]&#125; echo """global_defs &#123; router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state $&#123;STATE[$&#123;index&#125;]&#125; interface $&#123;NET_IF&#125; virtual_router_id 80 priority $&#123;PRIORITY[$&#123;index&#125;]&#125; advert_int 1 authentication &#123; auth_type PASS auth_pass just0kk &#125; virtual_ipaddress &#123; $&#123;VIP&#125;/24 &#125;&#125;virtual_server $&#123;VIP&#125; 6443 &#123; delay_loop 6 lb_algo loadbalance lb_kind DR nat_mask 255.255.255.0 persistence_timeout 0 protocol TCP$&#123;HEALTH_CHECK&#125;&#125;""" &gt; ~/ikube/keepalived-$&#123;index&#125;.conf scp ~/ikube/keepalived-$&#123;index&#125;.conf $&#123;ip&#125;:/etc/keepalived/keepalived.conf ssh $&#123;ip&#125; " systemctl stop keepalived systemctl enable keepalived systemctl start keepalived kubeadm reset -f rm -rf /etc/kubernetes/pki/"doneecho """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.1controlPlaneEndpoint: "$&#123;VIP&#125;:6443"apiServer: certSANs: - $&#123;CP0_IP&#125; - $&#123;CP1_IP&#125; - $&#123;CP2_IP&#125; - $&#123;VIP&#125;networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: $&#123;CIDR&#125;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs""" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yamlmkdir -p $HOME/.kubecp -f /etc/kubernetes/admin.conf $&#123;HOME&#125;/.kube/config# 加入其它节点JOIN_CMD=`kubeadm token create --print-join-command`for index in 1 2; do ip=$&#123;IPS[$&#123;index&#125;]&#125; ssh $ip "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/" scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $ip:~/.kube/config ssh $&#123;ip&#125; "$&#123;JOIN_CMD&#125; --experimental-control-plane"doneecho "Cluster create finished." 网络插件安装 进入weave目录下，执行 kubectl apply -f weave.yml]]></content>
      <categories>
        <category>k8s</category>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s高可用搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine]]></title>
    <url>%2Fblog%2F2019%2F01%2F06%2F20190106083354.html</url>
    <content type="text"><![CDATA[什么是协程 介绍协程之前，首先回顾一下进程与线程 进程与线程 进程： 通俗点讲，进程就是一个程序运行的实例，进程拥有自己运行时打开的各种资源和独立的内存空间。每个进程通过PCB（process control block）来保存自己的基本信息（在Linux中，该结构叫task_struct）PCB包含了进程运行的基本信息：标示符、状态、优先级、程序计数器、内存指针、上下文、IO信息、记账信息等 线程： 线程属于进程，是程序的实际执行者，一个程序可以包含多个线程，线程具有自己的栈空间，其基本信息存储在TCB（thread control block） 对OS来说，进程是最小的资源管理单元，线程是最小的执行单元 线程切换比进程切换更加轻量：线程进程切换时，都会调用内核切换上下文，不同进程之间内存空间相互独立，而不同线程之间共享进程的内存空间，因此线程上下文切换效率更高 进程和线程的切换都是抢占式的，由OS管理 go语言协程 协程,是一种比线程更轻量级的运行单元。主要特点： 轻量级“线程” 非抢占式多任务处理，由协程主动交出控制权 编译器、解释器、虚拟机层面的多任务 多个协程可以在一个或多个线程上运行 非抢占式测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import ( "testing" "fmt" "time")// 结束时间大于2秒，因为是非抢占式，main的协程无法及时得到执行// 打印属于IO操作，执行打印时会释放CPU资源给其他协程使用func Test_01(t *testing.T) &#123; for i := 0; i &lt; 100; i++ &#123; go func(x int) &#123; for &#123; fmt.Println(x) &#125; &#125;(i) &#125; time.Sleep(time.Second * 2)&#125;// 该程序不会退出，执行到最后一行打印之前，启动了4个协程，这些协程均不会释放CPU资源，没有空闲的线程用于主协程执行，因此不会退出// 当启动的协程数 &lt; runtime.NumCPU() ，可以正常退出func Test_02(t *testing.T) &#123; // N值要大于等于机器支持线程数（大于等于runtime.NumCPU()） // 测试机为2核4线程，这里设置成4 const N = 4 var a [N]int for i := 0; i &lt; N; i++ &#123; go func(x int) &#123; for &#123; a[x]++ &#125; &#125;(i) &#125; // 此处休眠1秒是为了让前边循环中的协程都启动 time.Sleep(time.Second) fmt.Println(a)&#125;// 主动交出控制权func Test_03(t *testing.T) &#123; // const N = 10 var a [N]int for i := 0; i &lt; N; i++ &#123; go func(x int) &#123; for &#123; a[x]++ // 主动交出控制权 runtime.Gosched() &#125; &#125;(i) &#125; time.Sleep(time.Second) fmt.Println(a)&#125; 任何函数只要加上go就可以交给调度器执行 调度器会在指定的点进行切换 可能切换点：（可能切换） IO,select channel 等待锁 函数调用(有时) runtime.Gosched() 可以通过使用 -race参数来进行数据多线程的读写竞争进行检测 go语言协程模型 模型简介 如下图，go语言协程的模型 M：系统线程（内核线程） P：执行Go协程所必须的资源（上下文环境） G：Go协程 可以把Go协程视为“用户线程”，它由用户态的Go调度器来实现调度，没有内核切换的开销。调度器通过将Go协程映射到内核线程中，使程序运行时“看起来”是在并发执行。 由于Go协程是非抢占式的，而且基本在指定的几种情况下才会释放CPU，所以在协程之间切换时程序上下文复制开销较小且不用进行内核调用，因此更轻量级。 模型 在并发执行程序时，调度器除了正常从协程队列里取出协程放到自己的执行队里中外，还有其他的执行调度行为： Work-sharing：当处理器生成新线程时，它会尝试着将一些工作任务交给其他闲置的线程执行，充分利用资源 Work-stealing：调度器主动“偷取”其他线程上执行任务来执行 在Go语言中有一个global goroutine queue，每个内核线程又有一个local goroutine queue， 对于每一个内核线程来说，在该线程上等待执行的协程是串行的，而对于外部来说，这些协程是并发的。 协程之于内核线程，有点像进程之于CPU（单核CPU上对于CPU来说进程是串行的，对于外部来说是并行的）]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>go协程</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[列表(list)——可快速增删的非连续空间的容器]]></title>
    <url>%2Fblog%2F2018%2F12%2F23%2F20181223104815.html</url>
    <content type="text"><![CDATA[基础基础结构12345678910111213141516171819202122// Element is an element of a linked list.type Element struct &#123; // Next and previous pointers in the doubly-linked list of elements. // To simplify the implementation, internally a list l is implemented // as a ring, such that &amp;l.root is both the next element of the last // list element (l.Back()) and the previous element of the first list // element (l.Front()). next, prev *Element // The list to which this element belongs. list *List // The value stored with this element. Value interface&#123;&#125;&#125;// List represents a doubly linked list.// The zero value for List is an empty list ready to use.type List struct &#123; root Element // sentinel list element, only &amp;root, root.prev, and root.next are used len int // current list length excluding (this) sentinel element&#125; Element，封装保存在list中的值， Element中的变量next,prev是指针，分别指向当前元素的下一个元素与前一个元素 Element中的变量list是指针，指向该Element所属的list Value存储的为List中存放的值 List，list本身 List中的变量root变量存储着List中的第一个元素，该元素Value为nil，只做为根，不存储数据 List中的变量len变量表示List的长度 本质就是数据结构中的双向循环链表 主要操作：Remove(e *Element) 移除元素 PushFront(v interface{}) 在列表头部增加元素 PushBack(v interface{}) 在列表尾部增加元素 InsertBefore(v interface{},mark *Element)*Element 在mark前增加元素 InsertAfter(v interface{},mark *Element)*Element 在mark后增加元素 MoveToFront(e *Element) 将元素移至列表头部 MoveToBack(e *Element) 将元素移至列表尾部 MoveBefore(e mark *Element) 将元素e移至mark前边 MoveAfter(e mark *Element) 将元素e移至mark后边 PushBackList(other *List) 将列表other放入当前列表尾部 PushFrontList(other *List) 将列表other放至当前列表头部]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sync.Map 底层实现分析]]></title>
    <url>%2Fblog%2F2018%2F12%2F17%2F20181217141509.html</url>
    <content type="text"><![CDATA[sync.Map的数据结构 首先看一下sync.Map的底层数据结构 123456789101112131415161718192021222324252627282930type Map struct &#123; // 互斥锁，锁定 dirty mu Mutex // 优先读 read atomic.Value // 保存最新map dirty map[interface&#123;&#125;]*entry // 记录从read中读取不到数据次数 // 当misses&gt;=len(dirty)时，会将dirty作为read,然后dirty=nil // (源码中 missLocked函数) misses int &#125;type readOnly struct &#123; // 主要用于存储 m map[interface&#123;&#125;]*entry // 如果数据在dirty中但没有在read中，该值为true,作为修改标识 amended bool &#125;type entry struct &#123; // nil: 表示为被删除，调用Delete()可以将read map中的元素置为nil // expunged: 也是表示被删除，但是该键只在read而没有在dirty中，这种情况出现在将read复制到dirty中，即复制的过程会先将nil标记为expunged，然后不将其复制到dirty // 其他: 表示存着真正的数据 p unsafe.Pointer // *interface&#123;&#125;&#125; mu 用于数据存取数据加锁 read dirty misses sync.Map的操作存数据，Store(key,val)1234567891011121314151617181920212223242526272829// Store sets the value for a key.func (m *Map) Store(key, value interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123; return &#125; m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok &#123; if e.unexpungeLocked() &#123; // The entry was previously expunged, which implies that there is a // non-nil dirty map and this entry is not in it. m.dirty[key] = e &#125; e.storeLocked(&amp;value) &#125; else if e, ok := m.dirty[key]; ok &#123; e.storeLocked(&amp;value) &#125; else &#123; if !read.amended &#123; // We're adding the first new key to the dirty map. // Make sure it is allocated and mark the read-only map as incomplete. m.dirtyLocked() m.read.Store(readOnly&#123;m: read.m, amended: true&#125;) &#125; m.dirty[key] = newEntry(value) &#125; m.mu.Unlock()&#125; 首先判断是否在map.read中，如果在，则调用entry的tryStore方法，tryStore方法使用了atomic.CompareAndSwapPointer操作，保证了操作原子性。如果成功，则直接返回①。当该key值已删去时tryStore方法返回false继续执行② 加锁，重新获取map.read值保证锁定操作期间数据最新且不会发生变化。 如果key存在于map.read中： 如果read中数据被删除③，则将k-v放到dirty中(dirty中没有该key) 如果read中数据未被删除④不处理 将v放到read中 如果key不存在于map.read中，存在于dirty中⑤，更新dirty中的值 如果key既不存在于map.read中，也不存在于map.dirty中： read与dirty中key一致（amended为false）⑥，初始化read，并标记amended参数为true，将数据存到dirty中 否则不做处理 ⑦ 将数据存入dirty中 解锁返回 删除数据 Delete(key)1234567891011121314151617// Delete deletes the value for a key.func (m *Map) Delete(key interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; delete(m.dirty, key) &#125; m.mu.Unlock() &#125; if ok &#123; e.delete() &#125;&#125; 判断如果待删除的key不在map.read中且map.dirty中含有map.read中不存在的key，则： 锁定，重新判断map.read中是否有key，如果同上，则调用从map.dirty中删除的函数，然后解锁 否则，若key存在于map.read中，则调用entry中的delete（entry里只有一个参数，应该是val的指针） 访问map数据 Load(key)123456789101112131415161718192021222324252627// Load returns the value stored in the map for a key, or nil if no// value is present.// The ok result indicates whether value was found in the map.func (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; m.mu.Lock() // Avoid reporting a spurious miss if m.dirty got promoted while we were // blocked on m.mu. (If further loads of the same key will not miss, it's // not worth copying the dirty map for this key.) read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; e, ok = m.dirty[key] // Regardless of whether the entry was present, record a miss: this key // will take the slow path until the dirty map is promoted to the read // map. m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125; 判断如果待访问的key不在map.read中且map.dirty中含有map.read中不存在的key，则 对map加锁，再次判断为真，则调用map.missLocked()，（通过判断在map.read中miss的次数来决定是否将map.dirty替换掉map.read） 解锁 判断前边是否获得到val值，未获取到，则返回 nil,false 若前边取到val，则返回 LoadOrStore(key,val) 如果map中存在key，则返回对应的val 否则，保存参数中的key,val，并返回参数中的val 遍历Map: range123456789101112131415161718package mainimport ( "fmt" "sync")func main() &#123; m := sync.Map&#123;&#125; m.Store("k1", "v1") m.Store("k2", "v2") m.Range(func(k, v interface&#123;&#125;) bool &#123; fmt.Println(k, v) return true &#125;)&#125; 总结map.read与map.dirty中数据一致性： map.dirty包含map.read中没有的key，map.read.amended为false，表明两者底层val引用是一致的，此时对数据的 更新、删除（此时删除操作实际是将底层val的指针置位nil，相当于只删除了val未删除key） 做cas操作即可 否则，map.read.amended为true，此时操作数据要考虑map.read与map.dirty中数据一致性 load数据时，在map.read中访问不到次数大于等于map.dirty长度时，会用map.dirty替换map.read，并将map.dirty置为nil，此时map.read.amended为false（因为map.dirty中不包含map.read中不存在的key） store数据时，当map.dirty数据为空时，会将map.read中的key的val引用复制过来（此时会判断是否标记未删除，标记删除则不复制，删除了val的key也删除掉），再在map.dirty中增加k-v，此时map.read.amended为true]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[map基础]]></title>
    <url>%2Fblog%2F2018%2F12%2F16%2F20181216153850.html</url>
    <content type="text"><![CDATA[mapmap实现方式 go语言中map使用hash（散列表）实现 定义方法12345678// 声明一个 map 变量，其值为nil，(m1==nil)为truevar m1 map[int]int// 创建一个空map （可在&#123;&#125;中填写map中的元素）m2 := map[int]int&#123;&#125;// 创建一个空mapm3 := make(map[int]int) map的key类型限制 map使用hash表存储实现，key类型必须可比较相等。map的key类型必须不为function,slice,map，在定义时，如果使用这些类型会报错。struct类型也可作为key类型，但如果使用带有这些类型属性的struct不可以作为key。可能编译不会报错，但运行时会报错。 Map操作map长度 获取长度方法：len(map) 访问测试代码1234567891011121314151617181920212223package mainimport "fmt"func main() &#123; m := map[int]interface&#123;&#125;&#123; 0: 0, 1: 2, 2: 4, 3: 6, 5: nil, &#125; // 直接访问不存在的元素 m4v := m[4] fmt.Println(m4v) // 使用ok参数接收是否取得正确的值 m4v, ok := m[4] fmt.Println(m4v, ok) // 使用ok参数接收是否取得正确的值 m5v, ok := m[5] fmt.Println(m5v, ok)&#125; 测试结果：123&lt;nil&gt;&lt;nil&gt; false&lt;nil&gt; true 注意： 不使用ok参数接收是否取得值时，如果不存在key值对应的val，则返回对应类型的空：int返回0，string返回&quot;&quot;，引用类型返回nil等。 使用ok参数可以通过判断ok知道map中是否存在key 不使用ok时可能存在的问题，如上代码，m[4]不存在于map中，取得结果为nil，m[5]存在于map中，其本身对应的值为nil所以取得结果也为nil，这种情况下可能会存在问题 遍历测试代码12345678910111213141516package mainimport "fmt"func main() &#123; m := map[int]int&#123; 0: 0, 1: 2, 2: 4, 3: 6, &#125; for k, v := range m &#123; fmt.Println(k, v) &#125;&#125; 注意： 每次运行程序输出顺序可能不一致，map不能保证遍历的顺序性 删除元素 删除方法 delete 测试代码1234567891011121314151617package mainimport "fmt"func main() &#123; m := map[int]int&#123; 0: 0, 1: 2, 2: 4, &#125; delete(m, 0) _, ok := m[0] fmt.Println(ok) // 测试删除不存在的元素，不会报错 delete(m,5)&#125; 测试结果 输出：false 安全map——sync.Map 普通map类型在只读的情况下是线程安全的，但同时读写时非线程安全。 普通线程安全性普通map线程安全测试12345678910111213141516171819202122package mainfunc main() &#123; m := make(map[int]int) // 无限向map写数据 go func() &#123; for &#123; m[1] = 1 &#125; &#125;() // 无限从map读数据 go func() &#123; for &#123; _ = m[1] &#125; &#125;() // 让主线程不退出，并发程序后台执行 for true &#123; _ = 1 &#125;&#125; 测试结果:报错1fatal error: concurrent map read and map write sync.Map操作添加元素 方法：Store(key,val)；LoadOrStore(key,val) Store(key,val)方法，将key:val对存入map中； LoadOrStore(key,val)方法，返回值有两个 (val,ok)，val表示在map中key对应的值（如果已存在，则为原map中的值，如果不存在，则先将key:val对存入map，再返回val）；ok表示map中是否已存在key值及其对应的val。 删除元素 方法： Delete(key) 删除map中key及其对应的val 获取元素 方法： Load(key)；LoadOrStore(key,val) 直接调用Load方法，返回值有两个(val,ok)，值以及是否存在 LoadOrStore方法在添加元素部分已解释 遍历元素 方法： Range(f func(key, value interface{}) bool) 参数是一个函数，在函数中执行遍历操作]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-slice操作]]></title>
    <url>%2Fblog%2F2018%2F12%2F16%2F20181216012715.html</url>
    <content type="text"><![CDATA[添加元素方法：append(slice,elems...) 测试12345678910111213141516171819202122package mainimport "fmt"func main() &#123; // 未触发扩容 arr := []int&#123;0,1,2,3,4,5,6,7,8,9&#125; s := arr[0:2] news := append(s, 100) fmt.Printf("news ptr : %p\ts ptr : %p\n",news,s) fmt.Println("s\t",s) fmt.Println("news\t",news) fmt.Println("arr\t",arr) fmt.Println("====================") // 触发扩容 arr = []int&#123;0,1,2&#125; s = arr[:] news = append(s,100) fmt.Printf("news ptr : %p\ts ptr : %p\n",news,s) fmt.Println("s\t",s) fmt.Println("news\t",news) fmt.Println("arr\t",arr)&#125; 测试结果：123456789news ptr : 0xc00007a0a0 s ptr : 0xc00007a0a0s [0 1]news [0 1 100]arr [0 1 100 3 4 5 6 7 8 9]====================news ptr : 0xc000072030 s ptr : 0xc00004a0e0s [0 1 2]news [0 1 2 100]arr [0 1 2] 结论： append方法会创建一个新slice，原slice不变。 调用append方法如果未触发扩容，则底层array对应位置会做出相应改变 调用append方法如果触发扩容，则原底层array对应位置不发生变化，因为新的slice底层array也已变化 复制元素方法：copy(dst []type,src []type) 测试：12345678910111213141516171819package mainimport "fmt"func main() &#123; s1 := make([]int,1) s2 := []int&#123;1,2,3&#125; // len(src) &gt; len(dst) fmt.Println("before\tdst :",s1,"src :",s2) copy(s1,s2) fmt.Println("after\tdst :",s1,"src :",s2) fmt.Println("=====================") // len(src) &lt; len(dst) s1 = make([]int,1) fmt.Println("before\tdst :",s2,"src :",s1) copy(s2,s1) fmt.Println("after\tdst :",s2,"src :",s1)&#125; 测试结果：12345before dst : [0] src : [1 2 3]after dst : [1] src : [1 2 3]=====================before dst : [1 2 3] src : [0]after dst : [0 2 3] src : [0] 结论： 将src数据拷贝到dst中 如果src长度大于dst长度，则只会复制len(dst)个数据到dst中 如果src长度小于dst长度，则dst的前len(src)个数据会被src的数据覆盖，后边不变 删除操作 无默认删除操作，可通过append实现 测试123456789101112131415package mainimport "fmt"func main() &#123; s1 := []int&#123;0,1,2,3,4,5&#125; fmt.Printf("before del s1's ptr :\t%p\n",s1) fmt.Println(s1) fmt.Println("=============") // do delete s1 = append(s1[:2],s1[3:]...) fmt.Printf("after del s1's ptr :\t%p\n",s1) fmt.Println(s1)&#125; 测试结果：12345before del s1&apos;s ptr : 0xc000072030[0 1 2 3 4 5]=============after del s1&apos;s ptr : 0xc000072030[0 1 3 4 5] 结论： 删除元素slice长度只会减小不会增大，因此底层数组不会发生变化。 修改方法：无方法，直接通过 slice[i] = val 修改测试： 略 遍历方法： for i,v := range slice遍历测试：1234567891011121314151617181920212223package mainimport "fmt"func main() &#123; s := []int&#123;0, 2, 4, 6, 8, 10&#125; // 只用 index for i := range s &#123; fmt.Printf("%d,", i) &#125; fmt.Println() // index,value都用 for i, v := range s &#123; fmt.Printf("%d:%d,\t", i, v) &#125; fmt.Println() // 只用value for _, v := range s &#123; fmt.Printf("%d,", v) &#125; fmt.Println()&#125; 输出1230,1,2,3,4,5,0:0, 1:2, 2:4, 3:6, 4:8, 5:10, 0,2,4,6,8,10,]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go slice原理]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2F20181215204437.html</url>
    <content type="text"><![CDATA[原理 go语言slice是底层数组的一个视图。slice主要参数有3个: ptr指向slice的首个元素； len表示slice长度； cap表示slice容量。 当向slice里append元素的时候，如果元素个数大于容量的某个百分比，将会扩容。其ptr值也会变。slice扩容的原理其实就是更换该slice底层array数组（因为slice只是某一数组的视图） 定义时与数组的区别 array与slice的定义 12345678910// array定义array1 := [3]int&#123;1,2,3&#125;array2 := [...]int&#123;1,2,3&#125;// slice定义slice1 := []int&#123;1,2,3&#125;slice2 := make([]int,0,10)slice3 := array1[:] // array1 := [3]int&#123;1,2,3&#125;slice4 := slice1[:]var slice5 = []int 测试测试1 测试slice与array定义区别 12345678910111213141516171819202122232425package mainimport ( "fmt" "reflect")func main() &#123; array1 := [3]int&#123;1,2,3&#125; array2 := [...]int&#123;1,2,3&#125; slice1 := []int&#123;1,2,3&#125; slice2 := make([]int,5,10) slice3 := array1[:] slice4 := slice1[:] var slice5 []int fmt.Printf("array1 type : %v\n",reflect.TypeOf(array1).Kind()) fmt.Printf("array2 type : %v\n",reflect.TypeOf(array2).Kind()) fmt.Println("===================") fmt.Printf("slice1 type : %v\n",reflect.TypeOf(slice1).Kind()) fmt.Printf("slice2 type : %v\n",reflect.TypeOf(slice2).Kind()) fmt.Printf("slice3 type : %v\n",reflect.TypeOf(slice3).Kind()) fmt.Printf("slice4 type : %v\n",reflect.TypeOf(slice4).Kind()) fmt.Printf("slice5 type : %v\n",reflect.TypeOf(slice5).Kind())&#125; 输出结果： 12345678array1 type : arrayarray2 type : array===================slice1 type : sliceslice2 type : sliceslice3 type : sliceslice4 type : sliceslice5 type : slice 测试2 测试slice扩容条件 123456789101112131415161718package mainimport "fmt"func main() &#123; tmp := [...]int&#123;0, 1, 2, 3&#125; arr := tmp[1:] Cap := cap(arr) lastCap := Cap for i := 0; i &lt; 20000; i++ &#123; arr = append(arr, 0) Cap = cap(arr) if Cap == lastCap &#123; continue &#125; fmt.Printf("%p\tlen:%v\t\tlastCap:%v\t\tcap:%v\n", arr, len(arr), lastCap, Cap) lastCap = Cap &#125;&#125; 输出结果 123456789101112131415161718190xc000072030 len:4 lastCap:3 cap:60xc00003e060 len:7 lastCap:6 cap:120xc00008a000 len:13 lastCap:12 cap:240xc00008c000 len:25 lastCap:24 cap:480xc00008e000 len:49 lastCap:48 cap:960xc000090000 len:97 lastCap:96 cap:1920xc000092000 len:193 lastCap:192 cap:3840xc000098000 len:385 lastCap:384 cap:7680xc00009e000 len:769 lastCap:768 cap:15360xc0000a4000 len:1537 lastCap:1536 cap:20480xc0000a8000 len:2049 lastCap:2048 cap:25600xc0000b2000 len:2561 lastCap:2560 cap:34080xc0000c6000 len:3409 lastCap:3408 cap:51200xc0000d0000 len:5121 lastCap:5120 cap:71680xc0000de000 len:7169 lastCap:7168 cap:92160xc0000f0000 len:9217 lastCap:9216 cap:122880xc000108000 len:12289 lastCap:12288 cap:153600xc000126000 len:15361 lastCap:15360 cap:194560xc00014c000 len:19457 lastCap:19456 cap:24576 结论： 当扩容时，ptr指针变（即slice这个视图所在的array发生变化） 只有当cap大小等于len的时候才会扩容，扩容大小视cap大小而定：cap较小时，直接扩容一倍，稍大时，扩容比例较小 测试3 测试扩容前后的slice与原slice有什么不同以及两个相同的slice执行某些操作后，另一个的变化情况 12345678910111213141516171819202122232425package mainimport "fmt"func main() &#123; arr := make([]int,2,16) arr2 := arr // 扩容前两者地址一样，修改一个，另一个值也修改 arr[0] = 5 fmt.Printf("arr\t%p\t[0] : %v\tlen : %v\tcap : %v\n", arr, arr[0], len(arr), cap(arr)) fmt.Printf("ar2\t%p\t[0] : %v\tlen : %v\tcap : %v\n\n", arr2, arr2[0], len(arr2), cap(arr2)) // 扩容前，当一个增加元素时，另一个slice长度不变 arr = append(arr, 0) fmt.Printf("arr\t%p\t[0] : %v\tlen : %v\tcap : %v\n", arr, arr[0], len(arr), cap(arr)) fmt.Printf("ar2\t%p\t[0] : %v\tlen : %v\tcap : %v\n\n", arr2, arr2[0], len(arr2), cap(arr2)) // 循环，令arr扩容 for i := 0; i &lt; 100; i++ &#123; arr = append(arr, 0) &#125; // 扩容后，arr变成一个新的slice，修改其中一个slice的值，另一个不会变 fmt.Printf("arr\tptr : %p\tlen : %v\tcap : %v\n", arr, len(arr), cap(arr)) fmt.Printf("ar2\tptr : %p\tlen : %v\tcap : %v\n\n", arr2, len(arr2), cap(arr2)) arr[0] = 100 fmt.Printf("arr\t[0] : %v\n", arr[0]) fmt.Printf("ar2\t[0] : %v\n", arr2[0])&#125; 输出结果 1234567891011arr 0xc00007e080 [0] : 5 len : 2 cap : 16ar2 0xc00007e080 [0] : 5 len : 2 cap : 16arr 0xc00007e080 [0] : 5 len : 3 cap : 16ar2 0xc00007e080 [0] : 5 len : 2 cap : 16arr ptr : 0xc00008c000 len : 103 cap : 128ar2 ptr : 0xc00007e080 len : 2 cap : 16arr [0] : 100ar2 [0] : 5 结论 未发生扩容，两个slice相同，改变其中一个slice，另一个也变 未发生扩容，向其中一个增加元素，另一个len不变 发生扩容，新生成slice，地址变，改变其中一个slice另一个不变 测试4 测试对于两个引用同一个数组但len不同的slice，能否取到超出自身len的元素 1234567891011package mainimport "fmt"func main() &#123; tmp := [...]int&#123;0,1,2,3,4,5,6,7&#125; s1 := tmp[2:6] // s1 : [2,3,4,5] fmt.Println(len(s1)) // s1 长度只有1 s2 := s1[3:6] // s2 : [5,6,7] // fmt.Println(s2[3]) // 报错 fmt.Println(s2)&#125; 输出 124[5 6 7] 结论： 如实例，s1长度为1，使用[:]操作，能取到后边的数字 如实例，s1长度为1，无法使用s1[1]取到对应位置的数据]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
        <category>容器</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客next主题侧边菜单栏设置]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2F20181215023114.html</url>
    <content type="text"><![CDATA[增加菜单 增加菜单 tags、categories、等 123# 创建页面hexo new page tagshexo new page categories 增加链接123456# 在主题config里去掉menu里相关注释menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th 修改新建的tags、categories目录下的md文件 在md文件中增加type 属性 123456---title: tagsdate: 2018-12-15 02:22:50type: &quot;tags&quot;--- 使用标签和分类 在post里增加tags和categories属性 123456789---title: 博客next主题侧边菜单栏设置date: 2018-12-15 02:31:14tags: - github博客categories: - github博客--- 注意：tags、categories均可用数组[1,2,3]的形式代替列表形式。tags列表里内容同级，categories列表里内容不同级 永久链接 主要在blog根目录下的_config.yml文件中配置，默认配置是 年/月/日/标题/，用标题作为url一部分不是很合理，如果标题发生改变，那链接也跟着变化了。 所以要使用一个唯一标示且不会发生改变的值作为url。hexo自带的有id，可使用id，但id生成规则不是很清楚，测试过修改题目后，id也发生变化。 本网站使用的方法是：给每个md文件增加一个属性，记为pid，然后使用pid作为url一部分。 首先，修改blog根目录下的_config.yml文件如下： 1234# 永久化链接。（最后加 .html 有利于搜索引擎收录）permalink: blog/:year/:month/:day/:pid.htmlpermalink_defaults: pid: default 然后，修改新增post时的模板文件blog/scaffolds/post.md，增加pid属性，这里把pid设置成时间了，直接用date在转化后不是我们想要的格式，这里就用这种笨方法。或者留白，新建文件后自己填写也可。 使用这种方法，pid是与文章内容无任何关系的属性，只要我们不修改pid和永久链接规则，文章的链接就不会变。 12345678---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;pid: &#123;&#123;date[0]&#125;&#125;&#123;&#123;date[1]&#125;&#125;&#123;&#123;date[2]&#125;&#125;&#123;&#123;date[3]&#125;&#125;&#123;&#123;date[5]&#125;&#125;&#123;&#123;date[6]&#125;&#125;&#123;&#123;date[8]&#125;&#125;&#123;&#123;date[9]&#125;&#125;&#123;&#123;date[11]&#125;&#125;&#123;&#123;date[12]&#125;&#125;&#123;&#123;date[14]&#125;&#125;&#123;&#123;date[15]&#125;&#125;&#123;&#123;date[17]&#125;&#125;&#123;&#123;date[18]&#125;&#125;tags:categories:---]]></content>
      <categories>
        <category>github博客</category>
      </categories>
      <tags>
        <tag>github博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在GitHub上创建博客]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2F20181215005754.html</url>
    <content type="text"><![CDATA[GitHub搭建博客安装node.js 安装完成后测试 1234npm -v# 6.4.1node -v# v10.14.2 安装hexo 全局安装hexo 123456789101112131415161718192021npm install hexo -g# 安装完查看hexo -v# 输出hexo-cli: 1.1.0os: Windows_NT 10.0.17763 win32 x64http_parser: 2.8.0node: 10.14.2v8: 6.8.275.32-node.45uv: 1.23.2zlib: 1.2.11ares: 1.15.0modules: 64nghttp2: 1.34.0napi: 3openssl: 1.1.0jicu: 62.1unicode: 11.0cldr: 33.1tz: 2018e GitHub创建项目，并拉至本地 注意：项目名格式必须为 用户名.github.io 初始化hexo环境123456# 新建blog目录，在目录下执行命令hexo init# 完成后安装依赖npm install # npm install hexo-deployer-git --save 配置ssh，让本地可免密push到GitHub 略 新建博客，并上传到git新建博客1hexo new post "博客名" 配置12345# 在_config.yml最后配置deploy: type: git repo: git地址 branch: master 部署1hexo d -g 增加搜索功能安装插件：1npm install hexo-generator-searchdb --save 修改配置文件123456789# blog配置文件search: path: search.xml field: post format: html limit: 10000# 主题配置文件 themes/主题名/_config.ymllocal_search: enable: true 报错 ERROR Deployer not found: git1npm install --save hexo-deployer-git]]></content>
      <categories>
        <category>github博客</category>
      </categories>
      <tags>
        <tag>github博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言与其他语言不一样的地方]]></title>
    <url>%2Fblog%2F2018%2F10%2F23%2F20181210104815.html</url>
    <content type="text"><![CDATA[没有：类、继承、多态、重载 的概念 不同世界观： go语言中使用 duck typing，面向接口编程、函数式编程 面向对象的世界里，也流行变继承为组合的思维 面向对象的元素容易被滥用 为组合提供了便捷的支持 没有：try/catch/finally 太多错误被当作异常 很多C++项目组禁用try/catch 正确的使用try/catch处理错误，导致代码混乱 在产品代码中try/catch并不能减小开发人员负担 真正异常情况有 defer/panic/recover 模式处理 没有构造/析构/RAII 大型项目很少使用构造函数，多使用工厂函数，如果需要，可直接使用结构体初始化语法实现 go有gc，不需要析构 没有泛型 泛型实际想实现duck typing，go语言提供了duck typing及接口组合支持 使用来约束参数类型：本身复杂，但go自带slice,map,channel 类似泛型参数 type assertion、go generation来实现自己的泛型 泛型支持是作者唯一态度不强硬的点 todo]]></content>
      <categories>
        <category>go</category>
        <category>go语言基础</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>go语言基础</tag>
      </tags>
  </entry>
</search>
